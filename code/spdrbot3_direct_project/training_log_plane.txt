Loading user config located at: 'c:/programdata/anaconda3/envs/env_isaaclab/lib/site-packages/isaacsim/kit/data/Kit/Isaac-Sim/5.1/user.config.json'
[Info] [carb] Logging to file: c:/programdata/anaconda3/envs/env_isaaclab/lib/site-packages/isaacsim/kit/logs/Kit/Isaac-Sim/5.1/kit_20260206_210756.log
[0.125s] [ext: omni.kit.async_engine-0.0.3] startup
[0.546s] [ext: omni.metrics.core-0.0.3] startup
[0.547s] [ext: omni.client.lib-1.1.0] startup
[0.596s] [ext: omni.blobkey-1.1.2] startup
[0.597s] [ext: omni.stats-1.0.1] startup
[0.601s] [ext: omni.datastore-0.0.0] startup
[0.604s] [ext: omni.client-1.3.0] startup
[0.618s] [ext: omni.ujitso.default-1.0.0] startup
[0.623s] [ext: omni.hsscclient-1.1.2] startup
[0.634s] [ext: omni.gpu_foundation.shadercache.vulkan-1.0.0] startup
[0.646s] [ext: omni.assets.plugins-0.0.0] startup
[0.652s] [ext: omni.gpu_foundation-0.0.0] startup
[0.702s] [ext: carb.windowing.plugins-1.0.0] startup
[0.807s] [ext: omni.kit.renderer.init-0.0.0] startup
[0.809s] [ext: omni.materialx.libs-1.0.7] startup
[0.847s] [ext: omni.kit.loop-isaac-1.3.7] startup
[0.850s] [ext: omni.kit.test-2.0.1] startup
[0.993s] [ext: omni.kit.pipapi-0.0.0] startup
[0.994s] [ext: omni.usd.config-1.0.6] startup
[1.013s] [ext: omni.gpucompute.plugins-0.0.0] startup
[1.015s] [ext: omni.usd.libs-1.0.1] startup
[1.288s] [ext: omni.kit.pip_archive-0.0.0] startup
[1.288s] [ext: omni.mdl-56.0.3] startup
2026-02-06T15:22:57Z [1,371ms] [Warning] [gpu.foundation.plugin] Skipping unsupported non-NVIDIA GPU: Intel(R) UHD Graphics 770
2026-02-06T15:22:57Z [1,372ms] [Warning] [gpu.foundation.plugin] Skipping unsupported non-NVIDIA GPU: Intel(R) UHD Graphics 770
[1.465s] [ext: omni.iray.libs-0.0.0] startup
[1.594s] [ext: omni.mdl.neuraylib-0.2.12] startup
[1.607s] [ext: omni.kit.usd.mdl-1.1.5] startup
[1.862s] [ext: omni.kit.telemetry-0.5.2] startup
[1.884s] [ext: omni.appwindow-1.1.10] startup
[1.906s] [ext: omni.kit.renderer.core-1.1.0] startup
[1.916s] [ext: omni.kit.renderer.capture-0.0.0] startup
[1.924s] [ext: omni.kit.renderer.imgui-2.0.5] startup
[1.944s] [ext: omni.ui-2.27.1] startup

|---------------------------------------------------------------------------------------------|
| Driver Version: 591.74        | Graphics API: Vulkan
|=============================================================================================|
| GPU | Name                             | Active | LDA | GPU Memory | Vendor-ID | LUID       |
|     |                                  |        |     |            | Device-ID | UUID       |
|     |                                  |        |     |            | Bus-ID    |            |
|---------------------------------------------------------------------------------------------|
| 0   | NVIDIA GeForce RTX 3080 Ti       | Yes: 0 |     | 12084   MB | 10de      | ffd50000.. |
|     |                                  |        |     |            | 2208      | 5d990520.. |
|     |                                  |        |     |            | 1         |            |
|---------------------------------------------------------------------------------------------|
| 1   | NVIDIA GeForce RTX 3080 Ti       | Yes: 1 |     | 12084   MB | 10de      | 9be60000.. |
|     |                                  |        |     |            | 2208      | ea5db13e.. |
|     |                                  |        |     |            | 6         |            |
|---------------------------------------------------------------------------------------------|
| 2   | Intel(R) UHD Graphics 770        |        |     | 16271   MB | 8086      | 21f60000.. |
|     |                                  |        |     |            | 4680      | 86808046.. |
|     |                                  |        |     |            | N/A       |            |
|=============================================================================================|
| OS: Windows 11 Pro, Version: 10.0 (25H2), Build: 26200, Kernel: 10.0.26100.7623
| Processor: 12th Gen Intel(R) Core(TM) i7-12700K
| Cores: 12 | Logical Cores: 20
|---------------------------------------------------------------------------------------------|
| Total Memory (MB): 32542 | Free Memory: 27991
| Total Page/Swap (MB): 63978 | Free Page/Swap: 54944
|---------------------------------------------------------------------------------------------|
[1.967s] [ext: omni.kit.mainwindow-1.0.3] startup
[1.969s] [ext: carb.audio-0.1.0] startup
2026-02-06T15:22:58Z [1,906ms] [Warning] [gpu.foundation.plugin] Device 0 PCIe link current width 16 anddevice 1 PCIe link current width 4 don't match.
2026-02-06T15:22:58Z [1,906ms] [Warning] [gpu.foundation.plugin] PCIe link width current (4) and maximum (16) for device 1 don't match.
[1.971s] [ext: omni.uiaudio-1.0.0] startup
[1.976s] [ext: omni.kit.uiapp-0.0.0] startup
[1.976s] [ext: omni.usd.schema.metrics.assembler-107.3.1] startup
[1.984s] [ext: omni.usd.schema.audio-0.0.0] startup
[1.994s] [ext: omni.usd_resolver-1.0.0] startup
[2.006s] [ext: omni.usd.core-1.5.3] startup
[2.009s] [ext: omni.usd.schema.render_settings.rtx-0.0.0] startup
[2.054s] [ext: omni.usd.schema.semantics-0.0.0] startup
[2.061s] [ext: omni.usd.schema.geospatial-0.0.0] startup
[2.067s] [ext: omni.usd.schema.anim-0.0.0] startup
[2.113s] [ext: omni.usd.schema.omni_lens_distortion-0.0.0] startup
[2.114s] [ext: isaacsim.robot.schema-3.6.0] startup
[2.133s] [ext: omni.usd.schema.omnigraph-1.0.0] startup
[2.149s] [ext: omni.usd.schema.physx-107.3.26] startup
[2.204s] [ext: omni.usd.schema.omni_sensors-0.0.0] startup
[2.206s] [ext: omni.usd.schema.omniscripting-1.0.0] startup
[2.216s] [ext: omni.graph.exec-0.9.6] startup
[2.218s] [ext: omni.kit.actions.core-1.0.0] startup
[2.224s] [ext: omni.kit.usd_undo-0.1.8] startup
[2.225s] [ext: omni.kit.exec.core-0.13.4] startup
[2.230s] [ext: omni.kit.commands-1.4.10] startup
[2.240s] [ext: omni.kit.window.popup_dialog-2.0.24] startup
[2.249s] [ext: omni.activity.core-1.0.3] startup
[2.252s] [ext: omni.resourcemonitor-107.0.1] startup
[2.257s] [ext: omni.timeline-1.0.14] startup
[2.261s] [ext: omni.kit.widget.nucleus_connector-2.0.1] startup
[2.267s] [ext: usdrt.scenegraph-7.6.1] startup
[2.361s] [ext: omni.kit.audiodeviceenum-1.0.2] startup
[2.365s] [ext: omni.hydra.usdrt_delegate-7.5.1] startup
[2.378s] [ext: omni.hydra.scene_delegate-0.3.4] startup
[2.383s] [ext: omni.usd-1.13.10] startup
[2.426s] [ext: omni.kit.asset_converter-5.0.17] startup
[2.452s] [ext: omni.index.libs-380600.8087.0] startup
[2.452s] [ext: omni.volume-0.5.2] startup
[2.468s] [ext: omni.ujitso.client-0.0.0] startup
[2.470s] [ext: omni.index-1.0.1] startup
[2.471s] [ext: omni.hydra.rtx.shadercache.vulkan-1.0.0] startup
[2.474s] [ext: omni.hydra.rtx-1.0.0] startup
[2.578s] [ext: omni.kit.notification_manager-1.0.10] startup
[2.582s] [ext: omni.kit.clipboard-1.0.5] startup
[2.584s] [ext: omni.kit.viewport.legacy_gizmos-1.0.19] startup
[2.587s] [ext: omni.kit.raycast.query-1.1.0] startup
[2.592s] [ext: omni.kit.menu.core-1.1.2] startup
[2.593s] [ext: omni.kit.widget.options_menu-1.1.6] startup
[2.600s] [ext: omni.kit.helper.file_utils-0.1.9] startup
[2.603s] [ext: omni.kit.widget.path_field-2.0.11] startup
[2.605s] [ext: omni.kit.widget.context_menu-1.2.5] startup
[2.608s] [ext: omni.kit.widget.options_button-1.0.3] startup
[2.609s] [ext: omni.kit.widget.filebrowser-2.12.3] startup
[2.622s] [ext: omni.kit.widget.browser_bar-2.0.10] startup
[2.624s] [ext: omni.kit.usd.layers-2.2.11] startup
[2.640s] [ext: omni.ui.scene-1.11.5] startup
[2.648s] [ext: omni.kit.viewport.registry-104.0.6] startup
[2.649s] [ext: omni.kit.window.filepicker-2.13.4] startup
[2.684s] [ext: omni.kit.menu.utils-2.0.5] startup
[2.699s] [ext: omni.kit.context_menu-1.8.6] startup
[2.703s] [ext: omni.kit.viewport.scene_camera_model-1.0.6] startup
[2.706s] [ext: omni.kit.hydra_texture-1.4.6] startup
[2.711s] [ext: omni.kit.window.file_importer-1.1.18] startup
[2.717s] [ext: omni.kit.widget.searchable_combobox-1.0.6] startup
[2.720s] [ext: omni.kit.window.drop_support-1.0.5] startup
[2.721s] [ext: omni.kit.widget.viewport-107.1.3] startup
[2.731s] [ext: omni.kit.material.library-2.0.7] startup
[2.742s] [ext: omni.hydra.engine.stats-1.0.3] startup
[2.745s] [ext: omni.kit.widget.settings-1.2.6] startup
[2.748s] [ext: omni.kit.viewport.window-107.2.0] startup
[2.994s] [ext: omni.kit.window.preferences-1.8.0] startup
[3.006s] [ext: omni.kit.widget.toolbar-2.0.1] startup
[3.019s] [ext: omni.kit.viewport.utility-1.1.2] startup
[3.021s] [ext: omni.kit.manipulator.transform-107.0.0] startup
[3.029s] [ext: omni.kit.manipulator.tool.snap-1.5.13] startup
[3.038s] [ext: omni.kit.manipulator.selector-1.1.3] startup
[3.041s] [ext: omni.kit.property.adapter.core-1.0.2] startup
[3.045s] [ext: omni.kit.viewport.manipulator.transform-107.0.4] startup
[3.049s] [ext: omni.kit.manipulator.viewport-107.0.1] startup
[3.051s] [ext: omni.kit.property.adapter.fabric-1.0.3] startup
[3.055s] [ext: omni.kit.manipulator.prim.core-107.0.8] startup
[3.069s] [ext: omni.kit.primitive.mesh-1.0.17] startup
[3.077s] [ext: omni.kit.widget.filter-1.1.4] startup
[3.079s] [ext: omni.kit.hotkeys.core-1.3.10] startup
[3.082s] [ext: omni.kit.manipulator.prim.usd-107.0.3] startup
[3.084s] [ext: omni.fabric.commands-1.1.6] startup
[3.091s] [ext: omni.kit.window.file_exporter-1.0.33] startup
[3.093s] [ext: omni.kit.widget.searchfield-1.1.8] startup
[3.095s] [ext: omni.kit.manipulator.prim.fabric-107.0.4] startup
[3.098s] [ext: omni.debugdraw-0.1.4] startup
[3.103s] [ext: omni.kit.widget.stage-3.1.4] startup
[3.132s] [ext: omni.kit.property.adapter.usd-1.0.2] startup
[3.135s] [ext: omni.kit.manipulator.prim-107.0.0] startup
[3.135s] [ext: omni.kvdb-107.3.26] startup
[3.140s] [ext: omni.convexdecomposition-107.3.26] startup
[3.145s] [ext: omni.physx.foundation-107.3.26] startup
[3.157s] [ext: omni.localcache-107.3.26] startup
[3.161s] [ext: omni.kit.window.content_browser_registry-0.0.6] startup
[3.163s] [ext: omni.kit.widget.highlight_label-1.0.3] startup
[3.165s] [ext: omni.kit.stage_template.core-1.1.22] startup
[3.166s] [ext: omni.usdphysics-107.3.26] startup
[3.169s] [ext: omni.kit.window.file-2.0.5] startup
[3.176s] [ext: omni.physx.cooking-107.3.26] startup
[3.182s] [ext: omni.physics-107.3.26] startup
[3.191s] [ext: omni.kit.window.property-1.12.1] startup
[3.198s] [ext: omni.kit.window.content_browser-3.1.3] startup
[3.215s] [ext: omni.physx-107.3.26] startup
[3.239s] [ext: omni.physics.stageupdate-107.3.26] startup
[3.245s] [ext: omni.kit.property.usd-4.5.12] startup
[3.264s] [ext: omni.kit.manipulator.selection-106.0.1] startup
[3.267s] [ext: omni.physics.physx-107.3.26] startup
2026-02-06T15:22:59Z [3,204ms] [Warning] [carb] Acquiring non optional plugin interface which is not listed as dependency: [omni::physx::IPhysxBenchmarks v1.0] (plugin: <default plugin>), by client: omni.physics.physx.plugin. Add it to CARB_PLUGIN_IMPL_DEPS() macro of a client.
[3.269s] [ext: omni.kit.widget.prompt-1.0.7] startup
[3.271s] [ext: omni.kit.viewport.menubar.core-107.2.1] startup
[3.300s] [ext: omni.kit.viewport.actions-107.0.2] startup
[3.306s] [ext: omni.inspect-1.0.2] startup
[3.311s] [ext: omni.kit.widget.layers-1.8.6] startup
[3.336s] [ext: omni.kit.viewport.menubar.display-107.0.3] startup
[3.339s] [ext: omni.usd.metrics.assembler-107.3.1] startup
[3.349s] [ext: omni.graph.core-2.184.5] startup
[3.355s] [ext: omni.kit.numpy.common-0.1.3] startup
[3.358s] [ext: omni.usdphysics.ui-107.3.26] startup
[3.386s] [ext: omni.physx.commands-107.3.26] startup
[3.393s] [ext: isaacsim.core.deprecation_manager-0.2.7] startup
[3.394s] [ext: omni.isaac.dynamic_control-2.0.7] startup
2026-02-06T15:22:59Z [3,339ms] [Warning] [omni.isaac.dynamic_control] omni.isaac.dynamic_control is deprecated as of Isaac Sim 4.5. No action is needed from end-users.
[3.405s] [ext: omni.physx.ui-107.3.26] startup
[3.453s] [ext: isaacsim.core.version-2.0.6] startup
[3.455s] [ext: omni.physics.tensors-107.3.26] startup
[3.464s] [ext: omni.warp.core-1.8.2] startup
[3.654s] [ext: omni.usd.metrics.assembler.physics-107.3.26] startup
[3.658s] [ext: isaacsim.storage.native-1.5.1] startup
[3.661s] [ext: omni.physx.tensors-107.3.26] startup
[3.666s] [ext: isaacsim.core.utils-3.5.1] startup
[3.673s] [ext: isaacsim.core.simulation_manager-1.4.4] startup
[6.371s] [ext: omni.kit.widget.stage_icons-1.0.8] startup
[6.373s] [ext: omni.kit.widget.text_editor-1.1.1] startup
[6.377s] [ext: omni.kit.window.stage-2.6.1] startup
[6.383s] [ext: omni.kit.menu.create-2.0.1] startup
[6.385s] [ext: omni.kit.window.extensions-1.4.27] startup
[6.397s] [ext: omni.kit.scripting-107.3.2] startup
[6.405s] [ext: omni.kit.stagerecorder.core-107.0.3] startup
[6.412s] [ext: isaacsim.replicator.behavior-1.1.16] startup
[6.414s] [ext: omni.graph.tools-1.79.2] startup
[6.461s] [ext: omni.ui_query-1.1.8] startup
[6.463s] [ext: omni.kit.widget.zoombar-1.0.6] startup
[6.465s] [ext: omni.graph-1.141.2] startup
[6.538s] [ext: omni.graph.action_core-1.1.7] startup
[6.544s] [ext: omni.kit.ui_test-1.3.7] startup
[6.548s] [ext: omni.kit.browser.core-2.3.13] startup
[6.558s] [ext: omni.kit.usd.collect-2.4.5] startup
[6.562s] [ext: omni.graph.action_nodes-1.50.4] startup
[6.573s] [ext: omni.kit.menu.stage-1.2.7] startup
[6.581s] [ext: omni.kit.browser.folder.core-1.10.9] startup
[6.597s] [ext: omni.kit.usdz_export-1.0.9] startup
[6.602s] [ext: omni.graph.visualization.nodes-2.1.3] startup
[6.617s] [ext: omni.graph.action-1.130.0] startup
[6.621s] [ext: omni.kit.tool.collect-2.2.18] startup
[6.629s] [ext: omni.kit.tool.asset_importer-4.3.2] startup
[6.664s] [ext: isaacsim.gui.components-1.2.1] startup
[6.672s] [ext: isaacsim.examples.browser-0.2.1] startup
[6.679s] [ext: isaacsim.asset.importer.urdf-2.4.31] startup
[6.791s] [ext: isaacsim.core.cloner-1.4.10] startup
[6.801s] [ext: omni.kit.stagerecorder.ui-107.0.1] startup
[6.808s] [ext: isaacsim.asset.browser-1.3.23] startup
[7.051s] [ext: semantics.schema.editor-2.0.2] startup
2026-02-06T15:23:03Z [6,995ms] [Warning] [pxr.Semantics] pxr.Semantics is deprecated - please use Semantics instead
[7.062s] [ext: omni.kit.stagerecorder.bundle-105.0.2] startup
[7.063s] [ext: omni.kit.window.status_bar-0.1.9] startup
[7.066s] [ext: omni.kit.widget.graph-2.0.0] startup
[7.078s] [ext: omni.kit.stage_templates-2.0.0] startup
[7.083s] [ext: omni.graph.image.core-0.6.1] startup
[7.085s] [ext: omni.kit.graph.delegate.default-1.2.3] startup
[7.086s] [ext: isaacsim.core.experimental.utils-0.3.0] startup
[7.089s] [ext: omni.graph.image.nodes-1.3.1] startup
[7.091s] [ext: omni.kit.graph.editor.core-1.5.3] startup
[7.097s] [ext: omni.kit.graph.usd.commands-1.3.1] startup
[7.099s] [ext: omni.graph.nodes-1.170.10] startup
[7.144s] [ext: omni.graph.ui_nodes-1.50.5] startup
[7.151s] [ext: omni.kit.widget.material_preview-1.0.16] startup
[7.157s] [ext: omni.syntheticdata-0.6.13] startup
[7.206s] [ext: omni.videoencoding-0.1.2] startup
[7.214s] [ext: omni.warp-1.8.2] startup
[7.234s] [ext: omni.kit.window.material_graph-1.9.1] startup
[7.261s] [ext: omni.graph.scriptnode-1.50.0] startup
[7.264s] [ext: isaacsim.core.prims-0.6.1] startup
[7.316s] [ext: isaacsim.test.docstring-1.1.0] startup
[7.329s] [ext: omni.replicator.core-1.12.27] startup
2026-02-06T15:23:03Z [7,599ms] [Warning] [omni.graph.core.plugin] Found duplicate of category 'Replicator' - was 'Annotators', adding 'Fabric Reader'
2026-02-06T15:23:03Z [7,599ms] [Warning] [omni.graph.core.plugin] Category 'Replicator' not accepted on node type 'omni.replicator.core.FabricReader' in extension 'omni.replicator.core'
2026-02-06T15:23:03Z [7,601ms] [Warning] [omni.replicator.core.scripts.extension] No material configuration file, adding configuration to material settings directly.
[7.670s] [ext: isaacsim.core.api-4.8.0] startup
[7.710s] [ext: isaacsim.core.experimental.prims-0.8.1] startup
[7.766s] [ext: isaacsim.core.nodes-3.4.3] startup
[7.782s] [ext: isaacsim.robot.surface_gripper-3.3.1] startup
[7.794s] [ext: isaacsim.util.debug_draw-3.1.0] startup
[7.818s] [ext: omni.sensors.nv.common-3.0.0] startup
[7.940s] [ext: isaacsim.robot.manipulators-3.3.6] startup
[7.972s] [ext: isaacsim.sensors.physx-2.3.2] startup
[8.006s] [ext: omni.sensors.nv.materials-2.0.0] startup
[8.034s] [ext: omni.sensors.net-1.0.0] startup
[8.082s] [ext: isaacsim.app.about-2.0.11] startup
[8.101s] [ext: isaacsim.simulation_app-2.12.2] startup
[8.102s] [ext: omni.sensors.nv.ids-2.0.0] startup
[8.111s] [ext: omni.sensors.nv.lidar-3.0.0] startup
[8.143s] [ext: omni.kit.property.audio-1.0.16] startup
[8.171s] [ext: omni.kit.property.camera-1.0.10] startup
[8.187s] [ext: omni.kit.property.geometry-2.0.4] startup
[8.209s] [ext: omni.hydra.scene_api-0.1.2] startup
[8.222s] [ext: omni.kit.property.light-1.0.12] startup
[8.250s] [ext: omni.sensors.nv.wpm-3.0.0] startup
[8.259s] [ext: omni.kit.selection-0.1.6] startup
[8.264s] [ext: omni.kit.property.material-1.11.9] startup
[8.352s] [ext: omni.kit.property.transform-1.5.13] startup
[8.396s] [ext: omni.kit.property.render-1.2.1] startup
[8.427s] [ext: omni.sensors.nv.radar-3.0.0] startup
[8.446s] [ext: isaacsim.gui.menu-2.4.4] startup
[8.468s] [ext: omni.kit.manipulator.camera-106.0.4] startup
[8.479s] [ext: isaacsim.gui.property-1.1.3] startup
[8.485s] [ext: omni.kit.property.bundle-1.4.1] startup
[8.489s] [ext: isaacsim.sensors.rtx-15.8.4] startup
[8.506s] [ext: isaacsim.sensors.physics-0.4.3] startup
[8.521s] [ext: omni.kit.viewport.menubar.camera-107.0.6] startup
[8.539s] [ext: omni.kit.viewport.menubar.lighting-107.3.1] startup
[8.549s] [ext: omni.kit.viewport.menubar.settings-107.0.3] startup
[8.566s] [ext: omni.kit.viewport.menubar.render-107.0.10] startup
[8.573s] [ext: isaacsim.robot.policy.examples-4.1.11] startup
[8.576s] [ext: isaacsim.asset.importer.mjcf-2.5.13] startup
[8.595s] [ext: omni.kit.window.console-1.1.4] startup
[8.604s] [ext: omni.rtx.window.settings-0.6.19] startup
[8.613s] [ext: omni.ocio-0.1.1] startup
[8.616s] [ext: omni.physx.demos-107.3.26] startup
[8.664s] [ext: omni.kit.property.physx-107.3.26] startup
[8.689s] [ext: omni.replicator.replicator_yaml-2.0.11] startup
[8.717s] [ext: omni.asset_validator.core-1.1.6] startup
[8.791s] [ext: omni.rtx.settings.core-0.6.5] startup
[8.798s] [ext: omni.physx.vehicle-107.3.26] startup
[8.827s] [ext: omni.usd.metrics.assembler.ui-107.3.1] startup
[8.839s] [ext: omni.kit.window.script_editor-2.0.1] startup
[8.845s] [ext: isaacsim.robot.wheeled_robots-4.0.24] startup
[8.862s] [ext: omni.physx.asset_validator-107.3.26] startup
[8.878s] [ext: omni.kit.window.toolbar-2.0.0] startup
[8.883s] [ext: omni.physx.camera-107.3.26] startup
[8.898s] [ext: omni.anim.curve.core-1.3.1] startup
[8.922s] [ext: omni.physx.cct-107.3.26] startup
[8.941s] [ext: omni.physx.graph-107.3.26] startup
[8.966s] [ext: omni.physx.supportui-107.3.26] startup
[8.991s] [ext: omni.physx.telemetry-107.3.26] startup
[8.995s] [ext: isaaclab-0.54.2] startup
[9.472s] [ext: isaaclab_contrib-0.0.2] startup
[9.474s] [ext: isaacsim.core.throttling-2.2.2] startup
[9.478s] [ext: omni.kit.ui.actions-1.0.5] startup
[9.481s] [ext: semantics.schema.property-2.0.1] startup
[9.485s] [ext: omni.physx.bundle-107.3.26] startup
[9.486s] [ext: isaacsim.sensors.camera-1.3.6] startup
[9.493s] [ext: isaaclab_assets-0.2.4] startup
[9.886s] [ext: isaaclab_tasks-0.11.12] startup
[10.185s] [ext: omni.kit.menu.common-2.0.1] startup
[10.188s] [ext: isaaclab_rl-0.4.7] startup
[10.189s] [ext: isaaclab_mimic-1.0.16] startup
[10.190s] [ext: isaaclab.python-2.3.2] startup
[10.194s] Simulation App Starting
2026-02-06T15:23:06Z [10,709ms] [Warning] [carb.audio.context] failed to set the requested output during context creation.  Using a null streamer instead {result = eOutOfRange (5)}
2026-02-06T15:23:06Z [10,751ms] [Warning] [omni.fabric.plugin] Warning: attribute overrideClipRange not found for bucket id 9

[10.968s] app ready
[INFO][AppLauncher]: Using device: cuda:0
[INFO][AppLauncher]: Loading experience file: D:\jeevi\IsaacLab\apps\isaaclab.python.kit
[INFO]: Parsing configuration from: spdrbot3.tasks.direct.spdrbot3.spdrbot3_env_cfg:Spdrbot3EnvCfg
[INFO]: Parsing configuration from: spdrbot3.tasks.direct.spdrbot3.agents.rsl_rl_ppo_cfg:PPORunnerCfg
[INFO] Logging experiment in directory: D:\jeevi\SpdrBot\spdrbot3_direct_project\logs\rsl_rl\spdr3
Exact experiment name requested from command line: 2026-02-06_21-08-12
[2026-02-06 21:08:12,487][__main__][WARNING] - IO descriptors are only supported for manager based RL environments. No IO descriptors will be exported.

[36m====================================================================================================================[0m
[36m[1m[INFO][IsaacLab]: Logging to file: C:\Users\AI_LaB\AppData\Local\Temp\isaaclab\logs\isaaclab_2026-02-06_21-08-12.log[0m
[36m====================================================================================================================[0m

[33m21:08:13 [simulation_context.py] WARNING: The `enable_external_forces_every_iteration` parameter in the PhysxCfg is set to False. If you are experiencing noisy velocities, consider enabling this flag. You may need to slightly increase the number of velocity iterations (setting it to 1 or 2 rather than 0), together with this flag, to improve the accuracy of velocity updates.[0m
[INFO]: Base environment:
	Environment device    : cuda:0
	Environment seed      : 42
	Physics step-size     : 0.005
	Rendering step-size   : 0.005
	Environment step-size : 0.02
[33m21:08:13 [direct_rl_env.py] WARNING: The render interval (1) is smaller than the decimation (4). Multiple render calls will happen for each environment step.If this is not intended, set the render interval to be equal to the decimation.[0m
[13.762s] Simulation App Startup Complete
[16.548s] [ext: omni.physx.fabric-107.3.26] startup
2026-02-06T15:23:20Z [23,897ms] [Error] [omni.physx.plugin] Replication of this type is not supported: 589824, prim path: /World/envs/env_0/Robot/colliders/robotCollisionGroup
2026-02-06T15:23:20Z [23,897ms] [Error] [omni.physx.plugin] Replication of this type is not supported: 589824, prim path: /World/envs/env_0/Robot/colliders/collidersCollisionGroup
[INFO]: Time taken for scene creation : 6.330989 seconds
[INFO]: Scene manager:  <class InteractiveScene>
	Number of environments: 512
	Environment spacing   : 2.0
	Source prim name      : /World/envs/env_0
	Global prim paths     : []
	Replicate physics     : True
[INFO]: Starting the simulation. This may take a few seconds. Please wait...
[33m21:08:20 [actuator_pd.py] WARNING: The <ImplicitActuatorCfg> object has a value for 'effort_limit'. This parameter will be removed in the future. To set the effort limit, please use 'effort_limit_sim' instead.[0m
[33m21:08:20 [actuator_pd.py] WARNING: The <ImplicitActuatorCfg> object has a value for 'velocity_limit'. Previously, although this value was specified, it was not getting used by implicit actuators. Since this parameter affects the simulation behavior, we continue to not use it. This parameter will be removed in the future. To set the velocity limit, please use 'velocity_limit_sim' instead.[0m
C:\ProgramData\anaconda3\envs\env_isaaclab\Lib\site-packages\rsl_rl\utils\utils.py:245: UserWarning: The observation configuration dictionary 'obs_groups' must contain the 'policy' key. As an observation group with the name 'policy' was found, this is assumed to be the observation set. Consider adding the 'policy' key to the 'obs_groups' dictionary for clarity. This behavior will be removed in a future version.
  warnings.warn(
C:\ProgramData\anaconda3\envs\env_isaaclab\Lib\site-packages\rsl_rl\utils\utils.py:291: UserWarning: The observation configuration dictionary 'obs_groups' must contain the 'critic' key. As the configuration for 'critic' is missing, the observations from the 'policy' set are used. Consider adding the 'critic' key to the 'obs_groups' dictionary for clarity. This behavior will be removed in a future version.
  warnings.warn(
[INFO]: Time taken for simulation start : 70.624380 seconds
Creating window for environment.
ManagerLiveVisualizer cannot be created for manager: action_manager, Manager does not exist
ManagerLiveVisualizer cannot be created for manager: observation_manager, Manager does not exist
[INFO] Event Manager:  <EventManager> contains 1 active terms.
+--------------------------------------+
| Active Event Terms in Mode: 'startup' |
+----------+---------------------------+
|  Index   | Name                      |
+----------+---------------------------+
|    0     | physics_material          |
|    1     | add_base_mass             |
+----------+---------------------------+

[INFO]: Completed setting up the environment...
--------------------------------------------------------------------------------
Resolved observation sets: 
	 policy :  ['policy']
	 critic :  ['policy']
--------------------------------------------------------------------------------
Actor MLP: MLP(
  (0): Linear(in_features=48, out_features=64, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=64, out_features=64, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=64, out_features=12, bias=True)
)
Critic MLP: MLP(
  (0): Linear(in_features=48, out_features=64, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=64, out_features=64, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=64, out_features=1, bias=True)
)
################################################################################
                       [1m Learning iteration 0/500 [0m                       

                       Computation: 2475 steps/s (collection: 6.112s, learning 0.506s)
             Mean action noise std: 1.00
          Mean value_function loss: 1.9855
               Mean surrogate loss: -0.0152
                 Mean entropy loss: 17.0251
                       Mean reward: -0.43
               Mean episode length: 12.04
Episode_Reward/track_lin_vel_xy_exp: 0.0170
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0012
      Episode_Reward/ang_vel_xy_l2: -0.0060
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0120
     Episode_Reward/action_rate_l2: -0.0008
Episode_Reward/flat_orientation_l2: -0.0008
    Episode_Reward/foot_contact_l2: -0.0041
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.1562
--------------------------------------------------------------------------------
                   Total timesteps: 16384
                    Iteration time: 6.62s
                      Time elapsed: 00:00:06
                               ETA: 00:55:08

Could not find git repository in C:\ProgramData\anaconda3\envs\env_isaaclab\Lib\site-packages\rsl_rl\__init__.py. Skipping.
Storing git diff for 'SpdrBot' in: D:\jeevi\SpdrBot\spdrbot3_direct_project\logs\rsl_rl\spdr3\2026-02-06_21-08-12\git\SpdrBot.diff
################################################################################
                       [1m Learning iteration 1/500 [0m                       

                       Computation: 3072 steps/s (collection: 5.151s, learning 0.181s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.8994
               Mean surrogate loss: -0.0067
                 Mean entropy loss: 16.9992
                       Mean reward: -0.49
               Mean episode length: 13.03
Episode_Reward/track_lin_vel_xy_exp: 0.0226
Episode_Reward/track_ang_vel_z_exp: 0.0003
       Episode_Reward/lin_vel_z_l2: -0.0017
      Episode_Reward/ang_vel_xy_l2: -0.0086
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0168
     Episode_Reward/action_rate_l2: -0.0011
Episode_Reward/flat_orientation_l2: -0.0012
    Episode_Reward/foot_contact_l2: -0.0057
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32768
                    Iteration time: 5.33s
                      Time elapsed: 00:00:11
                               ETA: 00:49:41

################################################################################
                       [1m Learning iteration 2/500 [0m                       

                       Computation: 3005 steps/s (collection: 5.248s, learning 0.203s)
             Mean action noise std: 0.99
          Mean value_function loss: 0.3396
               Mean surrogate loss: -0.0068
                 Mean entropy loss: 16.9614
                       Mean reward: -0.41
               Mean episode length: 11.68
Episode_Reward/track_lin_vel_xy_exp: 0.0216
Episode_Reward/track_ang_vel_z_exp: 0.0003
       Episode_Reward/lin_vel_z_l2: -0.0016
      Episode_Reward/ang_vel_xy_l2: -0.0081
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0154
     Episode_Reward/action_rate_l2: -0.0010
Episode_Reward/flat_orientation_l2: -0.0012
    Episode_Reward/foot_contact_l2: -0.0054
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 5.45s
                      Time elapsed: 00:00:17
                               ETA: 00:48:08

################################################################################
                       [1m Learning iteration 3/500 [0m                       

                       Computation: 3278 steps/s (collection: 4.824s, learning 0.173s)
             Mean action noise std: 0.99
          Mean value_function loss: 0.2216
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 16.9315
                       Mean reward: -0.35
               Mean episode length: 11.71
Episode_Reward/track_lin_vel_xy_exp: 0.0202
Episode_Reward/track_ang_vel_z_exp: 0.0003
       Episode_Reward/lin_vel_z_l2: -0.0016
      Episode_Reward/ang_vel_xy_l2: -0.0075
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0147
     Episode_Reward/action_rate_l2: -0.0010
Episode_Reward/flat_orientation_l2: -0.0011
    Episode_Reward/foot_contact_l2: -0.0051
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65536
                    Iteration time: 5.00s
                      Time elapsed: 00:00:22
                               ETA: 00:46:22

################################################################################
                       [1m Learning iteration 4/500 [0m                       

                       Computation: 3320 steps/s (collection: 4.747s, learning 0.188s)
             Mean action noise std: 0.99
          Mean value_function loss: 0.1710
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 16.8910
                       Mean reward: -0.55
               Mean episode length: 12.09
Episode_Reward/track_lin_vel_xy_exp: 0.0194
Episode_Reward/track_ang_vel_z_exp: 0.0003
       Episode_Reward/lin_vel_z_l2: -0.0014
      Episode_Reward/ang_vel_xy_l2: -0.0069
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0137
     Episode_Reward/action_rate_l2: -0.0009
Episode_Reward/flat_orientation_l2: -0.0010
    Episode_Reward/foot_contact_l2: -0.0047
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81920
                    Iteration time: 4.93s
                      Time elapsed: 00:00:27
                               ETA: 00:45:11

################################################################################
                       [1m Learning iteration 5/500 [0m                       

                       Computation: 3353 steps/s (collection: 4.698s, learning 0.188s)
             Mean action noise std: 0.99
          Mean value_function loss: 0.1354
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 16.8651
                       Mean reward: -0.18
               Mean episode length: 7.99
Episode_Reward/track_lin_vel_xy_exp: 0.0180
Episode_Reward/track_ang_vel_z_exp: 0.0003
       Episode_Reward/lin_vel_z_l2: -0.0014
      Episode_Reward/ang_vel_xy_l2: -0.0062
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0122
     Episode_Reward/action_rate_l2: -0.0008
Episode_Reward/flat_orientation_l2: -0.0009
    Episode_Reward/foot_contact_l2: -0.0043
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 4.89s
                      Time elapsed: 00:00:32
                               ETA: 00:44:18

################################################################################
                       [1m Learning iteration 6/500 [0m                       

                       Computation: 3267 steps/s (collection: 4.826s, learning 0.188s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.1057
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 16.8467
                       Mean reward: -0.22
               Mean episode length: 8.57
Episode_Reward/track_lin_vel_xy_exp: 0.0173
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0012
      Episode_Reward/ang_vel_xy_l2: -0.0057
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0110
     Episode_Reward/action_rate_l2: -0.0007
Episode_Reward/flat_orientation_l2: -0.0007
    Episode_Reward/foot_contact_l2: -0.0041
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114688
                    Iteration time: 5.01s
                      Time elapsed: 00:00:37
                               ETA: 00:43:47

################################################################################
                       [1m Learning iteration 7/500 [0m                       

                       Computation: 3297 steps/s (collection: 4.777s, learning 0.192s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.0865
               Mean surrogate loss: -0.0070
                 Mean entropy loss: 16.8326
                       Mean reward: -0.13
               Mean episode length: 8.02
Episode_Reward/track_lin_vel_xy_exp: 0.0160
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0011
      Episode_Reward/ang_vel_xy_l2: -0.0050
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0098
     Episode_Reward/action_rate_l2: -0.0006
Episode_Reward/flat_orientation_l2: -0.0006
    Episode_Reward/foot_contact_l2: -0.0036
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131072
                    Iteration time: 4.97s
                      Time elapsed: 00:00:42
                               ETA: 00:43:20

################################################################################
                       [1m Learning iteration 8/500 [0m                       

                       Computation: 3316 steps/s (collection: 4.768s, learning 0.172s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.0785
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 16.8142
                       Mean reward: -0.07
               Mean episode length: 7.18
Episode_Reward/track_lin_vel_xy_exp: 0.0154
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0011
      Episode_Reward/ang_vel_xy_l2: -0.0045
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0087
     Episode_Reward/action_rate_l2: -0.0006
Episode_Reward/flat_orientation_l2: -0.0005
    Episode_Reward/foot_contact_l2: -0.0034
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 4.94s
                      Time elapsed: 00:00:47
                               ETA: 00:42:57

################################################################################
                       [1m Learning iteration 9/500 [0m                       

                       Computation: 3345 steps/s (collection: 4.725s, learning 0.172s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.0582
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 16.7769
                       Mean reward: -0.11
               Mean episode length: 7.51
Episode_Reward/track_lin_vel_xy_exp: 0.0146
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0011
      Episode_Reward/ang_vel_xy_l2: -0.0041
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0081
     Episode_Reward/action_rate_l2: -0.0006
Episode_Reward/flat_orientation_l2: -0.0005
    Episode_Reward/foot_contact_l2: -0.0032
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163840
                    Iteration time: 4.90s
                      Time elapsed: 00:00:52
                               ETA: 00:42:35

################################################################################
                      [1m Learning iteration 10/500 [0m                       

                       Computation: 3621 steps/s (collection: 4.433s, learning 0.091s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.0525
               Mean surrogate loss: -0.0069
                 Mean entropy loss: 16.7527
                       Mean reward: -0.08
               Mean episode length: 7.10
Episode_Reward/track_lin_vel_xy_exp: 0.0141
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0011
      Episode_Reward/ang_vel_xy_l2: -0.0038
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0073
     Episode_Reward/action_rate_l2: -0.0005
Episode_Reward/flat_orientation_l2: -0.0004
    Episode_Reward/foot_contact_l2: -0.0030
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180224
                    Iteration time: 4.52s
                      Time elapsed: 00:00:56
                               ETA: 00:41:59

################################################################################
                      [1m Learning iteration 11/500 [0m                       

                       Computation: 4485 steps/s (collection: 3.559s, learning 0.094s)
             Mean action noise std: 0.97
          Mean value_function loss: 0.0804
               Mean surrogate loss: -0.0069
                 Mean entropy loss: 16.7314
                       Mean reward: -0.08
               Mean episode length: 6.60
Episode_Reward/track_lin_vel_xy_exp: 0.0140
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0011
      Episode_Reward/ang_vel_xy_l2: -0.0036
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0073
     Episode_Reward/action_rate_l2: -0.0005
Episode_Reward/flat_orientation_l2: -0.0004
    Episode_Reward/foot_contact_l2: -0.0029
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 3.65s
                      Time elapsed: 00:01:00
                               ETA: 00:40:53

################################################################################
                      [1m Learning iteration 12/500 [0m                       

                       Computation: 4266 steps/s (collection: 3.706s, learning 0.134s)
             Mean action noise std: 0.97
          Mean value_function loss: 0.0507
               Mean surrogate loss: -0.0073
                 Mean entropy loss: 16.7125
                       Mean reward: -0.00
               Mean episode length: 6.11
Episode_Reward/track_lin_vel_xy_exp: 0.0138
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0011
      Episode_Reward/ang_vel_xy_l2: -0.0034
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0067
     Episode_Reward/action_rate_l2: -0.0005
Episode_Reward/flat_orientation_l2: -0.0003
    Episode_Reward/foot_contact_l2: -0.0028
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 212992
                    Iteration time: 3.84s
                      Time elapsed: 00:01:04
                               ETA: 00:40:04

################################################################################
                      [1m Learning iteration 13/500 [0m                       

                       Computation: 4295 steps/s (collection: 3.710s, learning 0.105s)
             Mean action noise std: 0.97
          Mean value_function loss: 0.0525
               Mean surrogate loss: -0.0080
                 Mean entropy loss: 16.6955
                       Mean reward: -0.01
               Mean episode length: 5.62
Episode_Reward/track_lin_vel_xy_exp: 0.0134
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0011
      Episode_Reward/ang_vel_xy_l2: -0.0032
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0062
     Episode_Reward/action_rate_l2: -0.0004
Episode_Reward/flat_orientation_l2: -0.0003
    Episode_Reward/foot_contact_l2: -0.0027
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 229376
                    Iteration time: 3.81s
                      Time elapsed: 00:01:07
                               ETA: 00:39:20

################################################################################
                      [1m Learning iteration 14/500 [0m                       

                       Computation: 4335 steps/s (collection: 3.675s, learning 0.104s)
             Mean action noise std: 0.97
          Mean value_function loss: 0.0425
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 16.6720
                       Mean reward: 0.04
               Mean episode length: 5.96
Episode_Reward/track_lin_vel_xy_exp: 0.0136
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0011
      Episode_Reward/ang_vel_xy_l2: -0.0031
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0060
     Episode_Reward/action_rate_l2: -0.0004
Episode_Reward/flat_orientation_l2: -0.0003
    Episode_Reward/foot_contact_l2: -0.0027
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 3.78s
                      Time elapsed: 00:01:11
                               ETA: 00:38:41

################################################################################
                      [1m Learning iteration 15/500 [0m                       

                       Computation: 4332 steps/s (collection: 3.679s, learning 0.103s)
             Mean action noise std: 0.96
          Mean value_function loss: 0.0462
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 16.6305
                       Mean reward: 0.01
               Mean episode length: 6.02
Episode_Reward/track_lin_vel_xy_exp: 0.0137
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0011
      Episode_Reward/ang_vel_xy_l2: -0.0030
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0061
     Episode_Reward/action_rate_l2: -0.0004
Episode_Reward/flat_orientation_l2: -0.0003
    Episode_Reward/foot_contact_l2: -0.0026
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 262144
                    Iteration time: 3.78s
                      Time elapsed: 00:01:15
                               ETA: 00:38:06

################################################################################
                      [1m Learning iteration 16/500 [0m                       

                       Computation: 4428 steps/s (collection: 3.596s, learning 0.103s)
             Mean action noise std: 0.96
          Mean value_function loss: 0.0365
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 16.5461
                       Mean reward: 0.01
               Mean episode length: 5.75
Episode_Reward/track_lin_vel_xy_exp: 0.0137
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0011
      Episode_Reward/ang_vel_xy_l2: -0.0029
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0059
     Episode_Reward/action_rate_l2: -0.0004
Episode_Reward/flat_orientation_l2: -0.0003
    Episode_Reward/foot_contact_l2: -0.0026
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 278528
                    Iteration time: 3.70s
                      Time elapsed: 00:01:19
                               ETA: 00:37:32

################################################################################
                      [1m Learning iteration 17/500 [0m                       

                       Computation: 4249 steps/s (collection: 3.760s, learning 0.096s)
             Mean action noise std: 0.95
          Mean value_function loss: 0.0338
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 16.4634
                       Mean reward: 0.05
               Mean episode length: 5.74
Episode_Reward/track_lin_vel_xy_exp: 0.0139
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0011
      Episode_Reward/ang_vel_xy_l2: -0.0029
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0056
     Episode_Reward/action_rate_l2: -0.0004
Episode_Reward/flat_orientation_l2: -0.0003
    Episode_Reward/foot_contact_l2: -0.0026
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 3.86s
                      Time elapsed: 00:01:22
                               ETA: 00:37:06

################################################################################
                      [1m Learning iteration 18/500 [0m                       

                       Computation: 4368 steps/s (collection: 3.651s, learning 0.099s)
             Mean action noise std: 0.95
          Mean value_function loss: 0.0364
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 16.3918
                       Mean reward: 0.08
               Mean episode length: 5.39
Episode_Reward/track_lin_vel_xy_exp: 0.0139
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0011
      Episode_Reward/ang_vel_xy_l2: -0.0027
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0056
     Episode_Reward/action_rate_l2: -0.0004
Episode_Reward/flat_orientation_l2: -0.0002
    Episode_Reward/foot_contact_l2: -0.0026
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 311296
                    Iteration time: 3.75s
                      Time elapsed: 00:01:26
                               ETA: 00:36:40

################################################################################
                      [1m Learning iteration 19/500 [0m                       

                       Computation: 4334 steps/s (collection: 3.671s, learning 0.109s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0309
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 16.3120
                       Mean reward: 0.06
               Mean episode length: 5.99
Episode_Reward/track_lin_vel_xy_exp: 0.0138
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0012
      Episode_Reward/ang_vel_xy_l2: -0.0026
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0054
     Episode_Reward/action_rate_l2: -0.0004
Episode_Reward/flat_orientation_l2: -0.0002
    Episode_Reward/foot_contact_l2: -0.0025
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 327680
                    Iteration time: 3.78s
                      Time elapsed: 00:01:30
                               ETA: 00:36:16

################################################################################
                      [1m Learning iteration 20/500 [0m                       

                       Computation: 4103 steps/s (collection: 3.889s, learning 0.104s)
             Mean action noise std: 0.93
          Mean value_function loss: 0.0293
               Mean surrogate loss: -0.0152
                 Mean entropy loss: 16.1823
                       Mean reward: 0.11
               Mean episode length: 5.52
Episode_Reward/track_lin_vel_xy_exp: 0.0140
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0012
      Episode_Reward/ang_vel_xy_l2: -0.0026
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0052
     Episode_Reward/action_rate_l2: -0.0004
Episode_Reward/flat_orientation_l2: -0.0002
    Episode_Reward/foot_contact_l2: -0.0025
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 3.99s
                      Time elapsed: 00:01:34
                               ETA: 00:36:00

################################################################################
                      [1m Learning iteration 21/500 [0m                       

                       Computation: 4388 steps/s (collection: 3.634s, learning 0.099s)
             Mean action noise std: 0.92
          Mean value_function loss: 0.0250
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 16.0607
                       Mean reward: 0.12
               Mean episode length: 5.55
Episode_Reward/track_lin_vel_xy_exp: 0.0139
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0012
      Episode_Reward/ang_vel_xy_l2: -0.0025
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0048
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0002
    Episode_Reward/foot_contact_l2: -0.0025
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 360448
                    Iteration time: 3.73s
                      Time elapsed: 00:01:38
                               ETA: 00:35:38

################################################################################
                      [1m Learning iteration 22/500 [0m                       

                       Computation: 4501 steps/s (collection: 3.548s, learning 0.092s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.0325
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 15.9483
                       Mean reward: 0.15
               Mean episode length: 5.38
Episode_Reward/track_lin_vel_xy_exp: 0.0141
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0012
      Episode_Reward/ang_vel_xy_l2: -0.0024
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0048
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0002
    Episode_Reward/foot_contact_l2: -0.0025
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 376832
                    Iteration time: 3.64s
                      Time elapsed: 00:01:41
                               ETA: 00:35:17

################################################################################
                      [1m Learning iteration 23/500 [0m                       

                       Computation: 4494 steps/s (collection: 3.528s, learning 0.118s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.0222
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 15.8623
                       Mean reward: 0.11
               Mean episode length: 5.57
Episode_Reward/track_lin_vel_xy_exp: 0.0142
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0012
      Episode_Reward/ang_vel_xy_l2: -0.0024
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0048
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0002
    Episode_Reward/foot_contact_l2: -0.0025
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 3.65s
                      Time elapsed: 00:01:45
                               ETA: 00:34:57

################################################################################
                      [1m Learning iteration 24/500 [0m                       

                       Computation: 4450 steps/s (collection: 3.563s, learning 0.119s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0294
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 15.7397
                       Mean reward: 0.10
               Mean episode length: 5.49
Episode_Reward/track_lin_vel_xy_exp: 0.0144
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0012
      Episode_Reward/ang_vel_xy_l2: -0.0024
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0049
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0002
    Episode_Reward/foot_contact_l2: -0.0025
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 409600
                    Iteration time: 3.68s
                      Time elapsed: 00:01:49
                               ETA: 00:34:39

################################################################################
                      [1m Learning iteration 25/500 [0m                       

                       Computation: 4595 steps/s (collection: 3.473s, learning 0.093s)
             Mean action noise std: 0.88
          Mean value_function loss: 0.0255
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 15.5895
                       Mean reward: 0.09
               Mean episode length: 5.44
Episode_Reward/track_lin_vel_xy_exp: 0.0144
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0013
      Episode_Reward/ang_vel_xy_l2: -0.0023
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0046
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0002
    Episode_Reward/foot_contact_l2: -0.0024
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 425984
                    Iteration time: 3.57s
                      Time elapsed: 00:01:52
                               ETA: 00:34:20

################################################################################
                      [1m Learning iteration 26/500 [0m                       

                       Computation: 4593 steps/s (collection: 3.474s, learning 0.094s)
             Mean action noise std: 0.88
          Mean value_function loss: 0.0225
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 15.5000
                       Mean reward: 0.11
               Mean episode length: 5.06
Episode_Reward/track_lin_vel_xy_exp: 0.0143
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0013
      Episode_Reward/ang_vel_xy_l2: -0.0022
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0045
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0002
    Episode_Reward/foot_contact_l2: -0.0024
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 3.57s
                      Time elapsed: 00:01:56
                               ETA: 00:34:02

################################################################################
                      [1m Learning iteration 27/500 [0m                       

                       Computation: 4557 steps/s (collection: 3.502s, learning 0.093s)
             Mean action noise std: 0.86
          Mean value_function loss: 0.0229
               Mean surrogate loss: -0.0155
                 Mean entropy loss: 15.3626
                       Mean reward: 0.12
               Mean episode length: 5.48
Episode_Reward/track_lin_vel_xy_exp: 0.0146
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0013
      Episode_Reward/ang_vel_xy_l2: -0.0022
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0045
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0002
    Episode_Reward/foot_contact_l2: -0.0024
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 458752
                    Iteration time: 3.60s
                      Time elapsed: 00:01:59
                               ETA: 00:33:46

################################################################################
                      [1m Learning iteration 28/500 [0m                       

                       Computation: 4462 steps/s (collection: 3.579s, learning 0.093s)
             Mean action noise std: 0.85
          Mean value_function loss: 0.0207
               Mean surrogate loss: -0.0163
                 Mean entropy loss: 15.1767
                       Mean reward: 0.16
               Mean episode length: 5.33
Episode_Reward/track_lin_vel_xy_exp: 0.0145
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0013
      Episode_Reward/ang_vel_xy_l2: -0.0021
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0043
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0002
    Episode_Reward/foot_contact_l2: -0.0024
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 475136
                    Iteration time: 3.67s
                      Time elapsed: 00:02:03
                               ETA: 00:33:31

################################################################################
                      [1m Learning iteration 29/500 [0m                       

                       Computation: 4537 steps/s (collection: 3.518s, learning 0.093s)
             Mean action noise std: 0.84
          Mean value_function loss: 0.0227
               Mean surrogate loss: -0.0169
                 Mean entropy loss: 14.9999
                       Mean reward: 0.18
               Mean episode length: 5.32
Episode_Reward/track_lin_vel_xy_exp: 0.0147
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0013
      Episode_Reward/ang_vel_xy_l2: -0.0020
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0044
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0001
    Episode_Reward/foot_contact_l2: -0.0024
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 3.61s
                      Time elapsed: 00:02:07
                               ETA: 00:33:17

################################################################################
                      [1m Learning iteration 30/500 [0m                       

                       Computation: 4610 steps/s (collection: 3.462s, learning 0.092s)
             Mean action noise std: 0.84
          Mean value_function loss: 0.0182
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 14.8879
                       Mean reward: 0.19
               Mean episode length: 5.38
Episode_Reward/track_lin_vel_xy_exp: 0.0149
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0014
      Episode_Reward/ang_vel_xy_l2: -0.0020
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0042
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0001
    Episode_Reward/foot_contact_l2: -0.0024
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 507904
                    Iteration time: 3.55s
                      Time elapsed: 00:02:10
                               ETA: 00:33:02

################################################################################
                      [1m Learning iteration 31/500 [0m                       

                       Computation: 4484 steps/s (collection: 3.559s, learning 0.095s)
             Mean action noise std: 0.83
          Mean value_function loss: 0.0184
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 14.8072
                       Mean reward: 0.21
               Mean episode length: 5.36
Episode_Reward/track_lin_vel_xy_exp: 0.0148
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0014
      Episode_Reward/ang_vel_xy_l2: -0.0020
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0042
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0001
    Episode_Reward/foot_contact_l2: -0.0024
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 524288
                    Iteration time: 3.65s
                      Time elapsed: 00:02:14
                               ETA: 00:32:50

################################################################################
                      [1m Learning iteration 32/500 [0m                       

                       Computation: 4555 steps/s (collection: 3.505s, learning 0.092s)
             Mean action noise std: 0.82
          Mean value_function loss: 0.0212
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 14.7073
                       Mean reward: 0.18
               Mean episode length: 5.28
Episode_Reward/track_lin_vel_xy_exp: 0.0149
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0014
      Episode_Reward/ang_vel_xy_l2: -0.0019
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0040
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0001
    Episode_Reward/foot_contact_l2: -0.0024
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 540672
                    Iteration time: 3.60s
                      Time elapsed: 00:02:18
                               ETA: 00:32:37

################################################################################
                      [1m Learning iteration 33/500 [0m                       

                       Computation: 4553 steps/s (collection: 3.500s, learning 0.099s)
             Mean action noise std: 0.81
          Mean value_function loss: 0.0193
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 14.5307
                       Mean reward: 0.19
               Mean episode length: 5.26
Episode_Reward/track_lin_vel_xy_exp: 0.0151
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0014
      Episode_Reward/ang_vel_xy_l2: -0.0019
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0041
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0001
    Episode_Reward/foot_contact_l2: -0.0024
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 557056
                    Iteration time: 3.60s
                      Time elapsed: 00:02:21
                               ETA: 00:32:25

################################################################################
                      [1m Learning iteration 34/500 [0m                       

                       Computation: 4484 steps/s (collection: 3.552s, learning 0.101s)
             Mean action noise std: 0.80
          Mean value_function loss: 0.0180
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 14.4027
                       Mean reward: 0.23
               Mean episode length: 5.28
Episode_Reward/track_lin_vel_xy_exp: 0.0150
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0014
      Episode_Reward/ang_vel_xy_l2: -0.0018
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0038
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0001
    Episode_Reward/foot_contact_l2: -0.0024
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 573440
                    Iteration time: 3.65s
                      Time elapsed: 00:02:25
                               ETA: 00:32:14

################################################################################
                      [1m Learning iteration 35/500 [0m                       

                       Computation: 4487 steps/s (collection: 3.557s, learning 0.094s)
             Mean action noise std: 0.79
          Mean value_function loss: 0.0186
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 14.2620
                       Mean reward: 0.23
               Mean episode length: 5.24
Episode_Reward/track_lin_vel_xy_exp: 0.0153
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0015
      Episode_Reward/ang_vel_xy_l2: -0.0018
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0038
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0001
    Episode_Reward/foot_contact_l2: -0.0024
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 3.65s
                      Time elapsed: 00:02:28
                               ETA: 00:32:03

################################################################################
                      [1m Learning iteration 36/500 [0m                       

                       Computation: 4566 steps/s (collection: 3.497s, learning 0.091s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0145
               Mean surrogate loss: -0.0152
                 Mean entropy loss: 14.1112
                       Mean reward: 0.24
               Mean episode length: 5.10
Episode_Reward/track_lin_vel_xy_exp: 0.0153
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0015
      Episode_Reward/ang_vel_xy_l2: -0.0018
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0037
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0001
    Episode_Reward/foot_contact_l2: -0.0024
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 606208
                    Iteration time: 3.59s
                      Time elapsed: 00:02:32
                               ETA: 00:31:52

################################################################################
                      [1m Learning iteration 37/500 [0m                       

                       Computation: 4551 steps/s (collection: 3.501s, learning 0.099s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0175
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 14.0108
                       Mean reward: 0.23
               Mean episode length: 5.14
Episode_Reward/track_lin_vel_xy_exp: 0.0155
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0015
      Episode_Reward/ang_vel_xy_l2: -0.0018
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0038
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0001
    Episode_Reward/foot_contact_l2: -0.0024
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 622592
                    Iteration time: 3.60s
                      Time elapsed: 00:02:36
                               ETA: 00:31:42

################################################################################
                      [1m Learning iteration 38/500 [0m                       

                       Computation: 4542 steps/s (collection: 3.508s, learning 0.098s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0205
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.8488
                       Mean reward: 0.25
               Mean episode length: 5.35
Episode_Reward/track_lin_vel_xy_exp: 0.0158
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0015
      Episode_Reward/ang_vel_xy_l2: -0.0018
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0038
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0001
    Episode_Reward/foot_contact_l2: -0.0024
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 638976
                    Iteration time: 3.61s
                      Time elapsed: 00:02:39
                               ETA: 00:31:32

################################################################################
                      [1m Learning iteration 39/500 [0m                       

                       Computation: 4551 steps/s (collection: 3.504s, learning 0.095s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 13.6654
                       Mean reward: 0.23
               Mean episode length: 5.20
Episode_Reward/track_lin_vel_xy_exp: 0.0158
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0015
      Episode_Reward/ang_vel_xy_l2: -0.0018
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0038
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0001
    Episode_Reward/foot_contact_l2: -0.0024
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 655360
                    Iteration time: 3.60s
                      Time elapsed: 00:02:43
                               ETA: 00:31:22

################################################################################
                      [1m Learning iteration 40/500 [0m                       

                       Computation: 4550 steps/s (collection: 3.505s, learning 0.095s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 13.4802
                       Mean reward: 0.26
               Mean episode length: 5.56
Episode_Reward/track_lin_vel_xy_exp: 0.0159
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0015
      Episode_Reward/ang_vel_xy_l2: -0.0018
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0036
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0001
    Episode_Reward/foot_contact_l2: -0.0024
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 671744
                    Iteration time: 3.60s
                      Time elapsed: 00:02:46
                               ETA: 00:31:12

################################################################################
                      [1m Learning iteration 41/500 [0m                       

                       Computation: 4493 steps/s (collection: 3.550s, learning 0.096s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 13.3311
                       Mean reward: 0.27
               Mean episode length: 5.37
Episode_Reward/track_lin_vel_xy_exp: 0.0160
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0015
      Episode_Reward/ang_vel_xy_l2: -0.0018
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0036
     Episode_Reward/action_rate_l2: -0.0002
Episode_Reward/flat_orientation_l2: -0.0001
    Episode_Reward/foot_contact_l2: -0.0024
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 3.65s
                      Time elapsed: 00:02:50
                               ETA: 00:31:04

################################################################################
                      [1m Learning iteration 42/500 [0m                       

                       Computation: 4531 steps/s (collection: 3.521s, learning 0.094s)
             Mean action noise std: 0.72
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.1751
                       Mean reward: 0.28
               Mean episode length: 5.42
Episode_Reward/track_lin_vel_xy_exp: 0.0163
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0015
      Episode_Reward/ang_vel_xy_l2: -0.0018
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0037
     Episode_Reward/action_rate_l2: -0.0002
Episode_Reward/flat_orientation_l2: -0.0001
    Episode_Reward/foot_contact_l2: -0.0024
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 704512
                    Iteration time: 3.62s
                      Time elapsed: 00:02:54
                               ETA: 00:30:55

################################################################################
                      [1m Learning iteration 43/500 [0m                       

                       Computation: 4542 steps/s (collection: 3.510s, learning 0.097s)
             Mean action noise std: 0.71
          Mean value_function loss: 0.0128
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 12.9836
                       Mean reward: 0.28
               Mean episode length: 5.41
Episode_Reward/track_lin_vel_xy_exp: 0.0164
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0015
      Episode_Reward/ang_vel_xy_l2: -0.0018
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0036
     Episode_Reward/action_rate_l2: -0.0002
Episode_Reward/flat_orientation_l2: -0.0001
    Episode_Reward/foot_contact_l2: -0.0025
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 720896
                    Iteration time: 3.61s
                      Time elapsed: 00:02:57
                               ETA: 00:30:46

################################################################################
                      [1m Learning iteration 44/500 [0m                       

                       Computation: 4542 steps/s (collection: 3.513s, learning 0.093s)
             Mean action noise std: 0.70
          Mean value_function loss: 0.0134
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 12.8249
                       Mean reward: 0.29
               Mean episode length: 5.47
Episode_Reward/track_lin_vel_xy_exp: 0.0166
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0015
      Episode_Reward/ang_vel_xy_l2: -0.0017
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0036
     Episode_Reward/action_rate_l2: -0.0002
Episode_Reward/flat_orientation_l2: -0.0001
    Episode_Reward/foot_contact_l2: -0.0025
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 737280
                    Iteration time: 3.61s
                      Time elapsed: 00:03:01
                               ETA: 00:30:38

################################################################################
                      [1m Learning iteration 45/500 [0m                       

                       Computation: 4546 steps/s (collection: 3.508s, learning 0.096s)
             Mean action noise std: 0.69
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 12.6591
                       Mean reward: 0.30
               Mean episode length: 5.60
Episode_Reward/track_lin_vel_xy_exp: 0.0169
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0015
      Episode_Reward/ang_vel_xy_l2: -0.0017
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0037
     Episode_Reward/action_rate_l2: -0.0002
Episode_Reward/flat_orientation_l2: -0.0001
    Episode_Reward/foot_contact_l2: -0.0025
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 753664
                    Iteration time: 3.60s
                      Time elapsed: 00:03:04
                               ETA: 00:30:29

################################################################################
                      [1m Learning iteration 46/500 [0m                       

                       Computation: 4529 steps/s (collection: 3.522s, learning 0.096s)
             Mean action noise std: 0.68
          Mean value_function loss: 0.0187
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 12.4696
                       Mean reward: 0.30
               Mean episode length: 5.64
Episode_Reward/track_lin_vel_xy_exp: 0.0171
Episode_Reward/track_ang_vel_z_exp: 0.0003
       Episode_Reward/lin_vel_z_l2: -0.0015
      Episode_Reward/ang_vel_xy_l2: -0.0018
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0038
     Episode_Reward/action_rate_l2: -0.0002
Episode_Reward/flat_orientation_l2: -0.0001
    Episode_Reward/foot_contact_l2: -0.0025
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 770048
                    Iteration time: 3.62s
                      Time elapsed: 00:03:08
                               ETA: 00:30:21

################################################################################
                      [1m Learning iteration 47/500 [0m                       

                       Computation: 4527 steps/s (collection: 3.522s, learning 0.097s)
             Mean action noise std: 0.67
          Mean value_function loss: 0.0160
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 12.3224
                       Mean reward: 0.31
               Mean episode length: 5.73
Episode_Reward/track_lin_vel_xy_exp: 0.0174
Episode_Reward/track_ang_vel_z_exp: 0.0002
       Episode_Reward/lin_vel_z_l2: -0.0015
      Episode_Reward/ang_vel_xy_l2: -0.0018
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0036
     Episode_Reward/action_rate_l2: -0.0002
Episode_Reward/flat_orientation_l2: -0.0001
    Episode_Reward/foot_contact_l2: -0.0026
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 3.62s
                      Time elapsed: 00:03:12
                               ETA: 00:30:14

################################################################################
                      [1m Learning iteration 48/500 [0m                       

                       Computation: 4521 steps/s (collection: 3.526s, learning 0.097s)
             Mean action noise std: 0.66
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 12.1542
                       Mean reward: 0.34
               Mean episode length: 5.73
Episode_Reward/track_lin_vel_xy_exp: 0.0175
Episode_Reward/track_ang_vel_z_exp: 0.0003
       Episode_Reward/lin_vel_z_l2: -0.0015
      Episode_Reward/ang_vel_xy_l2: -0.0018
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0036
     Episode_Reward/action_rate_l2: -0.0002
Episode_Reward/flat_orientation_l2: -0.0001
    Episode_Reward/foot_contact_l2: -0.0026
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 802816
                    Iteration time: 3.62s
                      Time elapsed: 00:03:15
                               ETA: 00:30:06

################################################################################
                      [1m Learning iteration 49/500 [0m                       

                       Computation: 4534 steps/s (collection: 3.520s, learning 0.093s)
             Mean action noise std: 0.66
          Mean value_function loss: 0.0203
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 12.0133
                       Mean reward: 0.36
               Mean episode length: 5.93
Episode_Reward/track_lin_vel_xy_exp: 0.0178
Episode_Reward/track_ang_vel_z_exp: 0.0003
       Episode_Reward/lin_vel_z_l2: -0.0015
      Episode_Reward/ang_vel_xy_l2: -0.0018
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0038
     Episode_Reward/action_rate_l2: -0.0002
Episode_Reward/flat_orientation_l2: -0.0001
    Episode_Reward/foot_contact_l2: -0.0026
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 819200
                    Iteration time: 3.61s
                      Time elapsed: 00:03:19
                               ETA: 00:29:59

################################################################################
                      [1m Learning iteration 50/500 [0m                       

                       Computation: 4531 steps/s (collection: 3.520s, learning 0.096s)
             Mean action noise std: 0.65
          Mean value_function loss: 0.0181
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 11.8621
                       Mean reward: 0.32
               Mean episode length: 6.20
Episode_Reward/track_lin_vel_xy_exp: 0.0182
Episode_Reward/track_ang_vel_z_exp: 0.0003
       Episode_Reward/lin_vel_z_l2: -0.0015
      Episode_Reward/ang_vel_xy_l2: -0.0018
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0038
     Episode_Reward/action_rate_l2: -0.0002
Episode_Reward/flat_orientation_l2: -0.0001
    Episode_Reward/foot_contact_l2: -0.0027
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 835584
                    Iteration time: 3.62s
                      Time elapsed: 00:03:23
                               ETA: 00:29:51

################################################################################
                      [1m Learning iteration 51/500 [0m                       

                       Computation: 4533 steps/s (collection: 3.522s, learning 0.092s)
             Mean action noise std: 0.64
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 11.7208
                       Mean reward: 0.34
               Mean episode length: 6.15
Episode_Reward/track_lin_vel_xy_exp: 0.0186
Episode_Reward/track_ang_vel_z_exp: 0.0003
       Episode_Reward/lin_vel_z_l2: -0.0015
      Episode_Reward/ang_vel_xy_l2: -0.0019
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0038
     Episode_Reward/action_rate_l2: -0.0002
Episode_Reward/flat_orientation_l2: -0.0001
    Episode_Reward/foot_contact_l2: -0.0028
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 851968
                    Iteration time: 3.61s
                      Time elapsed: 00:03:26
                               ETA: 00:29:44

################################################################################
                      [1m Learning iteration 52/500 [0m                       

                       Computation: 4548 steps/s (collection: 3.510s, learning 0.092s)
             Mean action noise std: 0.64
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 11.6018
                       Mean reward: 0.31
               Mean episode length: 6.14
Episode_Reward/track_lin_vel_xy_exp: 0.0190
Episode_Reward/track_ang_vel_z_exp: 0.0003
       Episode_Reward/lin_vel_z_l2: -0.0015
      Episode_Reward/ang_vel_xy_l2: -0.0019
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0039
     Episode_Reward/action_rate_l2: -0.0002
Episode_Reward/flat_orientation_l2: -0.0001
    Episode_Reward/foot_contact_l2: -0.0028
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 868352
                    Iteration time: 3.60s
                      Time elapsed: 00:03:30
                               ETA: 00:29:37

################################################################################
                      [1m Learning iteration 53/500 [0m                       

                       Computation: 4533 steps/s (collection: 3.519s, learning 0.094s)
             Mean action noise std: 0.63
          Mean value_function loss: 0.0176
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 11.5062
                       Mean reward: 0.38
               Mean episode length: 6.18
Episode_Reward/track_lin_vel_xy_exp: 0.0194
Episode_Reward/track_ang_vel_z_exp: 0.0003
       Episode_Reward/lin_vel_z_l2: -0.0015
      Episode_Reward/ang_vel_xy_l2: -0.0020
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0039
     Episode_Reward/action_rate_l2: -0.0002
Episode_Reward/flat_orientation_l2: -0.0001
    Episode_Reward/foot_contact_l2: -0.0029
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 3.61s
                      Time elapsed: 00:03:33
                               ETA: 00:29:30

################################################################################
                      [1m Learning iteration 54/500 [0m                       

                       Computation: 4523 steps/s (collection: 3.529s, learning 0.093s)
             Mean action noise std: 0.63
          Mean value_function loss: 0.0298
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 11.4221
                       Mean reward: 0.39
               Mean episode length: 6.55
Episode_Reward/track_lin_vel_xy_exp: 0.0198
Episode_Reward/track_ang_vel_z_exp: 0.0003
       Episode_Reward/lin_vel_z_l2: -0.0015
      Episode_Reward/ang_vel_xy_l2: -0.0020
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0040
     Episode_Reward/action_rate_l2: -0.0002
Episode_Reward/flat_orientation_l2: -0.0001
    Episode_Reward/foot_contact_l2: -0.0029
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 901120
                    Iteration time: 3.62s
                      Time elapsed: 00:03:37
                               ETA: 00:29:24

################################################################################
                      [1m Learning iteration 55/500 [0m                       

                       Computation: 4416 steps/s (collection: 3.541s, learning 0.169s)
             Mean action noise std: 0.62
          Mean value_function loss: 0.0189
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 11.3397
                       Mean reward: 0.38
               Mean episode length: 6.72
Episode_Reward/track_lin_vel_xy_exp: 0.0206
Episode_Reward/track_ang_vel_z_exp: 0.0003
       Episode_Reward/lin_vel_z_l2: -0.0014
      Episode_Reward/ang_vel_xy_l2: -0.0021
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0043
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0001
    Episode_Reward/foot_contact_l2: -0.0031
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 917504
                    Iteration time: 3.71s
                      Time elapsed: 00:03:41
                               ETA: 00:29:18

################################################################################
                      [1m Learning iteration 56/500 [0m                       

                       Computation: 4517 steps/s (collection: 3.529s, learning 0.098s)
             Mean action noise std: 0.62
          Mean value_function loss: 0.0174
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 11.2624
                       Mean reward: 0.40
               Mean episode length: 6.97
Episode_Reward/track_lin_vel_xy_exp: 0.0210
Episode_Reward/track_ang_vel_z_exp: 0.0003
       Episode_Reward/lin_vel_z_l2: -0.0014
      Episode_Reward/ang_vel_xy_l2: -0.0021
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0044
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0001
    Episode_Reward/foot_contact_l2: -0.0031
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 933888
                    Iteration time: 3.63s
                      Time elapsed: 00:03:44
                               ETA: 00:29:11

################################################################################
                      [1m Learning iteration 57/500 [0m                       

                       Computation: 4508 steps/s (collection: 3.516s, learning 0.118s)
             Mean action noise std: 0.62
          Mean value_function loss: 0.0168
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 11.1959
                       Mean reward: 0.40
               Mean episode length: 7.50
Episode_Reward/track_lin_vel_xy_exp: 0.0216
Episode_Reward/track_ang_vel_z_exp: 0.0003
       Episode_Reward/lin_vel_z_l2: -0.0014
      Episode_Reward/ang_vel_xy_l2: -0.0022
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0046
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0001
    Episode_Reward/foot_contact_l2: -0.0032
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 950272
                    Iteration time: 3.63s
                      Time elapsed: 00:03:48
                               ETA: 00:29:05

################################################################################
                      [1m Learning iteration 58/500 [0m                       

                       Computation: 4542 steps/s (collection: 3.512s, learning 0.095s)
             Mean action noise std: 0.61
          Mean value_function loss: 0.0177
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 11.1190
                       Mean reward: 0.43
               Mean episode length: 7.64
Episode_Reward/track_lin_vel_xy_exp: 0.0224
Episode_Reward/track_ang_vel_z_exp: 0.0003
       Episode_Reward/lin_vel_z_l2: -0.0014
      Episode_Reward/ang_vel_xy_l2: -0.0023
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0047
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0002
    Episode_Reward/foot_contact_l2: -0.0034
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 966656
                    Iteration time: 3.61s
                      Time elapsed: 00:03:52
                               ETA: 00:28:58

################################################################################
                      [1m Learning iteration 59/500 [0m                       

                       Computation: 4549 steps/s (collection: 3.506s, learning 0.095s)
             Mean action noise std: 0.61
          Mean value_function loss: 0.0214
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 11.0134
                       Mean reward: 0.43
               Mean episode length: 7.68
Episode_Reward/track_lin_vel_xy_exp: 0.0229
Episode_Reward/track_ang_vel_z_exp: 0.0003
       Episode_Reward/lin_vel_z_l2: -0.0014
      Episode_Reward/ang_vel_xy_l2: -0.0023
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0049
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0002
    Episode_Reward/foot_contact_l2: -0.0035
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 3.60s
                      Time elapsed: 00:03:55
                               ETA: 00:28:52

################################################################################
                      [1m Learning iteration 60/500 [0m                       

                       Computation: 4531 steps/s (collection: 3.520s, learning 0.095s)
             Mean action noise std: 0.60
          Mean value_function loss: 0.0338
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 10.9267
                       Mean reward: 0.46
               Mean episode length: 8.22
Episode_Reward/track_lin_vel_xy_exp: 0.0239
Episode_Reward/track_ang_vel_z_exp: 0.0004
       Episode_Reward/lin_vel_z_l2: -0.0014
      Episode_Reward/ang_vel_xy_l2: -0.0024
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0053
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0002
    Episode_Reward/foot_contact_l2: -0.0036
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 999424
                    Iteration time: 3.62s
                      Time elapsed: 00:03:59
                               ETA: 00:28:46

################################################################################
                      [1m Learning iteration 61/500 [0m                       

                       Computation: 4512 steps/s (collection: 3.532s, learning 0.099s)
             Mean action noise std: 0.60
          Mean value_function loss: 0.0273
               Mean surrogate loss: -0.0094
                 Mean entropy loss: 10.8869
                       Mean reward: 0.48
               Mean episode length: 8.39
Episode_Reward/track_lin_vel_xy_exp: 0.0246
Episode_Reward/track_ang_vel_z_exp: 0.0004
       Episode_Reward/lin_vel_z_l2: -0.0014
      Episode_Reward/ang_vel_xy_l2: -0.0025
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0054
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0002
    Episode_Reward/foot_contact_l2: -0.0037
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1015808
                    Iteration time: 3.63s
                      Time elapsed: 00:04:02
                               ETA: 00:28:40

################################################################################
                      [1m Learning iteration 62/500 [0m                       

                       Computation: 4524 steps/s (collection: 3.522s, learning 0.099s)
             Mean action noise std: 0.60
          Mean value_function loss: 0.0276
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 10.8421
                       Mean reward: 0.47
               Mean episode length: 8.78
Episode_Reward/track_lin_vel_xy_exp: 0.0253
Episode_Reward/track_ang_vel_z_exp: 0.0004
       Episode_Reward/lin_vel_z_l2: -0.0014
      Episode_Reward/ang_vel_xy_l2: -0.0025
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0055
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0002
    Episode_Reward/foot_contact_l2: -0.0038
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1032192
                    Iteration time: 3.62s
                      Time elapsed: 00:04:06
                               ETA: 00:28:34

################################################################################
                      [1m Learning iteration 63/500 [0m                       

                       Computation: 4533 steps/s (collection: 3.514s, learning 0.099s)
             Mean action noise std: 0.59
          Mean value_function loss: 0.0286
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 10.7517
                       Mean reward: 0.51
               Mean episode length: 9.30
Episode_Reward/track_lin_vel_xy_exp: 0.0262
Episode_Reward/track_ang_vel_z_exp: 0.0004
       Episode_Reward/lin_vel_z_l2: -0.0014
      Episode_Reward/ang_vel_xy_l2: -0.0026
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0057
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0002
    Episode_Reward/foot_contact_l2: -0.0040
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1048576
                    Iteration time: 3.61s
                      Time elapsed: 00:04:10
                               ETA: 00:28:28

################################################################################
                      [1m Learning iteration 64/500 [0m                       

                       Computation: 4541 steps/s (collection: 3.509s, learning 0.099s)
             Mean action noise std: 0.59
          Mean value_function loss: 0.0259
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 10.6474
                       Mean reward: 0.48
               Mean episode length: 9.04
Episode_Reward/track_lin_vel_xy_exp: 0.0275
Episode_Reward/track_ang_vel_z_exp: 0.0004
       Episode_Reward/lin_vel_z_l2: -0.0014
      Episode_Reward/ang_vel_xy_l2: -0.0028
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0063
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0002
    Episode_Reward/foot_contact_l2: -0.0042
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1064960
                    Iteration time: 3.61s
                      Time elapsed: 00:04:13
                               ETA: 00:28:22

################################################################################
                      [1m Learning iteration 65/500 [0m                       

                       Computation: 4541 steps/s (collection: 3.514s, learning 0.094s)
             Mean action noise std: 0.58
          Mean value_function loss: 0.0384
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 10.5508
                       Mean reward: 0.56
               Mean episode length: 10.08
Episode_Reward/track_lin_vel_xy_exp: 0.0286
Episode_Reward/track_ang_vel_z_exp: 0.0004
       Episode_Reward/lin_vel_z_l2: -0.0014
      Episode_Reward/ang_vel_xy_l2: -0.0029
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0066
     Episode_Reward/action_rate_l2: -0.0003
Episode_Reward/flat_orientation_l2: -0.0002
    Episode_Reward/foot_contact_l2: -0.0044
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 3.61s
                      Time elapsed: 00:04:17
                               ETA: 00:28:16

################################################################################
                      [1m Learning iteration 66/500 [0m                       

                       Computation: 4487 steps/s (collection: 3.512s, learning 0.139s)
             Mean action noise std: 0.58
          Mean value_function loss: 0.0299
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 10.4396
                       Mean reward: 0.55
               Mean episode length: 10.70
Episode_Reward/track_lin_vel_xy_exp: 0.0301
Episode_Reward/track_ang_vel_z_exp: 0.0004
       Episode_Reward/lin_vel_z_l2: -0.0014
      Episode_Reward/ang_vel_xy_l2: -0.0030
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0069
     Episode_Reward/action_rate_l2: -0.0004
Episode_Reward/flat_orientation_l2: -0.0003
    Episode_Reward/foot_contact_l2: -0.0046
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1097728
                    Iteration time: 3.65s
                      Time elapsed: 00:04:21
                               ETA: 00:28:11

################################################################################
                      [1m Learning iteration 67/500 [0m                       

                       Computation: 4526 steps/s (collection: 3.526s, learning 0.094s)
             Mean action noise std: 0.57
          Mean value_function loss: 0.0302
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 10.3395
                       Mean reward: 0.66
               Mean episode length: 10.91
Episode_Reward/track_lin_vel_xy_exp: 0.0312
Episode_Reward/track_ang_vel_z_exp: 0.0005
       Episode_Reward/lin_vel_z_l2: -0.0015
      Episode_Reward/ang_vel_xy_l2: -0.0031
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0070
     Episode_Reward/action_rate_l2: -0.0004
Episode_Reward/flat_orientation_l2: -0.0003
    Episode_Reward/foot_contact_l2: -0.0048
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1114112
                    Iteration time: 3.62s
                      Time elapsed: 00:04:24
                               ETA: 00:28:05

################################################################################
                      [1m Learning iteration 68/500 [0m                       

                       Computation: 4430 steps/s (collection: 3.598s, learning 0.100s)
             Mean action noise std: 0.57
          Mean value_function loss: 0.0333
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 10.2635
                       Mean reward: 0.62
               Mean episode length: 11.02
Episode_Reward/track_lin_vel_xy_exp: 0.0327
Episode_Reward/track_ang_vel_z_exp: 0.0005
       Episode_Reward/lin_vel_z_l2: -0.0015
      Episode_Reward/ang_vel_xy_l2: -0.0033
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0074
     Episode_Reward/action_rate_l2: -0.0004
Episode_Reward/flat_orientation_l2: -0.0003
    Episode_Reward/foot_contact_l2: -0.0051
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1130496
                    Iteration time: 3.70s
                      Time elapsed: 00:04:28
                               ETA: 00:28:00

################################################################################
                      [1m Learning iteration 69/500 [0m                       

                       Computation: 4503 steps/s (collection: 3.542s, learning 0.096s)
             Mean action noise std: 0.57
          Mean value_function loss: 0.0313
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 10.1923
                       Mean reward: 0.67
               Mean episode length: 12.34
Episode_Reward/track_lin_vel_xy_exp: 0.0340
Episode_Reward/track_ang_vel_z_exp: 0.0005
       Episode_Reward/lin_vel_z_l2: -0.0016
      Episode_Reward/ang_vel_xy_l2: -0.0034
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0077
     Episode_Reward/action_rate_l2: -0.0004
Episode_Reward/flat_orientation_l2: -0.0004
    Episode_Reward/foot_contact_l2: -0.0053
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1146880
                    Iteration time: 3.64s
                      Time elapsed: 00:04:32
                               ETA: 00:27:54

################################################################################
                      [1m Learning iteration 70/500 [0m                       

                       Computation: 4482 steps/s (collection: 3.560s, learning 0.095s)
             Mean action noise std: 0.56
          Mean value_function loss: 0.0335
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 10.1096
                       Mean reward: 0.63
               Mean episode length: 12.74
Episode_Reward/track_lin_vel_xy_exp: 0.0351
Episode_Reward/track_ang_vel_z_exp: 0.0005
       Episode_Reward/lin_vel_z_l2: -0.0016
      Episode_Reward/ang_vel_xy_l2: -0.0036
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0082
     Episode_Reward/action_rate_l2: -0.0004
Episode_Reward/flat_orientation_l2: -0.0004
    Episode_Reward/foot_contact_l2: -0.0056
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1163264
                    Iteration time: 3.66s
                      Time elapsed: 00:04:35
                               ETA: 00:27:49

################################################################################
                      [1m Learning iteration 71/500 [0m                       

                       Computation: 4533 steps/s (collection: 3.521s, learning 0.093s)
             Mean action noise std: 0.56
          Mean value_function loss: 0.0350
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 9.9918
                       Mean reward: 0.69
               Mean episode length: 12.71
Episode_Reward/track_lin_vel_xy_exp: 0.0360
Episode_Reward/track_ang_vel_z_exp: 0.0005
       Episode_Reward/lin_vel_z_l2: -0.0016
      Episode_Reward/ang_vel_xy_l2: -0.0036
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0084
     Episode_Reward/action_rate_l2: -0.0004
Episode_Reward/flat_orientation_l2: -0.0004
    Episode_Reward/foot_contact_l2: -0.0057
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 3.61s
                      Time elapsed: 00:04:39
                               ETA: 00:27:44

################################################################################
                      [1m Learning iteration 72/500 [0m                       

                       Computation: 4509 steps/s (collection: 3.538s, learning 0.094s)
             Mean action noise std: 0.55
          Mean value_function loss: 0.0402
               Mean surrogate loss: -0.0159
                 Mean entropy loss: 9.8734
                       Mean reward: 0.63
               Mean episode length: 13.62
Episode_Reward/track_lin_vel_xy_exp: 0.0377
Episode_Reward/track_ang_vel_z_exp: 0.0005
       Episode_Reward/lin_vel_z_l2: -0.0017
      Episode_Reward/ang_vel_xy_l2: -0.0038
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0094
     Episode_Reward/action_rate_l2: -0.0005
Episode_Reward/flat_orientation_l2: -0.0005
    Episode_Reward/foot_contact_l2: -0.0060
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1196032
                    Iteration time: 3.63s
                      Time elapsed: 00:04:42
                               ETA: 00:27:38

################################################################################
                      [1m Learning iteration 73/500 [0m                       

                       Computation: 4530 steps/s (collection: 3.520s, learning 0.096s)
             Mean action noise std: 0.55
          Mean value_function loss: 0.0412
               Mean surrogate loss: -0.0154
                 Mean entropy loss: 9.7662
                       Mean reward: 0.78
               Mean episode length: 13.97
Episode_Reward/track_lin_vel_xy_exp: 0.0397
Episode_Reward/track_ang_vel_z_exp: 0.0006
       Episode_Reward/lin_vel_z_l2: -0.0018
      Episode_Reward/ang_vel_xy_l2: -0.0038
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0094
     Episode_Reward/action_rate_l2: -0.0005
Episode_Reward/flat_orientation_l2: -0.0005
    Episode_Reward/foot_contact_l2: -0.0063
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1212416
                    Iteration time: 3.62s
                      Time elapsed: 00:04:46
                               ETA: 00:27:33

################################################################################
                      [1m Learning iteration 74/500 [0m                       

                       Computation: 4523 steps/s (collection: 3.524s, learning 0.098s)
             Mean action noise std: 0.54
          Mean value_function loss: 0.0475
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 9.6555
                       Mean reward: 0.75
               Mean episode length: 14.39
Episode_Reward/track_lin_vel_xy_exp: 0.0409
Episode_Reward/track_ang_vel_z_exp: 0.0006
       Episode_Reward/lin_vel_z_l2: -0.0018
      Episode_Reward/ang_vel_xy_l2: -0.0041
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0099
     Episode_Reward/action_rate_l2: -0.0005
Episode_Reward/flat_orientation_l2: -0.0005
    Episode_Reward/foot_contact_l2: -0.0066
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1228800
                    Iteration time: 3.62s
                      Time elapsed: 00:04:50
                               ETA: 00:27:28

################################################################################
                      [1m Learning iteration 75/500 [0m                       

                       Computation: 4519 steps/s (collection: 3.530s, learning 0.096s)
             Mean action noise std: 0.54
          Mean value_function loss: 0.0489
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 9.5784
                       Mean reward: 0.70
               Mean episode length: 15.94
Episode_Reward/track_lin_vel_xy_exp: 0.0420
Episode_Reward/track_ang_vel_z_exp: 0.0006
       Episode_Reward/lin_vel_z_l2: -0.0018
      Episode_Reward/ang_vel_xy_l2: -0.0040
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0103
     Episode_Reward/action_rate_l2: -0.0005
Episode_Reward/flat_orientation_l2: -0.0006
    Episode_Reward/foot_contact_l2: -0.0068
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1245184
                    Iteration time: 3.63s
                      Time elapsed: 00:04:53
                               ETA: 00:27:22

################################################################################
                      [1m Learning iteration 76/500 [0m                       

                       Computation: 4497 steps/s (collection: 3.550s, learning 0.093s)
             Mean action noise std: 0.53
          Mean value_function loss: 0.0479
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 9.4827
                       Mean reward: 0.75
               Mean episode length: 16.04
Episode_Reward/track_lin_vel_xy_exp: 0.0433
Episode_Reward/track_ang_vel_z_exp: 0.0006
       Episode_Reward/lin_vel_z_l2: -0.0018
      Episode_Reward/ang_vel_xy_l2: -0.0041
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0105
     Episode_Reward/action_rate_l2: -0.0005
Episode_Reward/flat_orientation_l2: -0.0006
    Episode_Reward/foot_contact_l2: -0.0070
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1261568
                    Iteration time: 3.64s
                      Time elapsed: 00:04:57
                               ETA: 00:27:17

################################################################################
                      [1m Learning iteration 77/500 [0m                       

                       Computation: 4522 steps/s (collection: 3.521s, learning 0.102s)
             Mean action noise std: 0.53
          Mean value_function loss: 0.0515
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 9.3618
                       Mean reward: 0.78
               Mean episode length: 17.65
Episode_Reward/track_lin_vel_xy_exp: 0.0464
Episode_Reward/track_ang_vel_z_exp: 0.0007
       Episode_Reward/lin_vel_z_l2: -0.0019
      Episode_Reward/ang_vel_xy_l2: -0.0045
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0116
     Episode_Reward/action_rate_l2: -0.0006
Episode_Reward/flat_orientation_l2: -0.0007
    Episode_Reward/foot_contact_l2: -0.0076
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 3.62s
                      Time elapsed: 00:05:01
                               ETA: 00:27:12

################################################################################
                      [1m Learning iteration 78/500 [0m                       

                       Computation: 4533 steps/s (collection: 3.518s, learning 0.097s)
             Mean action noise std: 0.53
          Mean value_function loss: 0.0554
               Mean surrogate loss: -0.0098
                 Mean entropy loss: 9.2739
                       Mean reward: 0.86
               Mean episode length: 17.77
Episode_Reward/track_lin_vel_xy_exp: 0.0476
Episode_Reward/track_ang_vel_z_exp: 0.0007
       Episode_Reward/lin_vel_z_l2: -0.0020
      Episode_Reward/ang_vel_xy_l2: -0.0046
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0121
     Episode_Reward/action_rate_l2: -0.0006
Episode_Reward/flat_orientation_l2: -0.0007
    Episode_Reward/foot_contact_l2: -0.0079
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1294336
                    Iteration time: 3.61s
                      Time elapsed: 00:05:04
                               ETA: 00:27:07

################################################################################
                      [1m Learning iteration 79/500 [0m                       

                       Computation: 4540 steps/s (collection: 3.510s, learning 0.099s)
             Mean action noise std: 0.52
          Mean value_function loss: 0.0482
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 9.1956
                       Mean reward: 0.85
               Mean episode length: 18.33
Episode_Reward/track_lin_vel_xy_exp: 0.0496
Episode_Reward/track_ang_vel_z_exp: 0.0007
       Episode_Reward/lin_vel_z_l2: -0.0020
      Episode_Reward/ang_vel_xy_l2: -0.0047
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0127
     Episode_Reward/action_rate_l2: -0.0006
Episode_Reward/flat_orientation_l2: -0.0007
    Episode_Reward/foot_contact_l2: -0.0081
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1310720
                    Iteration time: 3.61s
                      Time elapsed: 00:05:08
                               ETA: 00:27:02

################################################################################
                      [1m Learning iteration 80/500 [0m                       

                       Computation: 4536 steps/s (collection: 3.513s, learning 0.099s)
             Mean action noise std: 0.52
          Mean value_function loss: 0.0575
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 9.1335
                       Mean reward: 0.89
               Mean episode length: 19.96
Episode_Reward/track_lin_vel_xy_exp: 0.0515
Episode_Reward/track_ang_vel_z_exp: 0.0008
       Episode_Reward/lin_vel_z_l2: -0.0021
      Episode_Reward/ang_vel_xy_l2: -0.0050
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0131
     Episode_Reward/action_rate_l2: -0.0006
Episode_Reward/flat_orientation_l2: -0.0008
    Episode_Reward/foot_contact_l2: -0.0086
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1327104
                    Iteration time: 3.61s
                      Time elapsed: 00:05:11
                               ETA: 00:26:57

################################################################################
                      [1m Learning iteration 81/500 [0m                       

                       Computation: 4552 steps/s (collection: 3.505s, learning 0.094s)
             Mean action noise std: 0.52
          Mean value_function loss: 0.0541
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 9.0754
                       Mean reward: 0.92
               Mean episode length: 19.39
Episode_Reward/track_lin_vel_xy_exp: 0.0541
Episode_Reward/track_ang_vel_z_exp: 0.0008
       Episode_Reward/lin_vel_z_l2: -0.0020
      Episode_Reward/ang_vel_xy_l2: -0.0051
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0137
     Episode_Reward/action_rate_l2: -0.0006
Episode_Reward/flat_orientation_l2: -0.0009
    Episode_Reward/foot_contact_l2: -0.0090
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1343488
                    Iteration time: 3.60s
                      Time elapsed: 00:05:15
                               ETA: 00:26:52

################################################################################
                      [1m Learning iteration 82/500 [0m                       

                       Computation: 4523 steps/s (collection: 3.522s, learning 0.100s)
             Mean action noise std: 0.52
          Mean value_function loss: 0.0586
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 9.0010
                       Mean reward: 1.01
               Mean episode length: 22.53
Episode_Reward/track_lin_vel_xy_exp: 0.0580
Episode_Reward/track_ang_vel_z_exp: 0.0009
       Episode_Reward/lin_vel_z_l2: -0.0021
      Episode_Reward/ang_vel_xy_l2: -0.0056
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0147
     Episode_Reward/action_rate_l2: -0.0007
Episode_Reward/flat_orientation_l2: -0.0012
    Episode_Reward/foot_contact_l2: -0.0098
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1359872
                    Iteration time: 3.62s
                      Time elapsed: 00:05:19
                               ETA: 00:26:47

################################################################################
                      [1m Learning iteration 83/500 [0m                       

                       Computation: 4530 steps/s (collection: 3.522s, learning 0.094s)
             Mean action noise std: 0.51
          Mean value_function loss: 0.0638
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 8.9480
                       Mean reward: 0.95
               Mean episode length: 23.59
Episode_Reward/track_lin_vel_xy_exp: 0.0587
Episode_Reward/track_ang_vel_z_exp: 0.0009
       Episode_Reward/lin_vel_z_l2: -0.0021
      Episode_Reward/ang_vel_xy_l2: -0.0059
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0154
     Episode_Reward/action_rate_l2: -0.0007
Episode_Reward/flat_orientation_l2: -0.0012
    Episode_Reward/foot_contact_l2: -0.0102
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 3.62s
                      Time elapsed: 00:05:22
                               ETA: 00:26:42

################################################################################
                      [1m Learning iteration 84/500 [0m                       

                       Computation: 4457 steps/s (collection: 3.574s, learning 0.102s)
             Mean action noise std: 0.51
          Mean value_function loss: 0.0730
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 8.9034
                       Mean reward: 1.01
               Mean episode length: 24.57
Episode_Reward/track_lin_vel_xy_exp: 0.0634
Episode_Reward/track_ang_vel_z_exp: 0.0010
       Episode_Reward/lin_vel_z_l2: -0.0022
      Episode_Reward/ang_vel_xy_l2: -0.0060
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0171
     Episode_Reward/action_rate_l2: -0.0007
Episode_Reward/flat_orientation_l2: -0.0012
    Episode_Reward/foot_contact_l2: -0.0110
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1392640
                    Iteration time: 3.68s
                      Time elapsed: 00:05:26
                               ETA: 00:26:37

################################################################################
                      [1m Learning iteration 85/500 [0m                       

                       Computation: 4486 steps/s (collection: 3.557s, learning 0.094s)
             Mean action noise std: 0.51
          Mean value_function loss: 0.0791
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 8.8261
                       Mean reward: 0.96
               Mean episode length: 25.29
Episode_Reward/track_lin_vel_xy_exp: 0.0636
Episode_Reward/track_ang_vel_z_exp: 0.0010
       Episode_Reward/lin_vel_z_l2: -0.0022
      Episode_Reward/ang_vel_xy_l2: -0.0063
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0175
     Episode_Reward/action_rate_l2: -0.0008
Episode_Reward/flat_orientation_l2: -0.0012
    Episode_Reward/foot_contact_l2: -0.0113
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1409024
                    Iteration time: 3.65s
                      Time elapsed: 00:05:30
                               ETA: 00:26:32

################################################################################
                      [1m Learning iteration 86/500 [0m                       

                       Computation: 4519 steps/s (collection: 3.531s, learning 0.095s)
             Mean action noise std: 0.50
          Mean value_function loss: 0.0882
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 8.7386
                       Mean reward: 1.08
               Mean episode length: 29.03
Episode_Reward/track_lin_vel_xy_exp: 0.0694
Episode_Reward/track_ang_vel_z_exp: 0.0011
       Episode_Reward/lin_vel_z_l2: -0.0024
      Episode_Reward/ang_vel_xy_l2: -0.0069
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0195
     Episode_Reward/action_rate_l2: -0.0008
Episode_Reward/flat_orientation_l2: -0.0016
    Episode_Reward/foot_contact_l2: -0.0125
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1425408
                    Iteration time: 3.63s
                      Time elapsed: 00:05:33
                               ETA: 00:26:27

################################################################################
                      [1m Learning iteration 87/500 [0m                       

                       Computation: 4539 steps/s (collection: 3.516s, learning 0.094s)
             Mean action noise std: 0.50
          Mean value_function loss: 0.0746
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 8.6613
                       Mean reward: 1.21
               Mean episode length: 29.33
Episode_Reward/track_lin_vel_xy_exp: 0.0752
Episode_Reward/track_ang_vel_z_exp: 0.0012
       Episode_Reward/lin_vel_z_l2: -0.0024
      Episode_Reward/ang_vel_xy_l2: -0.0073
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0213
     Episode_Reward/action_rate_l2: -0.0009
Episode_Reward/flat_orientation_l2: -0.0018
    Episode_Reward/foot_contact_l2: -0.0135
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1441792
                    Iteration time: 3.61s
                      Time elapsed: 00:05:37
                               ETA: 00:26:22

################################################################################
                      [1m Learning iteration 88/500 [0m                       

                       Computation: 4494 steps/s (collection: 3.518s, learning 0.127s)
             Mean action noise std: 0.50
          Mean value_function loss: 0.0916
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 8.5842
                       Mean reward: 1.30
               Mean episode length: 34.84
Episode_Reward/track_lin_vel_xy_exp: 0.0791
Episode_Reward/track_ang_vel_z_exp: 0.0013
       Episode_Reward/lin_vel_z_l2: -0.0025
      Episode_Reward/ang_vel_xy_l2: -0.0079
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0226
     Episode_Reward/action_rate_l2: -0.0010
Episode_Reward/flat_orientation_l2: -0.0018
    Episode_Reward/foot_contact_l2: -0.0146
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1458176
                    Iteration time: 3.65s
                      Time elapsed: 00:05:40
                               ETA: 00:26:18

################################################################################
                      [1m Learning iteration 89/500 [0m                       

                       Computation: 4451 steps/s (collection: 3.586s, learning 0.094s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0922
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 8.5090
                       Mean reward: 1.34
               Mean episode length: 34.91
Episode_Reward/track_lin_vel_xy_exp: 0.0856
Episode_Reward/track_ang_vel_z_exp: 0.0015
       Episode_Reward/lin_vel_z_l2: -0.0026
      Episode_Reward/ang_vel_xy_l2: -0.0086
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0248
     Episode_Reward/action_rate_l2: -0.0010
Episode_Reward/flat_orientation_l2: -0.0021
    Episode_Reward/foot_contact_l2: -0.0158
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 3.68s
                      Time elapsed: 00:05:44
                               ETA: 00:26:13

################################################################################
                      [1m Learning iteration 90/500 [0m                       

                       Computation: 4472 steps/s (collection: 3.539s, learning 0.125s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0933
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 8.4244
                       Mean reward: 1.39
               Mean episode length: 38.80
Episode_Reward/track_lin_vel_xy_exp: 0.0932
Episode_Reward/track_ang_vel_z_exp: 0.0016
       Episode_Reward/lin_vel_z_l2: -0.0029
      Episode_Reward/ang_vel_xy_l2: -0.0094
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0276
     Episode_Reward/action_rate_l2: -0.0012
Episode_Reward/flat_orientation_l2: -0.0023
    Episode_Reward/foot_contact_l2: -0.0177
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1490944
                    Iteration time: 3.66s
                      Time elapsed: 00:05:48
                               ETA: 00:26:09

################################################################################
                      [1m Learning iteration 91/500 [0m                       

                       Computation: 4478 steps/s (collection: 3.558s, learning 0.101s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.1018
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 8.3338
                       Mean reward: 1.64
               Mean episode length: 45.02
Episode_Reward/track_lin_vel_xy_exp: 0.1032
Episode_Reward/track_ang_vel_z_exp: 0.0018
       Episode_Reward/lin_vel_z_l2: -0.0030
      Episode_Reward/ang_vel_xy_l2: -0.0101
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0291
     Episode_Reward/action_rate_l2: -0.0012
Episode_Reward/flat_orientation_l2: -0.0026
    Episode_Reward/foot_contact_l2: -0.0192
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1507328
                    Iteration time: 3.66s
                      Time elapsed: 00:05:51
                               ETA: 00:26:04

################################################################################
                      [1m Learning iteration 92/500 [0m                       

                       Computation: 4511 steps/s (collection: 3.521s, learning 0.111s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.1095
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 8.2544
                       Mean reward: 1.79
               Mean episode length: 48.94
Episode_Reward/track_lin_vel_xy_exp: 0.1133
Episode_Reward/track_ang_vel_z_exp: 0.0019
       Episode_Reward/lin_vel_z_l2: -0.0033
      Episode_Reward/ang_vel_xy_l2: -0.0111
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0343
     Episode_Reward/action_rate_l2: -0.0014
Episode_Reward/flat_orientation_l2: -0.0026
    Episode_Reward/foot_contact_l2: -0.0215
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1523712
                    Iteration time: 3.63s
                      Time elapsed: 00:05:55
                               ETA: 00:25:59

################################################################################
                      [1m Learning iteration 93/500 [0m                       

                       Computation: 4457 steps/s (collection: 3.555s, learning 0.120s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.1315
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 8.1813
                       Mean reward: 1.81
               Mean episode length: 53.54
Episode_Reward/track_lin_vel_xy_exp: 0.1252
Episode_Reward/track_ang_vel_z_exp: 0.0021
       Episode_Reward/lin_vel_z_l2: -0.0036
      Episode_Reward/ang_vel_xy_l2: -0.0126
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0370
     Episode_Reward/action_rate_l2: -0.0015
Episode_Reward/flat_orientation_l2: -0.0034
    Episode_Reward/foot_contact_l2: -0.0238
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1540096
                    Iteration time: 3.68s
                      Time elapsed: 00:05:59
                               ETA: 00:25:55

################################################################################
                      [1m Learning iteration 94/500 [0m                       

                       Computation: 4553 steps/s (collection: 3.500s, learning 0.098s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.1086
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 8.1372
                       Mean reward: 2.03
               Mean episode length: 59.70
Episode_Reward/track_lin_vel_xy_exp: 0.1336
Episode_Reward/track_ang_vel_z_exp: 0.0023
       Episode_Reward/lin_vel_z_l2: -0.0036
      Episode_Reward/ang_vel_xy_l2: -0.0126
     Episode_Reward/dof_torques_l2: -0.0000
         Episode_Reward/dof_acc_l2: -0.0396
     Episode_Reward/action_rate_l2: -0.0016
Episode_Reward/flat_orientation_l2: -0.0033
    Episode_Reward/foot_contact_l2: -0.0257
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1556480
                    Iteration time: 3.60s
                      Time elapsed: 00:06:02
                               ETA: 00:25:50

################################################################################
                      [1m Learning iteration 95/500 [0m                       

                       Computation: 4554 steps/s (collection: 3.503s, learning 0.093s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.1194
               Mean surrogate loss: -0.0084
                 Mean entropy loss: 8.1025
                       Mean reward: 2.40
               Mean episode length: 68.88
Episode_Reward/track_lin_vel_xy_exp: 0.1509
Episode_Reward/track_ang_vel_z_exp: 0.0027
       Episode_Reward/lin_vel_z_l2: -0.0041
      Episode_Reward/ang_vel_xy_l2: -0.0142
     Episode_Reward/dof_torques_l2: -0.0001
         Episode_Reward/dof_acc_l2: -0.0446
     Episode_Reward/action_rate_l2: -0.0018
Episode_Reward/flat_orientation_l2: -0.0034
    Episode_Reward/foot_contact_l2: -0.0290
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 3.60s
                      Time elapsed: 00:06:06
                               ETA: 00:25:45

################################################################################
                      [1m Learning iteration 96/500 [0m                       

                       Computation: 4548 steps/s (collection: 3.506s, learning 0.096s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.1156
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 8.0701
                       Mean reward: 2.64
               Mean episode length: 77.67
Episode_Reward/track_lin_vel_xy_exp: 0.1830
Episode_Reward/track_ang_vel_z_exp: 0.0033
       Episode_Reward/lin_vel_z_l2: -0.0048
      Episode_Reward/ang_vel_xy_l2: -0.0172
     Episode_Reward/dof_torques_l2: -0.0001
         Episode_Reward/dof_acc_l2: -0.0553
     Episode_Reward/action_rate_l2: -0.0022
Episode_Reward/flat_orientation_l2: -0.0042
    Episode_Reward/foot_contact_l2: -0.0354
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1589248
                    Iteration time: 3.60s
                      Time elapsed: 00:06:10
                               ETA: 00:25:41

################################################################################
                      [1m Learning iteration 97/500 [0m                       

                       Computation: 4538 steps/s (collection: 3.515s, learning 0.095s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.1190
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 8.0045
                       Mean reward: 2.70
               Mean episode length: 83.47
Episode_Reward/track_lin_vel_xy_exp: 0.1987
Episode_Reward/track_ang_vel_z_exp: 0.0037
       Episode_Reward/lin_vel_z_l2: -0.0052
      Episode_Reward/ang_vel_xy_l2: -0.0195
     Episode_Reward/dof_torques_l2: -0.0001
         Episode_Reward/dof_acc_l2: -0.0615
     Episode_Reward/action_rate_l2: -0.0024
Episode_Reward/flat_orientation_l2: -0.0052
    Episode_Reward/foot_contact_l2: -0.0394
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1605632
                    Iteration time: 3.61s
                      Time elapsed: 00:06:13
                               ETA: 00:25:36

################################################################################
                      [1m Learning iteration 98/500 [0m                       

                       Computation: 4534 steps/s (collection: 3.515s, learning 0.098s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.1149
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 7.9424
                       Mean reward: 2.89
               Mean episode length: 104.68
Episode_Reward/track_lin_vel_xy_exp: 0.2514
Episode_Reward/track_ang_vel_z_exp: 0.0049
       Episode_Reward/lin_vel_z_l2: -0.0071
      Episode_Reward/ang_vel_xy_l2: -0.0265
     Episode_Reward/dof_torques_l2: -0.0001
         Episode_Reward/dof_acc_l2: -0.0856
     Episode_Reward/action_rate_l2: -0.0033
Episode_Reward/flat_orientation_l2: -0.0072
    Episode_Reward/foot_contact_l2: -0.0528
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1622016
                    Iteration time: 3.61s
                      Time elapsed: 00:06:17
                               ETA: 00:25:31

################################################################################
                      [1m Learning iteration 99/500 [0m                       

                       Computation: 4563 steps/s (collection: 3.494s, learning 0.096s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.1315
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 7.8752
                       Mean reward: 3.39
               Mean episode length: 119.09
Episode_Reward/track_lin_vel_xy_exp: 0.2940
Episode_Reward/track_ang_vel_z_exp: 0.0057
       Episode_Reward/lin_vel_z_l2: -0.0075
      Episode_Reward/ang_vel_xy_l2: -0.0282
     Episode_Reward/dof_torques_l2: -0.0001
         Episode_Reward/dof_acc_l2: -0.0907
     Episode_Reward/action_rate_l2: -0.0037
Episode_Reward/flat_orientation_l2: -0.0072
    Episode_Reward/foot_contact_l2: -0.0595
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1638400
                    Iteration time: 3.59s
                      Time elapsed: 00:06:20
                               ETA: 00:25:27

################################################################################
                      [1m Learning iteration 100/500 [0m                      

                       Computation: 4597 steps/s (collection: 3.469s, learning 0.095s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.1357
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 7.7970
                       Mean reward: 3.67
               Mean episode length: 126.43
Episode_Reward/track_lin_vel_xy_exp: 0.2945
Episode_Reward/track_ang_vel_z_exp: 0.0056
       Episode_Reward/lin_vel_z_l2: -0.0087
      Episode_Reward/ang_vel_xy_l2: -0.0319
     Episode_Reward/dof_torques_l2: -0.0001
         Episode_Reward/dof_acc_l2: -0.0961
     Episode_Reward/action_rate_l2: -0.0040
Episode_Reward/flat_orientation_l2: -0.0077
    Episode_Reward/foot_contact_l2: -0.0627
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1654784
                    Iteration time: 3.56s
                      Time elapsed: 00:06:24
                               ETA: 00:25:22

################################################################################
                      [1m Learning iteration 101/500 [0m                      

                       Computation: 4419 steps/s (collection: 3.595s, learning 0.112s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.1203
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 7.7396
                       Mean reward: 3.78
               Mean episode length: 135.29
Episode_Reward/track_lin_vel_xy_exp: 0.3340
Episode_Reward/track_ang_vel_z_exp: 0.0064
       Episode_Reward/lin_vel_z_l2: -0.0099
      Episode_Reward/ang_vel_xy_l2: -0.0349
     Episode_Reward/dof_torques_l2: -0.0001
         Episode_Reward/dof_acc_l2: -0.1162
     Episode_Reward/action_rate_l2: -0.0043
Episode_Reward/flat_orientation_l2: -0.0084
    Episode_Reward/foot_contact_l2: -0.0703
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 3.71s
                      Time elapsed: 00:06:28
                               ETA: 00:25:18

################################################################################
                      [1m Learning iteration 102/500 [0m                      

                       Computation: 4538 steps/s (collection: 3.517s, learning 0.092s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.1246
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 7.6413
                       Mean reward: 4.01
               Mean episode length: 144.38
Episode_Reward/track_lin_vel_xy_exp: 0.4119
Episode_Reward/track_ang_vel_z_exp: 0.0083
       Episode_Reward/lin_vel_z_l2: -0.0109
      Episode_Reward/ang_vel_xy_l2: -0.0408
     Episode_Reward/dof_torques_l2: -0.0002
         Episode_Reward/dof_acc_l2: -0.1335
     Episode_Reward/action_rate_l2: -0.0052
Episode_Reward/flat_orientation_l2: -0.0137
    Episode_Reward/foot_contact_l2: -0.0842
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1687552
                    Iteration time: 3.61s
                      Time elapsed: 00:06:31
                               ETA: 00:25:13

################################################################################
                      [1m Learning iteration 103/500 [0m                      

                       Computation: 4608 steps/s (collection: 3.458s, learning 0.098s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.1164
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 7.5678
                       Mean reward: 4.40
               Mean episode length: 165.57
Episode_Reward/track_lin_vel_xy_exp: 0.5219
Episode_Reward/track_ang_vel_z_exp: 0.0095
       Episode_Reward/lin_vel_z_l2: -0.0149
      Episode_Reward/ang_vel_xy_l2: -0.0557
     Episode_Reward/dof_torques_l2: -0.0002
         Episode_Reward/dof_acc_l2: -0.1914
     Episode_Reward/action_rate_l2: -0.0070
Episode_Reward/flat_orientation_l2: -0.0172
    Episode_Reward/foot_contact_l2: -0.1142
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1703936
                    Iteration time: 3.56s
                      Time elapsed: 00:06:35
                               ETA: 00:25:08

################################################################################
                      [1m Learning iteration 104/500 [0m                      

                       Computation: 4627 steps/s (collection: 3.445s, learning 0.096s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.1124
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 7.4835
                       Mean reward: 4.65
               Mean episode length: 172.13
Episode_Reward/track_lin_vel_xy_exp: 0.5445
Episode_Reward/track_ang_vel_z_exp: 0.0100
       Episode_Reward/lin_vel_z_l2: -0.0123
      Episode_Reward/ang_vel_xy_l2: -0.0473
     Episode_Reward/dof_torques_l2: -0.0002
         Episode_Reward/dof_acc_l2: -0.1634
     Episode_Reward/action_rate_l2: -0.0065
Episode_Reward/flat_orientation_l2: -0.0130
    Episode_Reward/foot_contact_l2: -0.1060
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1720320
                    Iteration time: 3.54s
                      Time elapsed: 00:06:38
                               ETA: 00:25:04

################################################################################
                      [1m Learning iteration 105/500 [0m                      

                       Computation: 4584 steps/s (collection: 3.476s, learning 0.098s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.1107
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 7.4267
                       Mean reward: 5.07
               Mean episode length: 182.66
Episode_Reward/track_lin_vel_xy_exp: 0.5112
Episode_Reward/track_ang_vel_z_exp: 0.0105
       Episode_Reward/lin_vel_z_l2: -0.0120
      Episode_Reward/ang_vel_xy_l2: -0.0480
     Episode_Reward/dof_torques_l2: -0.0002
         Episode_Reward/dof_acc_l2: -0.1527
     Episode_Reward/action_rate_l2: -0.0062
Episode_Reward/flat_orientation_l2: -0.0122
    Episode_Reward/foot_contact_l2: -0.1045
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1736704
                    Iteration time: 3.57s
                      Time elapsed: 00:06:42
                               ETA: 00:24:59

################################################################################
                      [1m Learning iteration 106/500 [0m                      

                       Computation: 4648 steps/s (collection: 3.425s, learning 0.100s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.1050
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 7.3911
                       Mean reward: 5.32
               Mean episode length: 191.08
Episode_Reward/track_lin_vel_xy_exp: 0.6892
Episode_Reward/track_ang_vel_z_exp: 0.0142
       Episode_Reward/lin_vel_z_l2: -0.0141
      Episode_Reward/ang_vel_xy_l2: -0.0608
     Episode_Reward/dof_torques_l2: -0.0003
         Episode_Reward/dof_acc_l2: -0.2407
     Episode_Reward/action_rate_l2: -0.0090
Episode_Reward/flat_orientation_l2: -0.0154
    Episode_Reward/foot_contact_l2: -0.1491
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1753088
                    Iteration time: 3.52s
                      Time elapsed: 00:06:45
                               ETA: 00:24:54

################################################################################
                      [1m Learning iteration 107/500 [0m                      

                       Computation: 4633 steps/s (collection: 3.436s, learning 0.100s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.1056
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 7.3713
                       Mean reward: 5.85
               Mean episode length: 208.42
Episode_Reward/track_lin_vel_xy_exp: 0.9182
Episode_Reward/track_ang_vel_z_exp: 0.0149
       Episode_Reward/lin_vel_z_l2: -0.0204
      Episode_Reward/ang_vel_xy_l2: -0.0734
     Episode_Reward/dof_torques_l2: -0.0003
         Episode_Reward/dof_acc_l2: -0.2767
     Episode_Reward/action_rate_l2: -0.0106
Episode_Reward/flat_orientation_l2: -0.0158
    Episode_Reward/foot_contact_l2: -0.1756
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 3.54s
                      Time elapsed: 00:06:49
                               ETA: 00:24:49

################################################################################
                      [1m Learning iteration 108/500 [0m                      

                       Computation: 4655 steps/s (collection: 3.420s, learning 0.100s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.1039
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 7.3093
                       Mean reward: 6.19
               Mean episode length: 215.18
Episode_Reward/track_lin_vel_xy_exp: 0.9291
Episode_Reward/track_ang_vel_z_exp: 0.0186
       Episode_Reward/lin_vel_z_l2: -0.0238
      Episode_Reward/ang_vel_xy_l2: -0.0848
     Episode_Reward/dof_torques_l2: -0.0004
         Episode_Reward/dof_acc_l2: -0.2821
     Episode_Reward/action_rate_l2: -0.0118
Episode_Reward/flat_orientation_l2: -0.0201
    Episode_Reward/foot_contact_l2: -0.1998
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1785856
                    Iteration time: 3.52s
                      Time elapsed: 00:06:52
                               ETA: 00:24:45

################################################################################
                      [1m Learning iteration 109/500 [0m                      

                       Computation: 4636 steps/s (collection: 3.439s, learning 0.095s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.1069
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 7.2174
                       Mean reward: 6.29
               Mean episode length: 220.80
Episode_Reward/track_lin_vel_xy_exp: 0.6934
Episode_Reward/track_ang_vel_z_exp: 0.0141
       Episode_Reward/lin_vel_z_l2: -0.0156
      Episode_Reward/ang_vel_xy_l2: -0.0657
     Episode_Reward/dof_torques_l2: -0.0003
         Episode_Reward/dof_acc_l2: -0.2236
     Episode_Reward/action_rate_l2: -0.0088
Episode_Reward/flat_orientation_l2: -0.0123
    Episode_Reward/foot_contact_l2: -0.1477
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1802240
                    Iteration time: 3.53s
                      Time elapsed: 00:06:56
                               ETA: 00:24:40

################################################################################
                      [1m Learning iteration 110/500 [0m                      

                       Computation: 4524 steps/s (collection: 3.525s, learning 0.096s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.1202
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 7.1570
                       Mean reward: 6.53
               Mean episode length: 227.07
Episode_Reward/track_lin_vel_xy_exp: 0.8748
Episode_Reward/track_ang_vel_z_exp: 0.0136
       Episode_Reward/lin_vel_z_l2: -0.0186
      Episode_Reward/ang_vel_xy_l2: -0.0734
     Episode_Reward/dof_torques_l2: -0.0003
         Episode_Reward/dof_acc_l2: -0.2387
     Episode_Reward/action_rate_l2: -0.0101
Episode_Reward/flat_orientation_l2: -0.0137
    Episode_Reward/foot_contact_l2: -0.1691
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1818624
                    Iteration time: 3.62s
                      Time elapsed: 00:07:00
                               ETA: 00:24:36

################################################################################
                      [1m Learning iteration 111/500 [0m                      

                       Computation: 4623 steps/s (collection: 3.441s, learning 0.103s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.1145
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 7.0879
                       Mean reward: 6.97
               Mean episode length: 243.96
Episode_Reward/track_lin_vel_xy_exp: 1.0468
Episode_Reward/track_ang_vel_z_exp: 0.0184
       Episode_Reward/lin_vel_z_l2: -0.0271
      Episode_Reward/ang_vel_xy_l2: -0.1007
     Episode_Reward/dof_torques_l2: -0.0004
         Episode_Reward/dof_acc_l2: -0.3214
     Episode_Reward/action_rate_l2: -0.0125
Episode_Reward/flat_orientation_l2: -0.0211
    Episode_Reward/foot_contact_l2: -0.2151
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1835008
                    Iteration time: 3.54s
                      Time elapsed: 00:07:03
                               ETA: 00:24:31

################################################################################
                      [1m Learning iteration 112/500 [0m                      

                       Computation: 4651 steps/s (collection: 3.417s, learning 0.105s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0881
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 7.0175
                       Mean reward: 6.87
               Mean episode length: 247.18
Episode_Reward/track_lin_vel_xy_exp: 1.1091
Episode_Reward/track_ang_vel_z_exp: 0.0189
       Episode_Reward/lin_vel_z_l2: -0.0386
      Episode_Reward/ang_vel_xy_l2: -0.1290
     Episode_Reward/dof_torques_l2: -0.0004
         Episode_Reward/dof_acc_l2: -0.4305
     Episode_Reward/action_rate_l2: -0.0141
Episode_Reward/flat_orientation_l2: -0.0326
    Episode_Reward/foot_contact_l2: -0.2382
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1851392
                    Iteration time: 3.52s
                      Time elapsed: 00:07:07
                               ETA: 00:24:26

################################################################################
                      [1m Learning iteration 113/500 [0m                      

                       Computation: 4615 steps/s (collection: 3.454s, learning 0.095s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0964
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 6.9434
                       Mean reward: 6.87
               Mean episode length: 247.18
Episode_Reward/track_lin_vel_xy_exp: 0.9931
Episode_Reward/track_ang_vel_z_exp: 0.0208
       Episode_Reward/lin_vel_z_l2: -0.0467
      Episode_Reward/ang_vel_xy_l2: -0.1486
     Episode_Reward/dof_torques_l2: -0.0004
         Episode_Reward/dof_acc_l2: -0.5227
     Episode_Reward/action_rate_l2: -0.0154
Episode_Reward/flat_orientation_l2: -0.0532
    Episode_Reward/foot_contact_l2: -0.2466
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 3.55s
                      Time elapsed: 00:07:10
                               ETA: 00:24:22

################################################################################
                      [1m Learning iteration 114/500 [0m                      

                       Computation: 4650 steps/s (collection: 3.425s, learning 0.099s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0870
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 6.8517
                       Mean reward: 6.99
               Mean episode length: 250.46
Episode_Reward/track_lin_vel_xy_exp: 1.1105
Episode_Reward/track_ang_vel_z_exp: 0.0191
       Episode_Reward/lin_vel_z_l2: -0.0312
      Episode_Reward/ang_vel_xy_l2: -0.1104
     Episode_Reward/dof_torques_l2: -0.0004
         Episode_Reward/dof_acc_l2: -0.3913
     Episode_Reward/action_rate_l2: -0.0139
Episode_Reward/flat_orientation_l2: -0.0292
    Episode_Reward/foot_contact_l2: -0.2331
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1884160
                    Iteration time: 3.52s
                      Time elapsed: 00:07:14
                               ETA: 00:24:17

################################################################################
                      [1m Learning iteration 115/500 [0m                      

                       Computation: 4660 steps/s (collection: 3.420s, learning 0.096s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0984
               Mean surrogate loss: -0.0091
                 Mean entropy loss: 6.8072
                       Mean reward: 7.30
               Mean episode length: 259.39
Episode_Reward/track_lin_vel_xy_exp: 1.2601
Episode_Reward/track_ang_vel_z_exp: 0.0223
       Episode_Reward/lin_vel_z_l2: -0.0286
      Episode_Reward/ang_vel_xy_l2: -0.1081
     Episode_Reward/dof_torques_l2: -0.0004
         Episode_Reward/dof_acc_l2: -0.3734
     Episode_Reward/action_rate_l2: -0.0146
Episode_Reward/flat_orientation_l2: -0.0209
    Episode_Reward/foot_contact_l2: -0.2472
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1900544
                    Iteration time: 3.52s
                      Time elapsed: 00:07:17
                               ETA: 00:24:13

################################################################################
                      [1m Learning iteration 116/500 [0m                      

                       Computation: 4673 steps/s (collection: 3.408s, learning 0.097s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0929
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 6.7385
                       Mean reward: 7.65
               Mean episode length: 263.85
Episode_Reward/track_lin_vel_xy_exp: 1.2674
Episode_Reward/track_ang_vel_z_exp: 0.0236
       Episode_Reward/lin_vel_z_l2: -0.0227
      Episode_Reward/ang_vel_xy_l2: -0.0920
     Episode_Reward/dof_torques_l2: -0.0004
         Episode_Reward/dof_acc_l2: -0.3323
     Episode_Reward/action_rate_l2: -0.0132
Episode_Reward/flat_orientation_l2: -0.0142
    Episode_Reward/foot_contact_l2: -0.2274
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1916928
                    Iteration time: 3.51s
                      Time elapsed: 00:07:21
                               ETA: 00:24:08

################################################################################
                      [1m Learning iteration 117/500 [0m                      

                       Computation: 4662 steps/s (collection: 3.409s, learning 0.105s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0839
               Mean surrogate loss: -0.0090
                 Mean entropy loss: 6.6551
                       Mean reward: 7.65
               Mean episode length: 263.85
Episode_Reward/track_lin_vel_xy_exp: 0.9986
Episode_Reward/track_ang_vel_z_exp: 0.0230
       Episode_Reward/lin_vel_z_l2: -0.0204
      Episode_Reward/ang_vel_xy_l2: -0.0727
     Episode_Reward/dof_torques_l2: -0.0004
         Episode_Reward/dof_acc_l2: -0.3138
     Episode_Reward/action_rate_l2: -0.0112
Episode_Reward/flat_orientation_l2: -0.0123
    Episode_Reward/foot_contact_l2: -0.2043
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1933312
                    Iteration time: 3.51s
                      Time elapsed: 00:07:24
                               ETA: 00:24:03

################################################################################
                      [1m Learning iteration 118/500 [0m                      

                       Computation: 4642 steps/s (collection: 3.426s, learning 0.103s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0824
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 6.5655
                       Mean reward: 7.88
               Mean episode length: 271.77
Episode_Reward/track_lin_vel_xy_exp: 1.6018
Episode_Reward/track_ang_vel_z_exp: 0.0399
       Episode_Reward/lin_vel_z_l2: -0.0329
      Episode_Reward/ang_vel_xy_l2: -0.1246
     Episode_Reward/dof_torques_l2: -0.0006
         Episode_Reward/dof_acc_l2: -0.5391
     Episode_Reward/action_rate_l2: -0.0210
Episode_Reward/flat_orientation_l2: -0.0260
    Episode_Reward/foot_contact_l2: -0.3504
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1949696
                    Iteration time: 3.53s
                      Time elapsed: 00:07:28
                               ETA: 00:23:59

################################################################################
                      [1m Learning iteration 119/500 [0m                      

                       Computation: 4634 steps/s (collection: 3.436s, learning 0.099s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0873
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 6.4712
                       Mean reward: 7.88
               Mean episode length: 271.77
Episode_Reward/track_lin_vel_xy_exp: 1.8029
Episode_Reward/track_ang_vel_z_exp: 0.0456
       Episode_Reward/lin_vel_z_l2: -0.0370
      Episode_Reward/ang_vel_xy_l2: -0.1418
     Episode_Reward/dof_torques_l2: -0.0007
         Episode_Reward/dof_acc_l2: -0.6142
     Episode_Reward/action_rate_l2: -0.0242
Episode_Reward/flat_orientation_l2: -0.0305
    Episode_Reward/foot_contact_l2: -0.3991
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 3.54s
                      Time elapsed: 00:07:31
                               ETA: 00:23:54

################################################################################
                      [1m Learning iteration 120/500 [0m                      

                       Computation: 4640 steps/s (collection: 3.434s, learning 0.097s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0757
               Mean surrogate loss: -0.0082
                 Mean entropy loss: 6.4357
                       Mean reward: 7.88
               Mean episode length: 271.77
Episode_Reward/track_lin_vel_xy_exp: 1.8029
Episode_Reward/track_ang_vel_z_exp: 0.0456
       Episode_Reward/lin_vel_z_l2: -0.0370
      Episode_Reward/ang_vel_xy_l2: -0.1418
     Episode_Reward/dof_torques_l2: -0.0007
         Episode_Reward/dof_acc_l2: -0.6142
     Episode_Reward/action_rate_l2: -0.0242
Episode_Reward/flat_orientation_l2: -0.0305
    Episode_Reward/foot_contact_l2: -0.3991
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1982464
                    Iteration time: 3.53s
                      Time elapsed: 00:07:35
                               ETA: 00:23:50

################################################################################
                      [1m Learning iteration 121/500 [0m                      

                       Computation: 4630 steps/s (collection: 3.436s, learning 0.102s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0707
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 6.3712
                       Mean reward: 7.88
               Mean episode length: 271.77
Episode_Reward/track_lin_vel_xy_exp: 1.8029
Episode_Reward/track_ang_vel_z_exp: 0.0456
       Episode_Reward/lin_vel_z_l2: -0.0370
      Episode_Reward/ang_vel_xy_l2: -0.1418
     Episode_Reward/dof_torques_l2: -0.0007
         Episode_Reward/dof_acc_l2: -0.6142
     Episode_Reward/action_rate_l2: -0.0242
Episode_Reward/flat_orientation_l2: -0.0305
    Episode_Reward/foot_contact_l2: -0.3991
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1998848
                    Iteration time: 3.54s
                      Time elapsed: 00:07:38
                               ETA: 00:23:45

################################################################################
                      [1m Learning iteration 122/500 [0m                      

                       Computation: 4631 steps/s (collection: 3.416s, learning 0.122s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0767
               Mean surrogate loss: -0.0091
                 Mean entropy loss: 6.2921
                       Mean reward: 7.88
               Mean episode length: 271.77
Episode_Reward/track_lin_vel_xy_exp: 1.8029
Episode_Reward/track_ang_vel_z_exp: 0.0456
       Episode_Reward/lin_vel_z_l2: -0.0370
      Episode_Reward/ang_vel_xy_l2: -0.1418
     Episode_Reward/dof_torques_l2: -0.0007
         Episode_Reward/dof_acc_l2: -0.6142
     Episode_Reward/action_rate_l2: -0.0242
Episode_Reward/flat_orientation_l2: -0.0305
    Episode_Reward/foot_contact_l2: -0.3991
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2015232
                    Iteration time: 3.54s
                      Time elapsed: 00:07:42
                               ETA: 00:23:41

################################################################################
                      [1m Learning iteration 123/500 [0m                      

                       Computation: 4659 steps/s (collection: 3.421s, learning 0.096s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0837
               Mean surrogate loss: -0.0089
                 Mean entropy loss: 6.2225
                       Mean reward: 7.92
               Mean episode length: 276.55
Episode_Reward/track_lin_vel_xy_exp: 1.4366
Episode_Reward/track_ang_vel_z_exp: 0.0348
       Episode_Reward/lin_vel_z_l2: -0.0354
      Episode_Reward/ang_vel_xy_l2: -0.1446
     Episode_Reward/dof_torques_l2: -0.0006
         Episode_Reward/dof_acc_l2: -0.4731
     Episode_Reward/action_rate_l2: -0.0205
Episode_Reward/flat_orientation_l2: -0.0226
    Episode_Reward/foot_contact_l2: -0.3574
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2031616
                    Iteration time: 3.52s
                      Time elapsed: 00:07:45
                               ETA: 00:23:36

################################################################################
                      [1m Learning iteration 124/500 [0m                      

                       Computation: 4625 steps/s (collection: 3.422s, learning 0.120s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0752
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 6.1583
                       Mean reward: 7.92
               Mean episode length: 276.55
Episode_Reward/track_lin_vel_xy_exp: 1.3688
Episode_Reward/track_ang_vel_z_exp: 0.0328
       Episode_Reward/lin_vel_z_l2: -0.0351
      Episode_Reward/ang_vel_xy_l2: -0.1451
     Episode_Reward/dof_torques_l2: -0.0006
         Episode_Reward/dof_acc_l2: -0.4470
     Episode_Reward/action_rate_l2: -0.0198
Episode_Reward/flat_orientation_l2: -0.0211
    Episode_Reward/foot_contact_l2: -0.3496
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2048000
                    Iteration time: 3.54s
                      Time elapsed: 00:07:49
                               ETA: 00:23:32

################################################################################
                      [1m Learning iteration 125/500 [0m                      

                       Computation: 4661 steps/s (collection: 3.417s, learning 0.097s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0741
               Mean surrogate loss: -0.0083
                 Mean entropy loss: 6.1040
                       Mean reward: 7.92
               Mean episode length: 276.55
Episode_Reward/track_lin_vel_xy_exp: 1.3688
Episode_Reward/track_ang_vel_z_exp: 0.0328
       Episode_Reward/lin_vel_z_l2: -0.0351
      Episode_Reward/ang_vel_xy_l2: -0.1451
     Episode_Reward/dof_torques_l2: -0.0006
         Episode_Reward/dof_acc_l2: -0.4470
     Episode_Reward/action_rate_l2: -0.0198
Episode_Reward/flat_orientation_l2: -0.0211
    Episode_Reward/foot_contact_l2: -0.3496
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 3.51s
                      Time elapsed: 00:07:53
                               ETA: 00:23:27

################################################################################
                      [1m Learning iteration 126/500 [0m                      

                       Computation: 4628 steps/s (collection: 3.424s, learning 0.117s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0748
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 6.0293
                       Mean reward: 7.92
               Mean episode length: 276.55
Episode_Reward/track_lin_vel_xy_exp: 1.3688
Episode_Reward/track_ang_vel_z_exp: 0.0328
       Episode_Reward/lin_vel_z_l2: -0.0351
      Episode_Reward/ang_vel_xy_l2: -0.1451
     Episode_Reward/dof_torques_l2: -0.0006
         Episode_Reward/dof_acc_l2: -0.4470
     Episode_Reward/action_rate_l2: -0.0198
Episode_Reward/flat_orientation_l2: -0.0211
    Episode_Reward/foot_contact_l2: -0.3496
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2080768
                    Iteration time: 3.54s
                      Time elapsed: 00:07:56
                               ETA: 00:23:23

################################################################################
                      [1m Learning iteration 127/500 [0m                      

                       Computation: 4656 steps/s (collection: 3.423s, learning 0.096s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0696
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 5.9201
                       Mean reward: 7.92
               Mean episode length: 276.55
Episode_Reward/track_lin_vel_xy_exp: 1.3688
Episode_Reward/track_ang_vel_z_exp: 0.0328
       Episode_Reward/lin_vel_z_l2: -0.0351
      Episode_Reward/ang_vel_xy_l2: -0.1451
     Episode_Reward/dof_torques_l2: -0.0006
         Episode_Reward/dof_acc_l2: -0.4470
     Episode_Reward/action_rate_l2: -0.0198
Episode_Reward/flat_orientation_l2: -0.0211
    Episode_Reward/foot_contact_l2: -0.3496
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2097152
                    Iteration time: 3.52s
                      Time elapsed: 00:08:00
                               ETA: 00:23:19

################################################################################
                      [1m Learning iteration 128/500 [0m                      

                       Computation: 4610 steps/s (collection: 3.455s, learning 0.099s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0668
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 5.8380
                       Mean reward: 7.92
               Mean episode length: 276.55
Episode_Reward/track_lin_vel_xy_exp: 1.3688
Episode_Reward/track_ang_vel_z_exp: 0.0328
       Episode_Reward/lin_vel_z_l2: -0.0351
      Episode_Reward/ang_vel_xy_l2: -0.1451
     Episode_Reward/dof_torques_l2: -0.0006
         Episode_Reward/dof_acc_l2: -0.4470
     Episode_Reward/action_rate_l2: -0.0198
Episode_Reward/flat_orientation_l2: -0.0211
    Episode_Reward/foot_contact_l2: -0.3496
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2113536
                    Iteration time: 3.55s
                      Time elapsed: 00:08:03
                               ETA: 00:23:14

################################################################################
                      [1m Learning iteration 129/500 [0m                      

                       Computation: 4641 steps/s (collection: 3.433s, learning 0.097s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0974
               Mean surrogate loss: -0.0087
                 Mean entropy loss: 5.7669
                       Mean reward: 9.17
               Mean episode length: 304.81
Episode_Reward/track_lin_vel_xy_exp: 2.0533
Episode_Reward/track_ang_vel_z_exp: 0.0431
       Episode_Reward/lin_vel_z_l2: -0.0483
      Episode_Reward/ang_vel_xy_l2: -0.2021
     Episode_Reward/dof_torques_l2: -0.0008
         Episode_Reward/dof_acc_l2: -0.6205
     Episode_Reward/action_rate_l2: -0.0255
Episode_Reward/flat_orientation_l2: -0.0353
    Episode_Reward/foot_contact_l2: -0.4502
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2129920
                    Iteration time: 3.53s
                      Time elapsed: 00:08:07
                               ETA: 00:23:10

################################################################################
                      [1m Learning iteration 130/500 [0m                      

                       Computation: 4666 steps/s (collection: 3.416s, learning 0.096s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0744
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 5.6975
                       Mean reward: 9.17
               Mean episode length: 304.81
Episode_Reward/track_lin_vel_xy_exp: 2.9608
Episode_Reward/track_ang_vel_z_exp: 0.0547
       Episode_Reward/lin_vel_z_l2: -0.0452
      Episode_Reward/ang_vel_xy_l2: -0.1844
     Episode_Reward/dof_torques_l2: -0.0009
         Episode_Reward/dof_acc_l2: -0.6849
     Episode_Reward/action_rate_l2: -0.0266
Episode_Reward/flat_orientation_l2: -0.0417
    Episode_Reward/foot_contact_l2: -0.4833
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2146304
                    Iteration time: 3.51s
                      Time elapsed: 00:08:10
                               ETA: 00:23:05

################################################################################
                      [1m Learning iteration 131/500 [0m                      

                       Computation: 4651 steps/s (collection: 3.425s, learning 0.097s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0754
               Mean surrogate loss: -0.0076
                 Mean entropy loss: 5.6429
                       Mean reward: 9.44
               Mean episode length: 314.50
Episode_Reward/track_lin_vel_xy_exp: 2.2770
Episode_Reward/track_ang_vel_z_exp: 0.0450
       Episode_Reward/lin_vel_z_l2: -0.0509
      Episode_Reward/ang_vel_xy_l2: -0.1897
     Episode_Reward/dof_torques_l2: -0.0008
         Episode_Reward/dof_acc_l2: -0.7436
     Episode_Reward/action_rate_l2: -0.0254
Episode_Reward/flat_orientation_l2: -0.0328
    Episode_Reward/foot_contact_l2: -0.4711
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 3.52s
                      Time elapsed: 00:08:14
                               ETA: 00:23:01

################################################################################
                      [1m Learning iteration 132/500 [0m                      

                       Computation: 4608 steps/s (collection: 3.460s, learning 0.096s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0755
               Mean surrogate loss: -0.0083
                 Mean entropy loss: 5.5722
                       Mean reward: 9.76
               Mean episode length: 324.04
Episode_Reward/track_lin_vel_xy_exp: 2.3079
Episode_Reward/track_ang_vel_z_exp: 0.0416
       Episode_Reward/lin_vel_z_l2: -0.0531
      Episode_Reward/ang_vel_xy_l2: -0.1945
     Episode_Reward/dof_torques_l2: -0.0009
         Episode_Reward/dof_acc_l2: -0.7364
     Episode_Reward/action_rate_l2: -0.0268
Episode_Reward/flat_orientation_l2: -0.0333
    Episode_Reward/foot_contact_l2: -0.4809
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2179072
                    Iteration time: 3.56s
                      Time elapsed: 00:08:17
                               ETA: 00:22:57

################################################################################
                      [1m Learning iteration 133/500 [0m                      

                       Computation: 4656 steps/s (collection: 3.420s, learning 0.099s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0734
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 5.5182
                       Mean reward: 9.76
               Mean episode length: 324.04
Episode_Reward/track_lin_vel_xy_exp: 2.3540
Episode_Reward/track_ang_vel_z_exp: 0.0405
       Episode_Reward/lin_vel_z_l2: -0.0538
      Episode_Reward/ang_vel_xy_l2: -0.1965
     Episode_Reward/dof_torques_l2: -0.0009
         Episode_Reward/dof_acc_l2: -0.7304
     Episode_Reward/action_rate_l2: -0.0275
Episode_Reward/flat_orientation_l2: -0.0340
    Episode_Reward/foot_contact_l2: -0.4860
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2195456
                    Iteration time: 3.52s
                      Time elapsed: 00:08:21
                               ETA: 00:22:52

################################################################################
                      [1m Learning iteration 134/500 [0m                      

                       Computation: 4653 steps/s (collection: 3.423s, learning 0.098s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0712
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 5.4390
                       Mean reward: 10.09
               Mean episode length: 333.17
Episode_Reward/track_lin_vel_xy_exp: 2.4036
Episode_Reward/track_ang_vel_z_exp: 0.0438
       Episode_Reward/lin_vel_z_l2: -0.0531
      Episode_Reward/ang_vel_xy_l2: -0.2073
     Episode_Reward/dof_torques_l2: -0.0009
         Episode_Reward/dof_acc_l2: -0.7226
     Episode_Reward/action_rate_l2: -0.0281
Episode_Reward/flat_orientation_l2: -0.0308
    Episode_Reward/foot_contact_l2: -0.5052
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2211840
                    Iteration time: 3.52s
                      Time elapsed: 00:08:24
                               ETA: 00:22:48

################################################################################
                      [1m Learning iteration 135/500 [0m                      

                       Computation: 4626 steps/s (collection: 3.444s, learning 0.098s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0641
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 5.3837
                       Mean reward: 10.09
               Mean episode length: 333.17
Episode_Reward/track_lin_vel_xy_exp: 2.4295
Episode_Reward/track_ang_vel_z_exp: 0.0455
       Episode_Reward/lin_vel_z_l2: -0.0527
      Episode_Reward/ang_vel_xy_l2: -0.2129
     Episode_Reward/dof_torques_l2: -0.0009
         Episode_Reward/dof_acc_l2: -0.7185
     Episode_Reward/action_rate_l2: -0.0285
Episode_Reward/flat_orientation_l2: -0.0291
    Episode_Reward/foot_contact_l2: -0.5152
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2228224
                    Iteration time: 3.54s
                      Time elapsed: 00:08:28
                               ETA: 00:22:44

################################################################################
                      [1m Learning iteration 136/500 [0m                      

                       Computation: 4627 steps/s (collection: 3.439s, learning 0.102s)
             Mean action noise std: 0.38
          Mean value_function loss: 0.0747
               Mean surrogate loss: -0.0095
                 Mean entropy loss: 5.3208
                       Mean reward: 10.40
               Mean episode length: 344.58
Episode_Reward/track_lin_vel_xy_exp: 2.4372
Episode_Reward/track_ang_vel_z_exp: 0.0464
       Episode_Reward/lin_vel_z_l2: -0.0529
      Episode_Reward/ang_vel_xy_l2: -0.2144
     Episode_Reward/dof_torques_l2: -0.0009
         Episode_Reward/dof_acc_l2: -0.7200
     Episode_Reward/action_rate_l2: -0.0288
Episode_Reward/flat_orientation_l2: -0.0301
    Episode_Reward/foot_contact_l2: -0.5214
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2244608
                    Iteration time: 3.54s
                      Time elapsed: 00:08:31
                               ETA: 00:22:40

################################################################################
                      [1m Learning iteration 137/500 [0m                      

                       Computation: 4622 steps/s (collection: 3.442s, learning 0.102s)
             Mean action noise std: 0.38
          Mean value_function loss: 0.0790
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 5.2572
                       Mean reward: 10.90
               Mean episode length: 356.88
Episode_Reward/track_lin_vel_xy_exp: 3.1097
Episode_Reward/track_ang_vel_z_exp: 0.0524
       Episode_Reward/lin_vel_z_l2: -0.0768
      Episode_Reward/ang_vel_xy_l2: -0.2800
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.8554
     Episode_Reward/action_rate_l2: -0.0364
Episode_Reward/flat_orientation_l2: -0.0491
    Episode_Reward/foot_contact_l2: -0.6344
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 3.54s
                      Time elapsed: 00:08:35
                               ETA: 00:22:35

################################################################################
                      [1m Learning iteration 138/500 [0m                      

                       Computation: 4577 steps/s (collection: 3.484s, learning 0.095s)
             Mean action noise std: 0.38
          Mean value_function loss: 0.0624
               Mean surrogate loss: -0.0086
                 Mean entropy loss: 5.1984
                       Mean reward: 10.90
               Mean episode length: 356.88
Episode_Reward/track_lin_vel_xy_exp: 3.2478
Episode_Reward/track_ang_vel_z_exp: 0.0519
       Episode_Reward/lin_vel_z_l2: -0.0818
      Episode_Reward/ang_vel_xy_l2: -0.2918
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.8834
     Episode_Reward/action_rate_l2: -0.0374
Episode_Reward/flat_orientation_l2: -0.0513
    Episode_Reward/foot_contact_l2: -0.6466
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2277376
                    Iteration time: 3.58s
                      Time elapsed: 00:08:39
                               ETA: 00:22:31

################################################################################
                      [1m Learning iteration 139/500 [0m                      

                       Computation: 4663 steps/s (collection: 3.419s, learning 0.094s)
             Mean action noise std: 0.38
          Mean value_function loss: 0.0642
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 5.1056
                       Mean reward: 10.90
               Mean episode length: 356.88
Episode_Reward/track_lin_vel_xy_exp: 3.2478
Episode_Reward/track_ang_vel_z_exp: 0.0519
       Episode_Reward/lin_vel_z_l2: -0.0818
      Episode_Reward/ang_vel_xy_l2: -0.2918
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.8834
     Episode_Reward/action_rate_l2: -0.0374
Episode_Reward/flat_orientation_l2: -0.0513
    Episode_Reward/foot_contact_l2: -0.6466
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2293760
                    Iteration time: 3.51s
                      Time elapsed: 00:08:42
                               ETA: 00:22:27

################################################################################
                      [1m Learning iteration 140/500 [0m                      

                       Computation: 4639 steps/s (collection: 3.434s, learning 0.097s)
             Mean action noise std: 0.38
          Mean value_function loss: 0.0732
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 5.0357
                       Mean reward: 11.46
               Mean episode length: 371.59
Episode_Reward/track_lin_vel_xy_exp: 3.2767
Episode_Reward/track_ang_vel_z_exp: 0.0531
       Episode_Reward/lin_vel_z_l2: -0.0812
      Episode_Reward/ang_vel_xy_l2: -0.2930
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.8835
     Episode_Reward/action_rate_l2: -0.0377
Episode_Reward/flat_orientation_l2: -0.0522
    Episode_Reward/foot_contact_l2: -0.6544
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2310144
                    Iteration time: 3.53s
                      Time elapsed: 00:08:46
                               ETA: 00:22:23

################################################################################
                      [1m Learning iteration 141/500 [0m                      

                       Computation: 4650 steps/s (collection: 3.425s, learning 0.098s)
             Mean action noise std: 0.37
          Mean value_function loss: 0.0547
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 4.9664
                       Mean reward: 11.46
               Mean episode length: 371.59
Episode_Reward/track_lin_vel_xy_exp: 3.5556
Episode_Reward/track_ang_vel_z_exp: 0.0647
       Episode_Reward/lin_vel_z_l2: -0.0749
      Episode_Reward/ang_vel_xy_l2: -0.3038
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.8845
     Episode_Reward/action_rate_l2: -0.0402
Episode_Reward/flat_orientation_l2: -0.0611
    Episode_Reward/foot_contact_l2: -0.7290
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2326528
                    Iteration time: 3.52s
                      Time elapsed: 00:08:49
                               ETA: 00:22:18

################################################################################
                      [1m Learning iteration 142/500 [0m                      

                       Computation: 4634 steps/s (collection: 3.428s, learning 0.107s)
             Mean action noise std: 0.37
          Mean value_function loss: 0.0614
               Mean surrogate loss: -0.0080
                 Mean entropy loss: 4.8960
                       Mean reward: 11.46
               Mean episode length: 371.59
Episode_Reward/track_lin_vel_xy_exp: 3.5556
Episode_Reward/track_ang_vel_z_exp: 0.0647
       Episode_Reward/lin_vel_z_l2: -0.0749
      Episode_Reward/ang_vel_xy_l2: -0.3038
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.8845
     Episode_Reward/action_rate_l2: -0.0402
Episode_Reward/flat_orientation_l2: -0.0611
    Episode_Reward/foot_contact_l2: -0.7290
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2342912
                    Iteration time: 3.54s
                      Time elapsed: 00:08:53
                               ETA: 00:22:14

################################################################################
                      [1m Learning iteration 143/500 [0m                      

                       Computation: 4629 steps/s (collection: 3.442s, learning 0.098s)
             Mean action noise std: 0.37
          Mean value_function loss: 0.0626
               Mean surrogate loss: -0.0084
                 Mean entropy loss: 4.7929
                       Mean reward: 11.46
               Mean episode length: 371.59
Episode_Reward/track_lin_vel_xy_exp: 3.5556
Episode_Reward/track_ang_vel_z_exp: 0.0647
       Episode_Reward/lin_vel_z_l2: -0.0749
      Episode_Reward/ang_vel_xy_l2: -0.3038
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.8845
     Episode_Reward/action_rate_l2: -0.0402
Episode_Reward/flat_orientation_l2: -0.0611
    Episode_Reward/foot_contact_l2: -0.7290
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 3.54s
                      Time elapsed: 00:08:56
                               ETA: 00:22:10

################################################################################
                      [1m Learning iteration 144/500 [0m                      

                       Computation: 4628 steps/s (collection: 3.438s, learning 0.102s)
             Mean action noise std: 0.37
          Mean value_function loss: 0.0581
               Mean surrogate loss: -0.0094
                 Mean entropy loss: 4.7479
                       Mean reward: 11.46
               Mean episode length: 371.59
Episode_Reward/track_lin_vel_xy_exp: 3.5556
Episode_Reward/track_ang_vel_z_exp: 0.0647
       Episode_Reward/lin_vel_z_l2: -0.0749
      Episode_Reward/ang_vel_xy_l2: -0.3038
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.8845
     Episode_Reward/action_rate_l2: -0.0402
Episode_Reward/flat_orientation_l2: -0.0611
    Episode_Reward/foot_contact_l2: -0.7290
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2375680
                    Iteration time: 3.54s
                      Time elapsed: 00:09:00
                               ETA: 00:22:06

################################################################################
                      [1m Learning iteration 145/500 [0m                      

                       Computation: 4642 steps/s (collection: 3.429s, learning 0.101s)
             Mean action noise std: 0.37
          Mean value_function loss: 0.0557
               Mean surrogate loss: -0.0092
                 Mean entropy loss: 4.6838
                       Mean reward: 11.46
               Mean episode length: 371.59
Episode_Reward/track_lin_vel_xy_exp: 3.5556
Episode_Reward/track_ang_vel_z_exp: 0.0647
       Episode_Reward/lin_vel_z_l2: -0.0749
      Episode_Reward/ang_vel_xy_l2: -0.3038
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.8845
     Episode_Reward/action_rate_l2: -0.0402
Episode_Reward/flat_orientation_l2: -0.0611
    Episode_Reward/foot_contact_l2: -0.7290
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2392064
                    Iteration time: 3.53s
                      Time elapsed: 00:09:03
                               ETA: 00:22:02

################################################################################
                      [1m Learning iteration 146/500 [0m                      

                       Computation: 4597 steps/s (collection: 3.463s, learning 0.101s)
             Mean action noise std: 0.36
          Mean value_function loss: 0.0557
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 4.6097
                       Mean reward: 11.46
               Mean episode length: 371.59
Episode_Reward/track_lin_vel_xy_exp: 3.5556
Episode_Reward/track_ang_vel_z_exp: 0.0647
       Episode_Reward/lin_vel_z_l2: -0.0749
      Episode_Reward/ang_vel_xy_l2: -0.3038
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.8845
     Episode_Reward/action_rate_l2: -0.0402
Episode_Reward/flat_orientation_l2: -0.0611
    Episode_Reward/foot_contact_l2: -0.7290
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2408448
                    Iteration time: 3.56s
                      Time elapsed: 00:09:07
                               ETA: 00:21:58

################################################################################
                      [1m Learning iteration 147/500 [0m                      

                       Computation: 4641 steps/s (collection: 3.434s, learning 0.096s)
             Mean action noise std: 0.36
          Mean value_function loss: 0.0720
               Mean surrogate loss: -0.0094
                 Mean entropy loss: 4.5187
                       Mean reward: 13.00
               Mean episode length: 397.51
Episode_Reward/track_lin_vel_xy_exp: 3.8295
Episode_Reward/track_ang_vel_z_exp: 0.0609
       Episode_Reward/lin_vel_z_l2: -0.0581
      Episode_Reward/ang_vel_xy_l2: -0.2248
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.8555
     Episode_Reward/action_rate_l2: -0.0319
Episode_Reward/flat_orientation_l2: -0.0382
    Episode_Reward/foot_contact_l2: -0.6356
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2424832
                    Iteration time: 3.53s
                      Time elapsed: 00:09:10
                               ETA: 00:21:53

################################################################################
                      [1m Learning iteration 148/500 [0m                      

                       Computation: 4633 steps/s (collection: 3.433s, learning 0.102s)
             Mean action noise std: 0.36
          Mean value_function loss: 0.0702
               Mean surrogate loss: -0.0082
                 Mean entropy loss: 4.4651
                       Mean reward: 13.68
               Mean episode length: 412.74
Episode_Reward/track_lin_vel_xy_exp: 4.1142
Episode_Reward/track_ang_vel_z_exp: 0.0728
       Episode_Reward/lin_vel_z_l2: -0.0700
      Episode_Reward/ang_vel_xy_l2: -0.2707
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -1.0493
     Episode_Reward/action_rate_l2: -0.0388
Episode_Reward/flat_orientation_l2: -0.0492
    Episode_Reward/foot_contact_l2: -0.7401
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2441216
                    Iteration time: 3.54s
                      Time elapsed: 00:09:14
                               ETA: 00:21:49

################################################################################
                      [1m Learning iteration 149/500 [0m                      

                       Computation: 4656 steps/s (collection: 3.421s, learning 0.097s)
             Mean action noise std: 0.35
          Mean value_function loss: 0.0566
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 4.3517
                       Mean reward: 13.68
               Mean episode length: 412.74
Episode_Reward/track_lin_vel_xy_exp: 4.2055
Episode_Reward/track_ang_vel_z_exp: 0.0822
       Episode_Reward/lin_vel_z_l2: -0.0747
      Episode_Reward/ang_vel_xy_l2: -0.2880
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -1.1915
     Episode_Reward/action_rate_l2: -0.0423
Episode_Reward/flat_orientation_l2: -0.0552
    Episode_Reward/foot_contact_l2: -0.7933
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 3.52s
                      Time elapsed: 00:09:17
                               ETA: 00:21:45

################################################################################
                      [1m Learning iteration 150/500 [0m                      

                       Computation: 4611 steps/s (collection: 3.451s, learning 0.102s)
             Mean action noise std: 0.35
          Mean value_function loss: 0.0521
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 4.2412
                       Mean reward: 13.68
               Mean episode length: 412.74
Episode_Reward/track_lin_vel_xy_exp: 4.2055
Episode_Reward/track_ang_vel_z_exp: 0.0822
       Episode_Reward/lin_vel_z_l2: -0.0747
      Episode_Reward/ang_vel_xy_l2: -0.2880
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -1.1915
     Episode_Reward/action_rate_l2: -0.0423
Episode_Reward/flat_orientation_l2: -0.0552
    Episode_Reward/foot_contact_l2: -0.7933
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2473984
                    Iteration time: 3.55s
                      Time elapsed: 00:09:21
                               ETA: 00:21:41

################################################################################
                      [1m Learning iteration 151/500 [0m                      

                       Computation: 4628 steps/s (collection: 3.446s, learning 0.093s)
             Mean action noise std: 0.35
          Mean value_function loss: 0.0491
               Mean surrogate loss: -0.0092
                 Mean entropy loss: 4.1572
                       Mean reward: 13.68
               Mean episode length: 412.74
Episode_Reward/track_lin_vel_xy_exp: 4.2055
Episode_Reward/track_ang_vel_z_exp: 0.0822
       Episode_Reward/lin_vel_z_l2: -0.0747
      Episode_Reward/ang_vel_xy_l2: -0.2880
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -1.1915
     Episode_Reward/action_rate_l2: -0.0423
Episode_Reward/flat_orientation_l2: -0.0552
    Episode_Reward/foot_contact_l2: -0.7933
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2490368
                    Iteration time: 3.54s
                      Time elapsed: 00:09:24
                               ETA: 00:21:37

################################################################################
                      [1m Learning iteration 152/500 [0m                      

                       Computation: 4658 steps/s (collection: 3.416s, learning 0.101s)
             Mean action noise std: 0.35
          Mean value_function loss: 0.0551
               Mean surrogate loss: -0.0094
                 Mean entropy loss: 4.0738
                       Mean reward: 14.45
               Mean episode length: 430.73
Episode_Reward/track_lin_vel_xy_exp: 4.2347
Episode_Reward/track_ang_vel_z_exp: 0.0818
       Episode_Reward/lin_vel_z_l2: -0.0778
      Episode_Reward/ang_vel_xy_l2: -0.2978
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -1.1833
     Episode_Reward/action_rate_l2: -0.0429
Episode_Reward/flat_orientation_l2: -0.0568
    Episode_Reward/foot_contact_l2: -0.8033
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.0938
--------------------------------------------------------------------------------
                   Total timesteps: 2506752
                    Iteration time: 3.52s
                      Time elapsed: 00:09:28
                               ETA: 00:21:33

################################################################################
                      [1m Learning iteration 153/500 [0m                      

                       Computation: 4628 steps/s (collection: 3.444s, learning 0.096s)
             Mean action noise std: 0.35
          Mean value_function loss: 0.0605
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 4.0071
                       Mean reward: 22.01
               Mean episode length: 597.19
Episode_Reward/track_lin_vel_xy_exp: 4.9172
Episode_Reward/track_ang_vel_z_exp: 0.0821
       Episode_Reward/lin_vel_z_l2: -0.0963
      Episode_Reward/ang_vel_xy_l2: -0.3671
     Episode_Reward/dof_torques_l2: -0.0016
         Episode_Reward/dof_acc_l2: -1.1803
     Episode_Reward/action_rate_l2: -0.0476
Episode_Reward/flat_orientation_l2: -0.0627
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2523136
                    Iteration time: 3.54s
                      Time elapsed: 00:09:32
                               ETA: 00:21:28

################################################################################
                      [1m Learning iteration 154/500 [0m                      

                       Computation: 4568 steps/s (collection: 3.457s, learning 0.129s)
             Mean action noise std: 0.34
          Mean value_function loss: 0.0515
               Mean surrogate loss: -0.0083
                 Mean entropy loss: 3.9348
                       Mean reward: 31.28
               Mean episode length: 797.83
Episode_Reward/track_lin_vel_xy_exp: 4.7498
Episode_Reward/track_ang_vel_z_exp: 0.0890
       Episode_Reward/lin_vel_z_l2: -0.0851
      Episode_Reward/ang_vel_xy_l2: -0.3341
     Episode_Reward/dof_torques_l2: -0.0016
         Episode_Reward/dof_acc_l2: -1.1719
     Episode_Reward/action_rate_l2: -0.0463
Episode_Reward/flat_orientation_l2: -0.0577
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0312
--------------------------------------------------------------------------------
                   Total timesteps: 2539520
                    Iteration time: 3.59s
                      Time elapsed: 00:09:35
                               ETA: 00:21:24

################################################################################
                      [1m Learning iteration 155/500 [0m                      

                       Computation: 4596 steps/s (collection: 3.466s, learning 0.099s)
             Mean action noise std: 0.34
          Mean value_function loss: 0.0520
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 3.8572
                       Mean reward: 44.46
               Mean episode length: 1093.94
Episode_Reward/track_lin_vel_xy_exp: 4.6480
Episode_Reward/track_ang_vel_z_exp: 0.0858
       Episode_Reward/lin_vel_z_l2: -0.0916
      Episode_Reward/ang_vel_xy_l2: -0.3422
     Episode_Reward/dof_torques_l2: -0.0016
         Episode_Reward/dof_acc_l2: -1.1845
     Episode_Reward/action_rate_l2: -0.0471
Episode_Reward/flat_orientation_l2: -0.0585
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.4688
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 3.56s
                      Time elapsed: 00:09:39
                               ETA: 00:21:20

################################################################################
                      [1m Learning iteration 156/500 [0m                      

                       Computation: 4545 steps/s (collection: 3.478s, learning 0.126s)
             Mean action noise std: 0.34
          Mean value_function loss: 0.0428
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 3.7692
                       Mean reward: 83.86
               Mean episode length: 1843.56
Episode_Reward/track_lin_vel_xy_exp: 4.8766
Episode_Reward/track_ang_vel_z_exp: 0.0893
       Episode_Reward/lin_vel_z_l2: -0.0822
      Episode_Reward/ang_vel_xy_l2: -0.3203
     Episode_Reward/dof_torques_l2: -0.0016
         Episode_Reward/dof_acc_l2: -1.1663
     Episode_Reward/action_rate_l2: -0.0461
Episode_Reward/flat_orientation_l2: -0.0565
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2572288
                    Iteration time: 3.60s
                      Time elapsed: 00:09:42
                               ETA: 00:21:16

################################################################################
                      [1m Learning iteration 157/500 [0m                      

                       Computation: 4550 steps/s (collection: 3.503s, learning 0.097s)
             Mean action noise std: 0.34
          Mean value_function loss: 0.0468
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 3.6699
                       Mean reward: 99.31
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 4.9670
Episode_Reward/track_ang_vel_z_exp: 0.0885
       Episode_Reward/lin_vel_z_l2: -0.0854
      Episode_Reward/ang_vel_xy_l2: -0.3274
     Episode_Reward/dof_torques_l2: -0.0016
         Episode_Reward/dof_acc_l2: -1.1804
     Episode_Reward/action_rate_l2: -0.0460
Episode_Reward/flat_orientation_l2: -0.0562
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 2588672
                    Iteration time: 3.60s
                      Time elapsed: 00:09:46
                               ETA: 00:21:12

################################################################################
                      [1m Learning iteration 158/500 [0m                      

                       Computation: 4531 steps/s (collection: 3.518s, learning 0.097s)
             Mean action noise std: 0.34
          Mean value_function loss: 0.0624
               Mean surrogate loss: -0.0087
                 Mean entropy loss: 3.6368
                       Mean reward: 100.33
               Mean episode length: 1997.98
Episode_Reward/track_lin_vel_xy_exp: 4.9701
Episode_Reward/track_ang_vel_z_exp: 0.0896
       Episode_Reward/lin_vel_z_l2: -0.0811
      Episode_Reward/ang_vel_xy_l2: -0.3132
     Episode_Reward/dof_torques_l2: -0.0016
         Episode_Reward/dof_acc_l2: -1.1767
     Episode_Reward/action_rate_l2: -0.0456
Episode_Reward/flat_orientation_l2: -0.0538
    Episode_Reward/foot_contact_l2: -0.8988
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 2605056
                    Iteration time: 3.62s
                      Time elapsed: 00:09:50
                               ETA: 00:21:09

################################################################################
                      [1m Learning iteration 159/500 [0m                      

                       Computation: 4558 steps/s (collection: 3.499s, learning 0.096s)
             Mean action noise std: 0.33
          Mean value_function loss: 0.0496
               Mean surrogate loss: -0.0098
                 Mean entropy loss: 3.5476
                       Mean reward: 96.33
               Mean episode length: 1997.98
Episode_Reward/track_lin_vel_xy_exp: 4.7887
Episode_Reward/track_ang_vel_z_exp: 0.0909
       Episode_Reward/lin_vel_z_l2: -0.0791
      Episode_Reward/ang_vel_xy_l2: -0.3096
     Episode_Reward/dof_torques_l2: -0.0016
         Episode_Reward/dof_acc_l2: -1.1658
     Episode_Reward/action_rate_l2: -0.0450
Episode_Reward/flat_orientation_l2: -0.0510
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 2.3438
--------------------------------------------------------------------------------
                   Total timesteps: 2621440
                    Iteration time: 3.59s
                      Time elapsed: 00:09:53
                               ETA: 00:21:05

################################################################################
                      [1m Learning iteration 160/500 [0m                      

                       Computation: 4577 steps/s (collection: 3.478s, learning 0.101s)
             Mean action noise std: 0.33
          Mean value_function loss: 0.0572
               Mean surrogate loss: -0.0086
                 Mean entropy loss: 3.4667
                       Mean reward: 100.04
               Mean episode length: 1998.37
Episode_Reward/track_lin_vel_xy_exp: 5.1068
Episode_Reward/track_ang_vel_z_exp: 0.0898
       Episode_Reward/lin_vel_z_l2: -0.0799
      Episode_Reward/ang_vel_xy_l2: -0.3109
     Episode_Reward/dof_torques_l2: -0.0016
         Episode_Reward/dof_acc_l2: -1.1513
     Episode_Reward/action_rate_l2: -0.0449
Episode_Reward/flat_orientation_l2: -0.0514
    Episode_Reward/foot_contact_l2: -0.8987
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.8125
--------------------------------------------------------------------------------
                   Total timesteps: 2637824
                    Iteration time: 3.58s
                      Time elapsed: 00:09:57
                               ETA: 00:21:01

################################################################################
                      [1m Learning iteration 161/500 [0m                      

                       Computation: 4537 steps/s (collection: 3.482s, learning 0.129s)
             Mean action noise std: 0.33
          Mean value_function loss: 0.0453
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 3.3817
                       Mean reward: 103.69
               Mean episode length: 1998.37
Episode_Reward/track_lin_vel_xy_exp: 4.8481
Episode_Reward/track_ang_vel_z_exp: 0.0874
       Episode_Reward/lin_vel_z_l2: -0.0835
      Episode_Reward/ang_vel_xy_l2: -0.3227
     Episode_Reward/dof_torques_l2: -0.0016
         Episode_Reward/dof_acc_l2: -1.1505
     Episode_Reward/action_rate_l2: -0.0449
Episode_Reward/flat_orientation_l2: -0.0535
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.5312
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 3.61s
                      Time elapsed: 00:10:00
                               ETA: 00:20:57

################################################################################
                      [1m Learning iteration 162/500 [0m                      

                       Computation: 4565 steps/s (collection: 3.491s, learning 0.097s)
             Mean action noise std: 0.33
          Mean value_function loss: 0.0480
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 3.3150
                       Mean reward: 104.85
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 5.0638
Episode_Reward/track_ang_vel_z_exp: 0.0919
       Episode_Reward/lin_vel_z_l2: -0.0768
      Episode_Reward/ang_vel_xy_l2: -0.2988
     Episode_Reward/dof_torques_l2: -0.0016
         Episode_Reward/dof_acc_l2: -1.1565
     Episode_Reward/action_rate_l2: -0.0443
Episode_Reward/flat_orientation_l2: -0.0492
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.4688
--------------------------------------------------------------------------------
                   Total timesteps: 2670592
                    Iteration time: 3.59s
                      Time elapsed: 00:10:04
                               ETA: 00:20:53

################################################################################
                      [1m Learning iteration 163/500 [0m                      

                       Computation: 4600 steps/s (collection: 3.457s, learning 0.104s)
             Mean action noise std: 0.33
          Mean value_function loss: 0.0550
               Mean surrogate loss: -0.0092
                 Mean entropy loss: 3.2543
                       Mean reward: 104.82
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 5.1307
Episode_Reward/track_ang_vel_z_exp: 0.0923
       Episode_Reward/lin_vel_z_l2: -0.0759
      Episode_Reward/ang_vel_xy_l2: -0.2999
     Episode_Reward/dof_torques_l2: -0.0016
         Episode_Reward/dof_acc_l2: -1.1147
     Episode_Reward/action_rate_l2: -0.0437
Episode_Reward/flat_orientation_l2: -0.0487
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.3438
--------------------------------------------------------------------------------
                   Total timesteps: 2686976
                    Iteration time: 3.56s
                      Time elapsed: 00:10:07
                               ETA: 00:20:49

################################################################################
                      [1m Learning iteration 164/500 [0m                      

                       Computation: 4580 steps/s (collection: 3.472s, learning 0.105s)
             Mean action noise std: 0.32
          Mean value_function loss: 0.0426
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 3.2059
                       Mean reward: 105.05
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 5.2184
Episode_Reward/track_ang_vel_z_exp: 0.0912
       Episode_Reward/lin_vel_z_l2: -0.0811
      Episode_Reward/ang_vel_xy_l2: -0.3046
     Episode_Reward/dof_torques_l2: -0.0016
         Episode_Reward/dof_acc_l2: -1.1099
     Episode_Reward/action_rate_l2: -0.0439
Episode_Reward/flat_orientation_l2: -0.0509
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 2703360
                    Iteration time: 3.58s
                      Time elapsed: 00:10:11
                               ETA: 00:20:45

################################################################################
                      [1m Learning iteration 165/500 [0m                      

                       Computation: 4577 steps/s (collection: 3.477s, learning 0.102s)
             Mean action noise std: 0.32
          Mean value_function loss: 0.0848
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 3.1104
                       Mean reward: 105.26
               Mean episode length: 1979.99
Episode_Reward/track_lin_vel_xy_exp: 5.0835
Episode_Reward/track_ang_vel_z_exp: 0.0866
       Episode_Reward/lin_vel_z_l2: -0.0748
      Episode_Reward/ang_vel_xy_l2: -0.2898
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -1.0901
     Episode_Reward/action_rate_l2: -0.0420
Episode_Reward/flat_orientation_l2: -0.0456
    Episode_Reward/foot_contact_l2: -0.8698
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.9375
--------------------------------------------------------------------------------
                   Total timesteps: 2719744
                    Iteration time: 3.58s
                      Time elapsed: 00:10:15
                               ETA: 00:20:41

################################################################################
                      [1m Learning iteration 166/500 [0m                      

                       Computation: 4585 steps/s (collection: 3.468s, learning 0.106s)
             Mean action noise std: 0.32
          Mean value_function loss: 0.0446
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 3.0565
                       Mean reward: 105.19
               Mean episode length: 1979.99
Episode_Reward/track_lin_vel_xy_exp: 5.2091
Episode_Reward/track_ang_vel_z_exp: 0.0881
       Episode_Reward/lin_vel_z_l2: -0.0795
      Episode_Reward/ang_vel_xy_l2: -0.3032
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -1.1831
     Episode_Reward/action_rate_l2: -0.0436
Episode_Reward/flat_orientation_l2: -0.0477
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.4062
--------------------------------------------------------------------------------
                   Total timesteps: 2736128
                    Iteration time: 3.57s
                      Time elapsed: 00:10:18
                               ETA: 00:20:37

################################################################################
                      [1m Learning iteration 167/500 [0m                      

                       Computation: 4624 steps/s (collection: 3.446s, learning 0.096s)
             Mean action noise std: 0.32
          Mean value_function loss: 0.0400
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 2.9221
                       Mean reward: 105.63
               Mean episode length: 1979.99
Episode_Reward/track_lin_vel_xy_exp: 5.1829
Episode_Reward/track_ang_vel_z_exp: 0.0914
       Episode_Reward/lin_vel_z_l2: -0.0744
      Episode_Reward/ang_vel_xy_l2: -0.2831
     Episode_Reward/dof_torques_l2: -0.0016
         Episode_Reward/dof_acc_l2: -1.1594
     Episode_Reward/action_rate_l2: -0.0425
Episode_Reward/flat_orientation_l2: -0.0453
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 3.54s
                      Time elapsed: 00:10:22
                               ETA: 00:20:33

################################################################################
                      [1m Learning iteration 168/500 [0m                      

                       Computation: 4622 steps/s (collection: 3.444s, learning 0.100s)
             Mean action noise std: 0.32
          Mean value_function loss: 0.0413
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 2.8268
                       Mean reward: 105.87
               Mean episode length: 1979.99
Episode_Reward/track_lin_vel_xy_exp: 5.2850
Episode_Reward/track_ang_vel_z_exp: 0.0940
       Episode_Reward/lin_vel_z_l2: -0.0693
      Episode_Reward/ang_vel_xy_l2: -0.2614
     Episode_Reward/dof_torques_l2: -0.0016
         Episode_Reward/dof_acc_l2: -1.0804
     Episode_Reward/action_rate_l2: -0.0423
Episode_Reward/flat_orientation_l2: -0.0461
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2768896
                    Iteration time: 3.54s
                      Time elapsed: 00:10:25
                               ETA: 00:20:29

################################################################################
                      [1m Learning iteration 169/500 [0m                      

                       Computation: 4625 steps/s (collection: 3.442s, learning 0.100s)
             Mean action noise std: 0.31
          Mean value_function loss: 0.0418
               Mean surrogate loss: -0.0083
                 Mean entropy loss: 2.7407
                       Mean reward: 107.26
               Mean episode length: 1979.99
Episode_Reward/track_lin_vel_xy_exp: 5.4689
Episode_Reward/track_ang_vel_z_exp: 0.0932
       Episode_Reward/lin_vel_z_l2: -0.0682
      Episode_Reward/ang_vel_xy_l2: -0.2621
     Episode_Reward/dof_torques_l2: -0.0016
         Episode_Reward/dof_acc_l2: -1.0876
     Episode_Reward/action_rate_l2: -0.0421
Episode_Reward/flat_orientation_l2: -0.0435
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0312
--------------------------------------------------------------------------------
                   Total timesteps: 2785280
                    Iteration time: 3.54s
                      Time elapsed: 00:10:29
                               ETA: 00:20:25

################################################################################
                      [1m Learning iteration 170/500 [0m                      

                       Computation: 4639 steps/s (collection: 3.434s, learning 0.098s)
             Mean action noise std: 0.31
          Mean value_function loss: 0.0426
               Mean surrogate loss: -0.0089
                 Mean entropy loss: 2.6723
                       Mean reward: 108.16
               Mean episode length: 1979.99
Episode_Reward/track_lin_vel_xy_exp: 5.0622
Episode_Reward/track_ang_vel_z_exp: 0.0902
       Episode_Reward/lin_vel_z_l2: -0.0724
      Episode_Reward/ang_vel_xy_l2: -0.2780
     Episode_Reward/dof_torques_l2: -0.0016
         Episode_Reward/dof_acc_l2: -1.0565
     Episode_Reward/action_rate_l2: -0.0419
Episode_Reward/flat_orientation_l2: -0.0424
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0938
--------------------------------------------------------------------------------
                   Total timesteps: 2801664
                    Iteration time: 3.53s
                      Time elapsed: 00:10:32
                               ETA: 00:20:21

################################################################################
                      [1m Learning iteration 171/500 [0m                      

                       Computation: 4643 steps/s (collection: 3.430s, learning 0.098s)
             Mean action noise std: 0.31
          Mean value_function loss: 0.0361
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 2.5692
                       Mean reward: 109.05
               Mean episode length: 1979.99
Episode_Reward/track_lin_vel_xy_exp: 5.5081
Episode_Reward/track_ang_vel_z_exp: 0.0987
       Episode_Reward/lin_vel_z_l2: -0.0618
      Episode_Reward/ang_vel_xy_l2: -0.2545
     Episode_Reward/dof_torques_l2: -0.0016
         Episode_Reward/dof_acc_l2: -1.0838
     Episode_Reward/action_rate_l2: -0.0408
Episode_Reward/flat_orientation_l2: -0.0439
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2818048
                    Iteration time: 3.53s
                      Time elapsed: 00:10:36
                               ETA: 00:20:17

################################################################################
                      [1m Learning iteration 172/500 [0m                      

                       Computation: 4596 steps/s (collection: 3.461s, learning 0.103s)
             Mean action noise std: 0.31
          Mean value_function loss: 0.0394
               Mean surrogate loss: -0.0097
                 Mean entropy loss: 2.4880
                       Mean reward: 108.52
               Mean episode length: 1979.99
Episode_Reward/track_lin_vel_xy_exp: 4.8419
Episode_Reward/track_ang_vel_z_exp: 0.0892
       Episode_Reward/lin_vel_z_l2: -0.0717
      Episode_Reward/ang_vel_xy_l2: -0.2796
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -1.1108
     Episode_Reward/action_rate_l2: -0.0412
Episode_Reward/flat_orientation_l2: -0.0441
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2834432
                    Iteration time: 3.56s
                      Time elapsed: 00:10:39
                               ETA: 00:20:13

################################################################################
                      [1m Learning iteration 173/500 [0m                      

                       Computation: 4597 steps/s (collection: 3.460s, learning 0.104s)
             Mean action noise std: 0.31
          Mean value_function loss: 0.0589
               Mean surrogate loss: -0.0069
                 Mean entropy loss: 2.4363
                       Mean reward: 108.77
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.1232
Episode_Reward/track_ang_vel_z_exp: 0.0893
       Episode_Reward/lin_vel_z_l2: -0.0607
      Episode_Reward/ang_vel_xy_l2: -0.2479
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -1.0070
     Episode_Reward/action_rate_l2: -0.0377
Episode_Reward/flat_orientation_l2: -0.0394
    Episode_Reward/foot_contact_l2: -0.8399
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 0.9062
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 3.56s
                      Time elapsed: 00:10:43
                               ETA: 00:20:09

################################################################################
                      [1m Learning iteration 174/500 [0m                      

                       Computation: 4640 steps/s (collection: 3.424s, learning 0.106s)
             Mean action noise std: 0.30
          Mean value_function loss: 0.0387
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 2.3616
                       Mean reward: 108.88
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.3250
Episode_Reward/track_ang_vel_z_exp: 0.0865
       Episode_Reward/lin_vel_z_l2: -0.0869
      Episode_Reward/ang_vel_xy_l2: -0.3254
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.9541
     Episode_Reward/action_rate_l2: -0.0406
Episode_Reward/flat_orientation_l2: -0.0511
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2867200
                    Iteration time: 3.53s
                      Time elapsed: 00:10:47
                               ETA: 00:20:05

################################################################################
                      [1m Learning iteration 175/500 [0m                      

                       Computation: 4611 steps/s (collection: 3.430s, learning 0.123s)
             Mean action noise std: 0.30
          Mean value_function loss: 0.0341
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 2.2809
                       Mean reward: 108.83
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 4.9783
Episode_Reward/track_ang_vel_z_exp: 0.0792
       Episode_Reward/lin_vel_z_l2: -0.0925
      Episode_Reward/ang_vel_xy_l2: -0.3346
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.9671
     Episode_Reward/action_rate_l2: -0.0420
Episode_Reward/flat_orientation_l2: -0.0468
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2883584
                    Iteration time: 3.55s
                      Time elapsed: 00:10:50
                               ETA: 00:20:01

################################################################################
                      [1m Learning iteration 176/500 [0m                      

                       Computation: 4653 steps/s (collection: 3.425s, learning 0.096s)
             Mean action noise std: 0.30
          Mean value_function loss: 0.0468
               Mean surrogate loss: -0.0095
                 Mean entropy loss: 2.1829
                       Mean reward: 108.94
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 4.8947
Episode_Reward/track_ang_vel_z_exp: 0.0832
       Episode_Reward/lin_vel_z_l2: -0.0871
      Episode_Reward/ang_vel_xy_l2: -0.3178
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -1.0129
     Episode_Reward/action_rate_l2: -0.0416
Episode_Reward/flat_orientation_l2: -0.0446
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2899968
                    Iteration time: 3.52s
                      Time elapsed: 00:10:54
                               ETA: 00:19:57

################################################################################
                      [1m Learning iteration 177/500 [0m                      

                       Computation: 4623 steps/s (collection: 3.425s, learning 0.119s)
             Mean action noise std: 0.30
          Mean value_function loss: 0.0359
               Mean surrogate loss: -0.0090
                 Mean entropy loss: 2.0862
                       Mean reward: 109.10
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 4.8767
Episode_Reward/track_ang_vel_z_exp: 0.0918
       Episode_Reward/lin_vel_z_l2: -0.0758
      Episode_Reward/ang_vel_xy_l2: -0.2861
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -1.0877
     Episode_Reward/action_rate_l2: -0.0405
Episode_Reward/flat_orientation_l2: -0.0416
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2916352
                    Iteration time: 3.54s
                      Time elapsed: 00:10:57
                               ETA: 00:19:53

################################################################################
                      [1m Learning iteration 178/500 [0m                      

                       Computation: 4639 steps/s (collection: 3.429s, learning 0.102s)
             Mean action noise std: 0.30
          Mean value_function loss: 0.0373
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 1.9913
                       Mean reward: 109.46
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 6.0274
Episode_Reward/track_ang_vel_z_exp: 0.0950
       Episode_Reward/lin_vel_z_l2: -0.0631
      Episode_Reward/ang_vel_xy_l2: -0.2478
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -1.0465
     Episode_Reward/action_rate_l2: -0.0395
Episode_Reward/flat_orientation_l2: -0.0375
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2932736
                    Iteration time: 3.53s
                      Time elapsed: 00:11:01
                               ETA: 00:19:49

################################################################################
                      [1m Learning iteration 179/500 [0m                      

                       Computation: 4619 steps/s (collection: 3.442s, learning 0.105s)
             Mean action noise std: 0.29
          Mean value_function loss: 0.0376
               Mean surrogate loss: -0.0069
                 Mean entropy loss: 1.8834
                       Mean reward: 110.28
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.7528
Episode_Reward/track_ang_vel_z_exp: 0.1000
       Episode_Reward/lin_vel_z_l2: -0.0630
      Episode_Reward/ang_vel_xy_l2: -0.2265
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -1.0467
     Episode_Reward/action_rate_l2: -0.0380
Episode_Reward/flat_orientation_l2: -0.0367
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 3.55s
                      Time elapsed: 00:11:04
                               ETA: 00:19:45

################################################################################
                      [1m Learning iteration 180/500 [0m                      

                       Computation: 4638 steps/s (collection: 3.430s, learning 0.101s)
             Mean action noise std: 0.29
          Mean value_function loss: 0.0343
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 1.8038
                       Mean reward: 110.51
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.7960
Episode_Reward/track_ang_vel_z_exp: 0.1008
       Episode_Reward/lin_vel_z_l2: -0.0616
      Episode_Reward/ang_vel_xy_l2: -0.2235
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -1.0775
     Episode_Reward/action_rate_l2: -0.0378
Episode_Reward/flat_orientation_l2: -0.0375
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2965504
                    Iteration time: 3.53s
                      Time elapsed: 00:11:08
                               ETA: 00:19:41

################################################################################
                      [1m Learning iteration 181/500 [0m                      

                       Computation: 4651 steps/s (collection: 3.427s, learning 0.095s)
             Mean action noise std: 0.29
          Mean value_function loss: 0.0326
               Mean surrogate loss: -0.0082
                 Mean entropy loss: 1.6771
                       Mean reward: 110.51
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.9723
Episode_Reward/track_ang_vel_z_exp: 0.1008
       Episode_Reward/lin_vel_z_l2: -0.0603
      Episode_Reward/ang_vel_xy_l2: -0.2256
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -1.1607
     Episode_Reward/action_rate_l2: -0.0379
Episode_Reward/flat_orientation_l2: -0.0403
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2981888
                    Iteration time: 3.52s
                      Time elapsed: 00:11:11
                               ETA: 00:19:37

################################################################################
                      [1m Learning iteration 182/500 [0m                      

                       Computation: 4672 steps/s (collection: 3.409s, learning 0.097s)
             Mean action noise std: 0.29
          Mean value_function loss: 0.0302
               Mean surrogate loss: -0.0093
                 Mean entropy loss: 1.5773
                       Mean reward: 110.51
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.9723
Episode_Reward/track_ang_vel_z_exp: 0.1008
       Episode_Reward/lin_vel_z_l2: -0.0603
      Episode_Reward/ang_vel_xy_l2: -0.2256
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -1.1607
     Episode_Reward/action_rate_l2: -0.0379
Episode_Reward/flat_orientation_l2: -0.0403
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2998272
                    Iteration time: 3.51s
                      Time elapsed: 00:11:15
                               ETA: 00:19:33

################################################################################
                      [1m Learning iteration 183/500 [0m                      

                       Computation: 4642 steps/s (collection: 3.427s, learning 0.102s)
             Mean action noise std: 0.28
          Mean value_function loss: 0.0357
               Mean surrogate loss: -0.0091
                 Mean entropy loss: 1.5017
                       Mean reward: 110.51
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.9723
Episode_Reward/track_ang_vel_z_exp: 0.1008
       Episode_Reward/lin_vel_z_l2: -0.0603
      Episode_Reward/ang_vel_xy_l2: -0.2256
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -1.1607
     Episode_Reward/action_rate_l2: -0.0379
Episode_Reward/flat_orientation_l2: -0.0403
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3014656
                    Iteration time: 3.53s
                      Time elapsed: 00:11:18
                               ETA: 00:19:29

################################################################################
                      [1m Learning iteration 184/500 [0m                      

                       Computation: 4649 steps/s (collection: 3.422s, learning 0.101s)
             Mean action noise std: 0.28
          Mean value_function loss: 0.0304
               Mean surrogate loss: -0.0073
                 Mean entropy loss: 1.4488
                       Mean reward: 110.51
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.9723
Episode_Reward/track_ang_vel_z_exp: 0.1008
       Episode_Reward/lin_vel_z_l2: -0.0603
      Episode_Reward/ang_vel_xy_l2: -0.2256
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -1.1607
     Episode_Reward/action_rate_l2: -0.0379
Episode_Reward/flat_orientation_l2: -0.0403
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3031040
                    Iteration time: 3.52s
                      Time elapsed: 00:11:22
                               ETA: 00:19:25

################################################################################
                      [1m Learning iteration 185/500 [0m                      

                       Computation: 4638 steps/s (collection: 3.434s, learning 0.097s)
             Mean action noise std: 0.28
          Mean value_function loss: 0.0337
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 1.3242
                       Mean reward: 110.87
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.7778
Episode_Reward/track_ang_vel_z_exp: 0.0944
       Episode_Reward/lin_vel_z_l2: -0.0648
      Episode_Reward/ang_vel_xy_l2: -0.2438
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -1.1431
     Episode_Reward/action_rate_l2: -0.0382
Episode_Reward/flat_orientation_l2: -0.0395
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 3.53s
                      Time elapsed: 00:11:25
                               ETA: 00:19:21

################################################################################
                      [1m Learning iteration 186/500 [0m                      

                       Computation: 4633 steps/s (collection: 3.433s, learning 0.103s)
             Mean action noise std: 0.28
          Mean value_function loss: 0.0332
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 1.2806
                       Mean reward: 110.87
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.4536
Episode_Reward/track_ang_vel_z_exp: 0.0837
       Episode_Reward/lin_vel_z_l2: -0.0722
      Episode_Reward/ang_vel_xy_l2: -0.2741
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -1.1137
     Episode_Reward/action_rate_l2: -0.0386
Episode_Reward/flat_orientation_l2: -0.0383
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3063808
                    Iteration time: 3.54s
                      Time elapsed: 00:11:29
                               ETA: 00:19:17

################################################################################
                      [1m Learning iteration 187/500 [0m                      

                       Computation: 4637 steps/s (collection: 3.426s, learning 0.107s)
             Mean action noise std: 0.28
          Mean value_function loss: 0.0325
               Mean surrogate loss: -0.0075
                 Mean entropy loss: 1.1554
                       Mean reward: 110.87
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.4536
Episode_Reward/track_ang_vel_z_exp: 0.0837
       Episode_Reward/lin_vel_z_l2: -0.0722
      Episode_Reward/ang_vel_xy_l2: -0.2741
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -1.1137
     Episode_Reward/action_rate_l2: -0.0386
Episode_Reward/flat_orientation_l2: -0.0383
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3080192
                    Iteration time: 3.53s
                      Time elapsed: 00:11:32
                               ETA: 00:19:13

################################################################################
                      [1m Learning iteration 188/500 [0m                      

                       Computation: 4551 steps/s (collection: 3.473s, learning 0.127s)
             Mean action noise std: 0.27
          Mean value_function loss: 0.0299
               Mean surrogate loss: -0.0084
                 Mean entropy loss: 1.0775
                       Mean reward: 110.87
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.4536
Episode_Reward/track_ang_vel_z_exp: 0.0837
       Episode_Reward/lin_vel_z_l2: -0.0722
      Episode_Reward/ang_vel_xy_l2: -0.2741
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -1.1137
     Episode_Reward/action_rate_l2: -0.0386
Episode_Reward/flat_orientation_l2: -0.0383
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3096576
                    Iteration time: 3.60s
                      Time elapsed: 00:11:36
                               ETA: 00:19:09

################################################################################
                      [1m Learning iteration 189/500 [0m                      

                       Computation: 4710 steps/s (collection: 3.382s, learning 0.097s)
             Mean action noise std: 0.27
          Mean value_function loss: 0.0279
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 0.9847
                       Mean reward: 110.87
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.4536
Episode_Reward/track_ang_vel_z_exp: 0.0837
       Episode_Reward/lin_vel_z_l2: -0.0722
      Episode_Reward/ang_vel_xy_l2: -0.2741
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -1.1137
     Episode_Reward/action_rate_l2: -0.0386
Episode_Reward/flat_orientation_l2: -0.0383
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3112960
                    Iteration time: 3.48s
                      Time elapsed: 00:11:40
                               ETA: 00:19:05

################################################################################
                      [1m Learning iteration 190/500 [0m                      

                       Computation: 4660 steps/s (collection: 3.419s, learning 0.096s)
             Mean action noise std: 0.27
          Mean value_function loss: 0.0270
               Mean surrogate loss: -0.0089
                 Mean entropy loss: 0.8693
                       Mean reward: 110.87
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.4536
Episode_Reward/track_ang_vel_z_exp: 0.0837
       Episode_Reward/lin_vel_z_l2: -0.0722
      Episode_Reward/ang_vel_xy_l2: -0.2741
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -1.1137
     Episode_Reward/action_rate_l2: -0.0386
Episode_Reward/flat_orientation_l2: -0.0383
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3129344
                    Iteration time: 3.52s
                      Time elapsed: 00:11:43
                               ETA: 00:19:01

################################################################################
                      [1m Learning iteration 191/500 [0m                      

                       Computation: 4578 steps/s (collection: 3.450s, learning 0.128s)
             Mean action noise std: 0.27
          Mean value_function loss: 0.0294
               Mean surrogate loss: -0.0086
                 Mean entropy loss: 0.7591
                       Mean reward: 111.21
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.4194
Episode_Reward/track_ang_vel_z_exp: 0.0864
       Episode_Reward/lin_vel_z_l2: -0.0717
      Episode_Reward/ang_vel_xy_l2: -0.2682
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -1.0694
     Episode_Reward/action_rate_l2: -0.0379
Episode_Reward/flat_orientation_l2: -0.0373
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 3.58s
                      Time elapsed: 00:11:47
                               ETA: 00:18:57

################################################################################
                      [1m Learning iteration 192/500 [0m                      

                       Computation: 4630 steps/s (collection: 3.436s, learning 0.101s)
             Mean action noise std: 0.26
          Mean value_function loss: 0.0274
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 0.6769
                       Mean reward: 111.85
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.8487
Episode_Reward/track_ang_vel_z_exp: 0.0950
       Episode_Reward/lin_vel_z_l2: -0.0652
      Episode_Reward/ang_vel_xy_l2: -0.2320
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.9648
     Episode_Reward/action_rate_l2: -0.0350
Episode_Reward/flat_orientation_l2: -0.0326
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3162112
                    Iteration time: 3.54s
                      Time elapsed: 00:11:50
                               ETA: 00:18:54

################################################################################
                      [1m Learning iteration 193/500 [0m                      

                       Computation: 4655 steps/s (collection: 3.422s, learning 0.097s)
             Mean action noise std: 0.26
          Mean value_function loss: 0.0295
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 0.5565
                       Mean reward: 112.23
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 6.0998
Episode_Reward/track_ang_vel_z_exp: 0.0993
       Episode_Reward/lin_vel_z_l2: -0.0641
      Episode_Reward/ang_vel_xy_l2: -0.2241
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.9678
     Episode_Reward/action_rate_l2: -0.0346
Episode_Reward/flat_orientation_l2: -0.0320
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3178496
                    Iteration time: 3.52s
                      Time elapsed: 00:11:54
                               ETA: 00:18:50

################################################################################
                      [1m Learning iteration 194/500 [0m                      

                       Computation: 4720 steps/s (collection: 3.371s, learning 0.099s)
             Mean action noise std: 0.26
          Mean value_function loss: 0.0251
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 0.4581
                       Mean reward: 112.76
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.8106
Episode_Reward/track_ang_vel_z_exp: 0.0934
       Episode_Reward/lin_vel_z_l2: -0.0714
      Episode_Reward/ang_vel_xy_l2: -0.2445
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.9084
     Episode_Reward/action_rate_l2: -0.0351
Episode_Reward/flat_orientation_l2: -0.0327
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3194880
                    Iteration time: 3.47s
                      Time elapsed: 00:11:57
                               ETA: 00:18:46

################################################################################
                      [1m Learning iteration 195/500 [0m                      

                       Computation: 4694 steps/s (collection: 3.390s, learning 0.100s)
             Mean action noise std: 0.26
          Mean value_function loss: 0.0243
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 0.3596
                       Mean reward: 112.76
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.9119
Episode_Reward/track_ang_vel_z_exp: 0.0952
       Episode_Reward/lin_vel_z_l2: -0.0616
      Episode_Reward/ang_vel_xy_l2: -0.2344
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.8797
     Episode_Reward/action_rate_l2: -0.0346
Episode_Reward/flat_orientation_l2: -0.0323
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3211264
                    Iteration time: 3.49s
                      Time elapsed: 00:12:01
                               ETA: 00:18:42

################################################################################
                      [1m Learning iteration 196/500 [0m                      

                       Computation: 4709 steps/s (collection: 3.379s, learning 0.100s)
             Mean action noise std: 0.26
          Mean value_function loss: 0.0232
               Mean surrogate loss: -0.0093
                 Mean entropy loss: 0.2414
                       Mean reward: 113.16
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.8448
Episode_Reward/track_ang_vel_z_exp: 0.0940
       Episode_Reward/lin_vel_z_l2: -0.0633
      Episode_Reward/ang_vel_xy_l2: -0.2355
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.8831
     Episode_Reward/action_rate_l2: -0.0345
Episode_Reward/flat_orientation_l2: -0.0323
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3227648
                    Iteration time: 3.48s
                      Time elapsed: 00:12:04
                               ETA: 00:18:38

################################################################################
                      [1m Learning iteration 197/500 [0m                      

                       Computation: 4592 steps/s (collection: 3.470s, learning 0.098s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0295
               Mean surrogate loss: -0.0094
                 Mean entropy loss: 0.1609
                       Mean reward: 113.16
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.5540
Episode_Reward/track_ang_vel_z_exp: 0.0889
       Episode_Reward/lin_vel_z_l2: -0.0704
      Episode_Reward/ang_vel_xy_l2: -0.2406
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.8977
     Episode_Reward/action_rate_l2: -0.0343
Episode_Reward/flat_orientation_l2: -0.0320
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 3.57s
                      Time elapsed: 00:12:08
                               ETA: 00:18:34

################################################################################
                      [1m Learning iteration 198/500 [0m                      

                       Computation: 4616 steps/s (collection: 3.449s, learning 0.100s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0267
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 0.0499
                       Mean reward: 113.16
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.5540
Episode_Reward/track_ang_vel_z_exp: 0.0889
       Episode_Reward/lin_vel_z_l2: -0.0704
      Episode_Reward/ang_vel_xy_l2: -0.2406
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.8977
     Episode_Reward/action_rate_l2: -0.0343
Episode_Reward/flat_orientation_l2: -0.0320
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3260416
                    Iteration time: 3.55s
                      Time elapsed: 00:12:11
                               ETA: 00:18:30

################################################################################
                      [1m Learning iteration 199/500 [0m                      

                       Computation: 2887 steps/s (collection: 5.586s, learning 0.088s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0239
               Mean surrogate loss: -0.0084
                 Mean entropy loss: -0.0303
                       Mean reward: 113.71
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.7004
Episode_Reward/track_ang_vel_z_exp: 0.0899
       Episode_Reward/lin_vel_z_l2: -0.0709
      Episode_Reward/ang_vel_xy_l2: -0.2491
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.9110
     Episode_Reward/action_rate_l2: -0.0342
Episode_Reward/flat_orientation_l2: -0.0333
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3276800
                    Iteration time: 5.67s
                      Time elapsed: 00:12:17
                               ETA: 00:18:29

################################################################################
                      [1m Learning iteration 200/500 [0m                      

                       Computation: 2714 steps/s (collection: 5.947s, learning 0.090s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0234
               Mean surrogate loss: -0.0068
                 Mean entropy loss: -0.1332
                       Mean reward: 113.71
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.8900
Episode_Reward/track_ang_vel_z_exp: 0.0826
       Episode_Reward/lin_vel_z_l2: -0.0757
      Episode_Reward/ang_vel_xy_l2: -0.2726
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.9570
     Episode_Reward/action_rate_l2: -0.0344
Episode_Reward/flat_orientation_l2: -0.0360
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3293184
                    Iteration time: 6.04s
                      Time elapsed: 00:12:23
                               ETA: 00:18:29

################################################################################
                      [1m Learning iteration 201/500 [0m                      

                       Computation: 2816 steps/s (collection: 5.728s, learning 0.089s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0259
               Mean surrogate loss: -0.0089
                 Mean entropy loss: -0.2624
                       Mean reward: 113.71
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.8900
Episode_Reward/track_ang_vel_z_exp: 0.0826
       Episode_Reward/lin_vel_z_l2: -0.0757
      Episode_Reward/ang_vel_xy_l2: -0.2726
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.9570
     Episode_Reward/action_rate_l2: -0.0344
Episode_Reward/flat_orientation_l2: -0.0360
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3309568
                    Iteration time: 5.82s
                      Time elapsed: 00:12:29
                               ETA: 00:18:29

################################################################################
                      [1m Learning iteration 202/500 [0m                      

                       Computation: 2817 steps/s (collection: 5.720s, learning 0.096s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0265
               Mean surrogate loss: -0.0091
                 Mean entropy loss: -0.3440
                       Mean reward: 113.71
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.8900
Episode_Reward/track_ang_vel_z_exp: 0.0826
       Episode_Reward/lin_vel_z_l2: -0.0757
      Episode_Reward/ang_vel_xy_l2: -0.2726
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.9570
     Episode_Reward/action_rate_l2: -0.0344
Episode_Reward/flat_orientation_l2: -0.0360
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3325952
                    Iteration time: 5.82s
                      Time elapsed: 00:12:35
                               ETA: 00:18:28

################################################################################
                      [1m Learning iteration 203/500 [0m                      

                       Computation: 2776 steps/s (collection: 5.817s, learning 0.084s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0334
               Mean surrogate loss: -0.0084
                 Mean entropy loss: -0.3817
                       Mean reward: 113.87
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.5209
Episode_Reward/track_ang_vel_z_exp: 0.0891
       Episode_Reward/lin_vel_z_l2: -0.0716
      Episode_Reward/ang_vel_xy_l2: -0.2415
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.9060
     Episode_Reward/action_rate_l2: -0.0334
Episode_Reward/flat_orientation_l2: -0.0332
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 5.90s
                      Time elapsed: 00:12:40
                               ETA: 00:18:27

################################################################################
                      [1m Learning iteration 204/500 [0m                      

                       Computation: 2790 steps/s (collection: 5.786s, learning 0.085s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0234
               Mean surrogate loss: -0.0077
                 Mean entropy loss: -0.4552
                       Mean reward: 113.87
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.2995
Episode_Reward/track_ang_vel_z_exp: 0.0930
       Episode_Reward/lin_vel_z_l2: -0.0691
      Episode_Reward/ang_vel_xy_l2: -0.2228
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.8754
     Episode_Reward/action_rate_l2: -0.0328
Episode_Reward/flat_orientation_l2: -0.0315
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3358720
                    Iteration time: 5.87s
                      Time elapsed: 00:12:46
                               ETA: 00:18:27

################################################################################
                      [1m Learning iteration 205/500 [0m                      

                       Computation: 2817 steps/s (collection: 5.730s, learning 0.085s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0235
               Mean surrogate loss: -0.0074
                 Mean entropy loss: -0.5297
                       Mean reward: 113.87
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.2995
Episode_Reward/track_ang_vel_z_exp: 0.0930
       Episode_Reward/lin_vel_z_l2: -0.0691
      Episode_Reward/ang_vel_xy_l2: -0.2228
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.8754
     Episode_Reward/action_rate_l2: -0.0328
Episode_Reward/flat_orientation_l2: -0.0315
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3375104
                    Iteration time: 5.82s
                      Time elapsed: 00:12:52
                               ETA: 00:18:26

################################################################################
                      [1m Learning iteration 206/500 [0m                      

                       Computation: 2831 steps/s (collection: 5.700s, learning 0.086s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0230
               Mean surrogate loss: -0.0105
                 Mean entropy loss: -0.6465
                       Mean reward: 113.87
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.2995
Episode_Reward/track_ang_vel_z_exp: 0.0930
       Episode_Reward/lin_vel_z_l2: -0.0691
      Episode_Reward/ang_vel_xy_l2: -0.2228
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.8754
     Episode_Reward/action_rate_l2: -0.0328
Episode_Reward/flat_orientation_l2: -0.0315
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3391488
                    Iteration time: 5.79s
                      Time elapsed: 00:12:58
                               ETA: 00:18:25

################################################################################
                      [1m Learning iteration 207/500 [0m                      

                       Computation: 2796 steps/s (collection: 5.769s, learning 0.090s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0219
               Mean surrogate loss: -0.0094
                 Mean entropy loss: -0.7135
                       Mean reward: 113.87
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.2995
Episode_Reward/track_ang_vel_z_exp: 0.0930
       Episode_Reward/lin_vel_z_l2: -0.0691
      Episode_Reward/ang_vel_xy_l2: -0.2228
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.8754
     Episode_Reward/action_rate_l2: -0.0328
Episode_Reward/flat_orientation_l2: -0.0315
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3407872
                    Iteration time: 5.86s
                      Time elapsed: 00:13:04
                               ETA: 00:18:24

################################################################################
                      [1m Learning iteration 208/500 [0m                      

                       Computation: 2776 steps/s (collection: 5.818s, learning 0.084s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0203
               Mean surrogate loss: -0.0074
                 Mean entropy loss: -0.8210
                       Mean reward: 113.87
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.2995
Episode_Reward/track_ang_vel_z_exp: 0.0930
       Episode_Reward/lin_vel_z_l2: -0.0691
      Episode_Reward/ang_vel_xy_l2: -0.2228
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.8754
     Episode_Reward/action_rate_l2: -0.0328
Episode_Reward/flat_orientation_l2: -0.0315
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3424256
                    Iteration time: 5.90s
                      Time elapsed: 00:13:10
                               ETA: 00:18:23

################################################################################
                      [1m Learning iteration 209/500 [0m                      

                       Computation: 2778 steps/s (collection: 5.809s, learning 0.086s)
             Mean action noise std: 0.23
          Mean value_function loss: 0.0218
               Mean surrogate loss: -0.0076
                 Mean entropy loss: -0.9133
                       Mean reward: 114.92
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 5.7573
Episode_Reward/track_ang_vel_z_exp: 0.0963
       Episode_Reward/lin_vel_z_l2: -0.0628
      Episode_Reward/ang_vel_xy_l2: -0.2101
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.8580
     Episode_Reward/action_rate_l2: -0.0311
Episode_Reward/flat_orientation_l2: -0.0299
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 5.90s
                      Time elapsed: 00:13:16
                               ETA: 00:18:23

################################################################################
                      [1m Learning iteration 210/500 [0m                      

                       Computation: 2800 steps/s (collection: 5.765s, learning 0.086s)
             Mean action noise std: 0.23
          Mean value_function loss: 0.0219
               Mean surrogate loss: -0.0076
                 Mean entropy loss: -1.0145
                       Mean reward: 115.57
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 6.4623
Episode_Reward/track_ang_vel_z_exp: 0.1006
       Episode_Reward/lin_vel_z_l2: -0.0620
      Episode_Reward/ang_vel_xy_l2: -0.2100
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.8288
     Episode_Reward/action_rate_l2: -0.0304
Episode_Reward/flat_orientation_l2: -0.0293
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3457024
                    Iteration time: 5.85s
                      Time elapsed: 00:13:21
                               ETA: 00:18:22

################################################################################
                      [1m Learning iteration 211/500 [0m                      

                       Computation: 2781 steps/s (collection: 5.758s, learning 0.132s)
             Mean action noise std: 0.23
          Mean value_function loss: 0.0204
               Mean surrogate loss: -0.0082
                 Mean entropy loss: -1.0981
                       Mean reward: 115.57
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 6.4077
Episode_Reward/track_ang_vel_z_exp: 0.1033
       Episode_Reward/lin_vel_z_l2: -0.0556
      Episode_Reward/ang_vel_xy_l2: -0.1875
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.8315
     Episode_Reward/action_rate_l2: -0.0296
Episode_Reward/flat_orientation_l2: -0.0302
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3473408
                    Iteration time: 5.89s
                      Time elapsed: 00:13:27
                               ETA: 00:18:21

################################################################################
                      [1m Learning iteration 212/500 [0m                      

                       Computation: 2796 steps/s (collection: 5.772s, learning 0.088s)
             Mean action noise std: 0.23
          Mean value_function loss: 0.0235
               Mean surrogate loss: -0.0071
                 Mean entropy loss: -1.1676
                       Mean reward: 115.57
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 6.4077
Episode_Reward/track_ang_vel_z_exp: 0.1033
       Episode_Reward/lin_vel_z_l2: -0.0556
      Episode_Reward/ang_vel_xy_l2: -0.1875
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.8315
     Episode_Reward/action_rate_l2: -0.0296
Episode_Reward/flat_orientation_l2: -0.0302
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3489792
                    Iteration time: 5.86s
                      Time elapsed: 00:13:33
                               ETA: 00:18:20

################################################################################
                      [1m Learning iteration 213/500 [0m                      

                       Computation: 2823 steps/s (collection: 5.717s, learning 0.085s)
             Mean action noise std: 0.23
          Mean value_function loss: 0.0202
               Mean surrogate loss: -0.0093
                 Mean entropy loss: -1.2566
                       Mean reward: 115.57
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 6.4077
Episode_Reward/track_ang_vel_z_exp: 0.1033
       Episode_Reward/lin_vel_z_l2: -0.0556
      Episode_Reward/ang_vel_xy_l2: -0.1875
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.8315
     Episode_Reward/action_rate_l2: -0.0296
Episode_Reward/flat_orientation_l2: -0.0302
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3506176
                    Iteration time: 5.80s
                      Time elapsed: 00:13:39
                               ETA: 00:18:19

################################################################################
                      [1m Learning iteration 214/500 [0m                      

                       Computation: 2821 steps/s (collection: 5.708s, learning 0.098s)
             Mean action noise std: 0.23
          Mean value_function loss: 0.0216
               Mean surrogate loss: -0.0071
                 Mean entropy loss: -1.3297
                       Mean reward: 115.57
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 6.4077
Episode_Reward/track_ang_vel_z_exp: 0.1033
       Episode_Reward/lin_vel_z_l2: -0.0556
      Episode_Reward/ang_vel_xy_l2: -0.1875
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.8315
     Episode_Reward/action_rate_l2: -0.0296
Episode_Reward/flat_orientation_l2: -0.0302
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3522560
                    Iteration time: 5.81s
                      Time elapsed: 00:13:45
                               ETA: 00:18:17

################################################################################
                      [1m Learning iteration 215/500 [0m                      

                       Computation: 2799 steps/s (collection: 5.747s, learning 0.106s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0231
               Mean surrogate loss: -0.0084
                 Mean entropy loss: -1.4200
                       Mean reward: 118.34
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 6.1307
Episode_Reward/track_ang_vel_z_exp: 0.0974
       Episode_Reward/lin_vel_z_l2: -0.0612
      Episode_Reward/ang_vel_xy_l2: -0.2005
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.7962
     Episode_Reward/action_rate_l2: -0.0294
Episode_Reward/flat_orientation_l2: -0.0297
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 5.85s
                      Time elapsed: 00:13:51
                               ETA: 00:18:16

################################################################################
                      [1m Learning iteration 216/500 [0m                      

                       Computation: 2775 steps/s (collection: 5.817s, learning 0.086s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0192
               Mean surrogate loss: -0.0089
                 Mean entropy loss: -1.5036
                       Mean reward: 123.29
               Mean episode length: 1965.86
Episode_Reward/track_lin_vel_xy_exp: 6.1910
Episode_Reward/track_ang_vel_z_exp: 0.0981
       Episode_Reward/lin_vel_z_l2: -0.0647
      Episode_Reward/ang_vel_xy_l2: -0.2064
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.7601
     Episode_Reward/action_rate_l2: -0.0288
Episode_Reward/flat_orientation_l2: -0.0295
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3555328
                    Iteration time: 5.90s
                      Time elapsed: 00:13:57
                               ETA: 00:18:15

################################################################################
                      [1m Learning iteration 217/500 [0m                      

                       Computation: 2781 steps/s (collection: 5.806s, learning 0.085s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0172
               Mean surrogate loss: -0.0078
                 Mean entropy loss: -1.5956
                       Mean reward: 131.88
               Mean episode length: 1984.87
Episode_Reward/track_lin_vel_xy_exp: 6.0267
Episode_Reward/track_ang_vel_z_exp: 0.0994
       Episode_Reward/lin_vel_z_l2: -0.0594
      Episode_Reward/ang_vel_xy_l2: -0.1944
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.8019
     Episode_Reward/action_rate_l2: -0.0281
Episode_Reward/flat_orientation_l2: -0.0279
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0312
--------------------------------------------------------------------------------
                   Total timesteps: 3571712
                    Iteration time: 5.89s
                      Time elapsed: 00:14:02
                               ETA: 00:18:14

################################################################################
                      [1m Learning iteration 218/500 [0m                      

                       Computation: 2767 steps/s (collection: 5.835s, learning 0.085s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0188
               Mean surrogate loss: -0.0069
                 Mean entropy loss: -1.6682
                       Mean reward: 150.15
               Mean episode length: 1984.87
Episode_Reward/track_lin_vel_xy_exp: 6.1978
Episode_Reward/track_ang_vel_z_exp: 0.0984
       Episode_Reward/lin_vel_z_l2: -0.0631
      Episode_Reward/ang_vel_xy_l2: -0.1999
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.8184
     Episode_Reward/action_rate_l2: -0.0282
Episode_Reward/flat_orientation_l2: -0.0281
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.7812
--------------------------------------------------------------------------------
                   Total timesteps: 3588096
                    Iteration time: 5.92s
                      Time elapsed: 00:14:08
                               ETA: 00:18:13

################################################################################
                      [1m Learning iteration 219/500 [0m                      

                       Computation: 2773 steps/s (collection: 5.823s, learning 0.084s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0198
               Mean surrogate loss: -0.0070
                 Mean entropy loss: -1.7815
                       Mean reward: 169.21
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.1659
Episode_Reward/track_ang_vel_z_exp: 0.1010
       Episode_Reward/lin_vel_z_l2: -0.0600
      Episode_Reward/ang_vel_xy_l2: -0.1910
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.8030
     Episode_Reward/action_rate_l2: -0.0277
Episode_Reward/flat_orientation_l2: -0.0271
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 2.1562
--------------------------------------------------------------------------------
                   Total timesteps: 3604480
                    Iteration time: 5.91s
                      Time elapsed: 00:14:14
                               ETA: 00:18:11

################################################################################
                      [1m Learning iteration 220/500 [0m                      

                       Computation: 2755 steps/s (collection: 5.846s, learning 0.099s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0175
               Mean surrogate loss: -0.0087
                 Mean entropy loss: -1.8414
                       Mean reward: 169.65
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.1002
Episode_Reward/track_ang_vel_z_exp: 0.1016
       Episode_Reward/lin_vel_z_l2: -0.0589
      Episode_Reward/ang_vel_xy_l2: -0.1894
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.8015
     Episode_Reward/action_rate_l2: -0.0273
Episode_Reward/flat_orientation_l2: -0.0272
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 2.0625
--------------------------------------------------------------------------------
                   Total timesteps: 3620864
                    Iteration time: 5.95s
                      Time elapsed: 00:14:20
                               ETA: 00:18:10

################################################################################
                      [1m Learning iteration 221/500 [0m                      

                       Computation: 2764 steps/s (collection: 5.838s, learning 0.088s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0175
               Mean surrogate loss: -0.0077
                 Mean entropy loss: -1.8971
                       Mean reward: 171.13
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.1733
Episode_Reward/track_ang_vel_z_exp: 0.1012
       Episode_Reward/lin_vel_z_l2: -0.0588
      Episode_Reward/ang_vel_xy_l2: -0.1876
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.7875
     Episode_Reward/action_rate_l2: -0.0271
Episode_Reward/flat_orientation_l2: -0.0271
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 5.93s
                      Time elapsed: 00:14:26
                               ETA: 00:18:09

################################################################################
                      [1m Learning iteration 222/500 [0m                      

                       Computation: 2779 steps/s (collection: 5.806s, learning 0.087s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0168
               Mean surrogate loss: -0.0089
                 Mean entropy loss: -1.9943
                       Mean reward: 171.97
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.1596
Episode_Reward/track_ang_vel_z_exp: 0.1020
       Episode_Reward/lin_vel_z_l2: -0.0565
      Episode_Reward/ang_vel_xy_l2: -0.1814
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.7754
     Episode_Reward/action_rate_l2: -0.0266
Episode_Reward/flat_orientation_l2: -0.0262
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 2.0625
--------------------------------------------------------------------------------
                   Total timesteps: 3653632
                    Iteration time: 5.89s
                      Time elapsed: 00:14:32
                               ETA: 00:18:07

################################################################################
                      [1m Learning iteration 223/500 [0m                      

                       Computation: 2762 steps/s (collection: 5.847s, learning 0.085s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0182
               Mean surrogate loss: -0.0092
                 Mean entropy loss: -2.0720
                       Mean reward: 172.82
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.1456
Episode_Reward/track_ang_vel_z_exp: 0.1031
       Episode_Reward/lin_vel_z_l2: -0.0572
      Episode_Reward/ang_vel_xy_l2: -0.1798
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.7783
     Episode_Reward/action_rate_l2: -0.0266
Episode_Reward/flat_orientation_l2: -0.0262
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.5312
--------------------------------------------------------------------------------
                   Total timesteps: 3670016
                    Iteration time: 5.93s
                      Time elapsed: 00:14:38
                               ETA: 00:18:06

################################################################################
                      [1m Learning iteration 224/500 [0m                      

                       Computation: 2777 steps/s (collection: 5.815s, learning 0.084s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0061
                 Mean entropy loss: -2.1690
                       Mean reward: 173.29
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.2082
Episode_Reward/track_ang_vel_z_exp: 0.1005
       Episode_Reward/lin_vel_z_l2: -0.0603
      Episode_Reward/ang_vel_xy_l2: -0.1876
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.7454
     Episode_Reward/action_rate_l2: -0.0263
Episode_Reward/flat_orientation_l2: -0.0266
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.4375
--------------------------------------------------------------------------------
                   Total timesteps: 3686400
                    Iteration time: 5.90s
                      Time elapsed: 00:14:44
                               ETA: 00:18:04

################################################################################
                      [1m Learning iteration 225/500 [0m                      

                       Computation: 2780 steps/s (collection: 5.796s, learning 0.095s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0175
               Mean surrogate loss: -0.0071
                 Mean entropy loss: -2.2223
                       Mean reward: 175.56
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.3404
Episode_Reward/track_ang_vel_z_exp: 0.1024
       Episode_Reward/lin_vel_z_l2: -0.0582
      Episode_Reward/ang_vel_xy_l2: -0.1824
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.7619
     Episode_Reward/action_rate_l2: -0.0262
Episode_Reward/flat_orientation_l2: -0.0254
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.4062
--------------------------------------------------------------------------------
                   Total timesteps: 3702784
                    Iteration time: 5.89s
                      Time elapsed: 00:14:50
                               ETA: 00:18:03

################################################################################
                      [1m Learning iteration 226/500 [0m                      

                       Computation: 2784 steps/s (collection: 5.789s, learning 0.095s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0073
                 Mean entropy loss: -2.3160
                       Mean reward: 176.21
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.2931
Episode_Reward/track_ang_vel_z_exp: 0.1039
       Episode_Reward/lin_vel_z_l2: -0.0575
      Episode_Reward/ang_vel_xy_l2: -0.1735
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.7631
     Episode_Reward/action_rate_l2: -0.0255
Episode_Reward/flat_orientation_l2: -0.0254
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 3719168
                    Iteration time: 5.88s
                      Time elapsed: 00:14:56
                               ETA: 00:18:01

################################################################################
                      [1m Learning iteration 227/500 [0m                      

                       Computation: 2794 steps/s (collection: 5.772s, learning 0.091s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0180
               Mean surrogate loss: -0.0059
                 Mean entropy loss: -2.4152
                       Mean reward: 176.39
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.2148
Episode_Reward/track_ang_vel_z_exp: 0.1030
       Episode_Reward/lin_vel_z_l2: -0.0571
      Episode_Reward/ang_vel_xy_l2: -0.1762
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.7483
     Episode_Reward/action_rate_l2: -0.0256
Episode_Reward/flat_orientation_l2: -0.0245
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.2188
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 5.86s
                      Time elapsed: 00:15:02
                               ETA: 00:18:00

################################################################################
                      [1m Learning iteration 228/500 [0m                      

                       Computation: 2813 steps/s (collection: 5.740s, learning 0.083s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0169
               Mean surrogate loss: -0.0059
                 Mean entropy loss: -2.4782
                       Mean reward: 178.20
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.3556
Episode_Reward/track_ang_vel_z_exp: 0.1004
       Episode_Reward/lin_vel_z_l2: -0.0602
      Episode_Reward/ang_vel_xy_l2: -0.1786
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.7222
     Episode_Reward/action_rate_l2: -0.0251
Episode_Reward/flat_orientation_l2: -0.0245
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 3751936
                    Iteration time: 5.82s
                      Time elapsed: 00:15:07
                               ETA: 00:17:58

################################################################################
                      [1m Learning iteration 229/500 [0m                      

                       Computation: 2822 steps/s (collection: 5.714s, learning 0.091s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0097
                 Mean entropy loss: -2.5705
                       Mean reward: 177.32
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 5.8984
Episode_Reward/track_ang_vel_z_exp: 0.1072
       Episode_Reward/lin_vel_z_l2: -0.0557
      Episode_Reward/ang_vel_xy_l2: -0.1703
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.7217
     Episode_Reward/action_rate_l2: -0.0249
Episode_Reward/flat_orientation_l2: -0.0252
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.3125
--------------------------------------------------------------------------------
                   Total timesteps: 3768320
                    Iteration time: 5.80s
                      Time elapsed: 00:15:13
                               ETA: 00:17:56

################################################################################
                      [1m Learning iteration 230/500 [0m                      

                       Computation: 2832 steps/s (collection: 5.700s, learning 0.085s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0210
               Mean surrogate loss: -0.0085
                 Mean entropy loss: -2.6221
                       Mean reward: 176.42
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.0006
Episode_Reward/track_ang_vel_z_exp: 0.1056
       Episode_Reward/lin_vel_z_l2: -0.0531
      Episode_Reward/ang_vel_xy_l2: -0.1606
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.7562
     Episode_Reward/action_rate_l2: -0.0245
Episode_Reward/flat_orientation_l2: -0.0255
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3784704
                    Iteration time: 5.79s
                      Time elapsed: 00:15:19
                               ETA: 00:17:54

################################################################################
                      [1m Learning iteration 231/500 [0m                      

                       Computation: 2824 steps/s (collection: 5.714s, learning 0.086s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0084
                 Mean entropy loss: -2.7030
                       Mean reward: 176.15
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.2556
Episode_Reward/track_ang_vel_z_exp: 0.1106
       Episode_Reward/lin_vel_z_l2: -0.0476
      Episode_Reward/ang_vel_xy_l2: -0.1521
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.7385
     Episode_Reward/action_rate_l2: -0.0239
Episode_Reward/flat_orientation_l2: -0.0252
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0312
--------------------------------------------------------------------------------
                   Total timesteps: 3801088
                    Iteration time: 5.80s
                      Time elapsed: 00:15:25
                               ETA: 00:17:52

################################################################################
                      [1m Learning iteration 232/500 [0m                      

                       Computation: 2807 steps/s (collection: 5.725s, learning 0.112s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0077
                 Mean entropy loss: -2.7338
                       Mean reward: 177.01
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.6658
Episode_Reward/track_ang_vel_z_exp: 0.1085
       Episode_Reward/lin_vel_z_l2: -0.0482
      Episode_Reward/ang_vel_xy_l2: -0.1464
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.7159
     Episode_Reward/action_rate_l2: -0.0234
Episode_Reward/flat_orientation_l2: -0.0224
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3817472
                    Iteration time: 5.84s
                      Time elapsed: 00:15:31
                               ETA: 00:17:50

################################################################################
                      [1m Learning iteration 233/500 [0m                      

                       Computation: 2837 steps/s (collection: 5.691s, learning 0.083s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0076
                 Mean entropy loss: -2.8190
                       Mean reward: 177.51
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.1568
Episode_Reward/track_ang_vel_z_exp: 0.1099
       Episode_Reward/lin_vel_z_l2: -0.0526
      Episode_Reward/ang_vel_xy_l2: -0.1557
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.7265
     Episode_Reward/action_rate_l2: -0.0235
Episode_Reward/flat_orientation_l2: -0.0232
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0938
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 5.77s
                      Time elapsed: 00:15:36
                               ETA: 00:17:48

################################################################################
                      [1m Learning iteration 234/500 [0m                      

                       Computation: 2830 steps/s (collection: 5.685s, learning 0.103s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0138
               Mean surrogate loss: -0.0080
                 Mean entropy loss: -2.9033
                       Mean reward: 177.69
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.3148
Episode_Reward/track_ang_vel_z_exp: 0.1086
       Episode_Reward/lin_vel_z_l2: -0.0532
      Episode_Reward/ang_vel_xy_l2: -0.1601
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.6804
     Episode_Reward/action_rate_l2: -0.0235
Episode_Reward/flat_orientation_l2: -0.0237
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3850240
                    Iteration time: 5.79s
                      Time elapsed: 00:15:42
                               ETA: 00:17:46

################################################################################
                      [1m Learning iteration 235/500 [0m                      

                       Computation: 2823 steps/s (collection: 5.702s, learning 0.101s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0144
               Mean surrogate loss: -0.0064
                 Mean entropy loss: -2.9743
                       Mean reward: 177.72
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.1078
Episode_Reward/track_ang_vel_z_exp: 0.1094
       Episode_Reward/lin_vel_z_l2: -0.0511
      Episode_Reward/ang_vel_xy_l2: -0.1526
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.7167
     Episode_Reward/action_rate_l2: -0.0237
Episode_Reward/flat_orientation_l2: -0.0219
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3866624
                    Iteration time: 5.80s
                      Time elapsed: 00:15:48
                               ETA: 00:17:44

################################################################################
                      [1m Learning iteration 236/500 [0m                      

                       Computation: 2806 steps/s (collection: 5.739s, learning 0.099s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0074
                 Mean entropy loss: -3.0341
                       Mean reward: 178.29
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.4051
Episode_Reward/track_ang_vel_z_exp: 0.1013
       Episode_Reward/lin_vel_z_l2: -0.0602
      Episode_Reward/ang_vel_xy_l2: -0.1780
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.6937
     Episode_Reward/action_rate_l2: -0.0239
Episode_Reward/flat_orientation_l2: -0.0247
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3883008
                    Iteration time: 5.84s
                      Time elapsed: 00:15:54
                               ETA: 00:17:42

################################################################################
                      [1m Learning iteration 237/500 [0m                      

                       Computation: 2834 steps/s (collection: 5.697s, learning 0.083s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0137
               Mean surrogate loss: -0.0080
                 Mean entropy loss: -3.1019
                       Mean reward: 178.22
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.2115
Episode_Reward/track_ang_vel_z_exp: 0.0948
       Episode_Reward/lin_vel_z_l2: -0.0667
      Episode_Reward/ang_vel_xy_l2: -0.1918
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.6815
     Episode_Reward/action_rate_l2: -0.0244
Episode_Reward/flat_orientation_l2: -0.0248
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3899392
                    Iteration time: 5.78s
                      Time elapsed: 00:16:00
                               ETA: 00:17:40

################################################################################
                      [1m Learning iteration 238/500 [0m                      

                       Computation: 2840 steps/s (collection: 5.684s, learning 0.084s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0071
                 Mean entropy loss: -3.1759
                       Mean reward: 178.22
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.0981
Episode_Reward/track_ang_vel_z_exp: 0.0955
       Episode_Reward/lin_vel_z_l2: -0.0651
      Episode_Reward/ang_vel_xy_l2: -0.1818
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.6803
     Episode_Reward/action_rate_l2: -0.0244
Episode_Reward/flat_orientation_l2: -0.0224
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3915776
                    Iteration time: 5.77s
                      Time elapsed: 00:16:05
                               ETA: 00:17:38

################################################################################
                      [1m Learning iteration 239/500 [0m                      

                       Computation: 2834 steps/s (collection: 5.698s, learning 0.083s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0061
                 Mean entropy loss: -3.2727
                       Mean reward: 178.30
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.5833
Episode_Reward/track_ang_vel_z_exp: 0.1026
       Episode_Reward/lin_vel_z_l2: -0.0601
      Episode_Reward/ang_vel_xy_l2: -0.1606
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.7645
     Episode_Reward/action_rate_l2: -0.0230
Episode_Reward/flat_orientation_l2: -0.0222
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 5.78s
                      Time elapsed: 00:16:11
                               ETA: 00:17:36

################################################################################
                      [1m Learning iteration 240/500 [0m                      

                       Computation: 2815 steps/s (collection: 5.728s, learning 0.091s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0132
               Mean surrogate loss: -0.0057
                 Mean entropy loss: -3.3294
                       Mean reward: 178.60
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.6341
Episode_Reward/track_ang_vel_z_exp: 0.1061
       Episode_Reward/lin_vel_z_l2: -0.0563
      Episode_Reward/ang_vel_xy_l2: -0.1573
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.7200
     Episode_Reward/action_rate_l2: -0.0226
Episode_Reward/flat_orientation_l2: -0.0228
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3948544
                    Iteration time: 5.82s
                      Time elapsed: 00:16:17
                               ETA: 00:17:34

################################################################################
                      [1m Learning iteration 241/500 [0m                      

                       Computation: 2822 steps/s (collection: 5.713s, learning 0.092s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0140
               Mean surrogate loss: -0.0102
                 Mean entropy loss: -3.4357
                       Mean reward: 178.90
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.7834
Episode_Reward/track_ang_vel_z_exp: 0.1136
       Episode_Reward/lin_vel_z_l2: -0.0480
      Episode_Reward/ang_vel_xy_l2: -0.1411
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.6969
     Episode_Reward/action_rate_l2: -0.0218
Episode_Reward/flat_orientation_l2: -0.0217
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3964928
                    Iteration time: 5.80s
                      Time elapsed: 00:16:23
                               ETA: 00:17:32

################################################################################
                      [1m Learning iteration 242/500 [0m                      

                       Computation: 2825 steps/s (collection: 5.708s, learning 0.092s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0125
               Mean surrogate loss: -0.0077
                 Mean entropy loss: -3.5430
                       Mean reward: 178.90
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.9602
Episode_Reward/track_ang_vel_z_exp: 0.1172
       Episode_Reward/lin_vel_z_l2: -0.0473
      Episode_Reward/ang_vel_xy_l2: -0.1329
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.7295
     Episode_Reward/action_rate_l2: -0.0214
Episode_Reward/flat_orientation_l2: -0.0202
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3981312
                    Iteration time: 5.80s
                      Time elapsed: 00:16:29
                               ETA: 00:17:30

################################################################################
                      [1m Learning iteration 243/500 [0m                      

                       Computation: 2811 steps/s (collection: 5.720s, learning 0.108s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0132
               Mean surrogate loss: -0.0085
                 Mean entropy loss: -3.6733
                       Mean reward: 179.17
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.5973
Episode_Reward/track_ang_vel_z_exp: 0.1238
       Episode_Reward/lin_vel_z_l2: -0.0446
      Episode_Reward/ang_vel_xy_l2: -0.1327
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.7015
     Episode_Reward/action_rate_l2: -0.0213
Episode_Reward/flat_orientation_l2: -0.0225
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3997696
                    Iteration time: 5.83s
                      Time elapsed: 00:16:34
                               ETA: 00:17:27

################################################################################
                      [1m Learning iteration 244/500 [0m                      

                       Computation: 2818 steps/s (collection: 5.717s, learning 0.097s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0060
                 Mean entropy loss: -3.7099
                       Mean reward: 179.17
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.5136
Episode_Reward/track_ang_vel_z_exp: 0.1253
       Episode_Reward/lin_vel_z_l2: -0.0440
      Episode_Reward/ang_vel_xy_l2: -0.1326
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.6950
     Episode_Reward/action_rate_l2: -0.0212
Episode_Reward/flat_orientation_l2: -0.0230
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4014080
                    Iteration time: 5.81s
                      Time elapsed: 00:16:40
                               ETA: 00:17:25

################################################################################
                      [1m Learning iteration 245/500 [0m                      

                       Computation: 2841 steps/s (collection: 5.671s, learning 0.095s)
             Mean action noise std: 0.18
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0072
                 Mean entropy loss: -3.8355
                       Mean reward: 179.17
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.5136
Episode_Reward/track_ang_vel_z_exp: 0.1253
       Episode_Reward/lin_vel_z_l2: -0.0440
      Episode_Reward/ang_vel_xy_l2: -0.1326
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.6950
     Episode_Reward/action_rate_l2: -0.0212
Episode_Reward/flat_orientation_l2: -0.0230
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 5.77s
                      Time elapsed: 00:16:46
                               ETA: 00:17:23

################################################################################
                      [1m Learning iteration 246/500 [0m                      

                       Computation: 2845 steps/s (collection: 5.677s, learning 0.081s)
             Mean action noise std: 0.18
          Mean value_function loss: 0.0112
               Mean surrogate loss: -0.0069
                 Mean entropy loss: -3.9551
                       Mean reward: 179.17
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.5136
Episode_Reward/track_ang_vel_z_exp: 0.1253
       Episode_Reward/lin_vel_z_l2: -0.0440
      Episode_Reward/ang_vel_xy_l2: -0.1326
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.6950
     Episode_Reward/action_rate_l2: -0.0212
Episode_Reward/flat_orientation_l2: -0.0230
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4046848
                    Iteration time: 5.76s
                      Time elapsed: 00:16:52
                               ETA: 00:17:20

################################################################################
                      [1m Learning iteration 247/500 [0m                      

                       Computation: 2838 steps/s (collection: 5.691s, learning 0.082s)
             Mean action noise std: 0.18
          Mean value_function loss: 0.0123
               Mean surrogate loss: -0.0059
                 Mean entropy loss: -4.0485
                       Mean reward: 179.17
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.5136
Episode_Reward/track_ang_vel_z_exp: 0.1253
       Episode_Reward/lin_vel_z_l2: -0.0440
      Episode_Reward/ang_vel_xy_l2: -0.1326
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.6950
     Episode_Reward/action_rate_l2: -0.0212
Episode_Reward/flat_orientation_l2: -0.0230
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4063232
                    Iteration time: 5.77s
                      Time elapsed: 00:16:57
                               ETA: 00:17:18

################################################################################
                      [1m Learning iteration 248/500 [0m                      

                       Computation: 2841 steps/s (collection: 5.674s, learning 0.091s)
             Mean action noise std: 0.18
          Mean value_function loss: 0.0142
               Mean surrogate loss: -0.0039
                 Mean entropy loss: -4.1236
                       Mean reward: 179.30
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.7135
Episode_Reward/track_ang_vel_z_exp: 0.1090
       Episode_Reward/lin_vel_z_l2: -0.0575
      Episode_Reward/ang_vel_xy_l2: -0.1454
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.6662
     Episode_Reward/action_rate_l2: -0.0206
Episode_Reward/flat_orientation_l2: -0.0228
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4079616
                    Iteration time: 5.77s
                      Time elapsed: 00:17:03
                               ETA: 00:17:16

################################################################################
                      [1m Learning iteration 249/500 [0m                      

                       Computation: 2830 steps/s (collection: 5.692s, learning 0.096s)
             Mean action noise std: 0.18
          Mean value_function loss: 0.0195
               Mean surrogate loss: -0.0055
                 Mean entropy loss: -4.2275
                       Mean reward: 179.30
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.7342
Episode_Reward/track_ang_vel_z_exp: 0.1073
       Episode_Reward/lin_vel_z_l2: -0.0589
      Episode_Reward/ang_vel_xy_l2: -0.1467
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.6632
     Episode_Reward/action_rate_l2: -0.0205
Episode_Reward/flat_orientation_l2: -0.0228
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4096000
                    Iteration time: 5.79s
                      Time elapsed: 00:17:09
                               ETA: 00:17:13

################################################################################
                      [1m Learning iteration 250/500 [0m                      

                       Computation: 2847 steps/s (collection: 5.659s, learning 0.095s)
             Mean action noise std: 0.18
          Mean value_function loss: 0.0128
               Mean surrogate loss: -0.0082
                 Mean entropy loss: -4.2760
                       Mean reward: 179.30
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.7342
Episode_Reward/track_ang_vel_z_exp: 0.1073
       Episode_Reward/lin_vel_z_l2: -0.0589
      Episode_Reward/ang_vel_xy_l2: -0.1467
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.6632
     Episode_Reward/action_rate_l2: -0.0205
Episode_Reward/flat_orientation_l2: -0.0228
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4112384
                    Iteration time: 5.75s
                      Time elapsed: 00:17:15
                               ETA: 00:17:11

################################################################################
                      [1m Learning iteration 251/500 [0m                      

                       Computation: 2835 steps/s (collection: 5.694s, learning 0.085s)
             Mean action noise std: 0.18
          Mean value_function loss: 0.0129
               Mean surrogate loss: -0.0071
                 Mean entropy loss: -4.3343
                       Mean reward: 179.30
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.7342
Episode_Reward/track_ang_vel_z_exp: 0.1073
       Episode_Reward/lin_vel_z_l2: -0.0589
      Episode_Reward/ang_vel_xy_l2: -0.1467
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.6632
     Episode_Reward/action_rate_l2: -0.0205
Episode_Reward/flat_orientation_l2: -0.0228
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 5.78s
                      Time elapsed: 00:17:21
                               ETA: 00:17:08

################################################################################
                      [1m Learning iteration 252/500 [0m                      

                       Computation: 2813 steps/s (collection: 5.738s, learning 0.085s)
             Mean action noise std: 0.18
          Mean value_function loss: 0.0106
               Mean surrogate loss: -0.0091
                 Mean entropy loss: -4.4590
                       Mean reward: 179.30
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.7342
Episode_Reward/track_ang_vel_z_exp: 0.1073
       Episode_Reward/lin_vel_z_l2: -0.0589
      Episode_Reward/ang_vel_xy_l2: -0.1467
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.6632
     Episode_Reward/action_rate_l2: -0.0205
Episode_Reward/flat_orientation_l2: -0.0228
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4145152
                    Iteration time: 5.82s
                      Time elapsed: 00:17:26
                               ETA: 00:17:06

################################################################################
                      [1m Learning iteration 253/500 [0m                      

                       Computation: 2841 steps/s (collection: 5.681s, learning 0.086s)
             Mean action noise std: 0.17
          Mean value_function loss: 0.0131
               Mean surrogate loss: -0.0087
                 Mean entropy loss: -4.5357
                       Mean reward: 179.30
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.7342
Episode_Reward/track_ang_vel_z_exp: 0.1073
       Episode_Reward/lin_vel_z_l2: -0.0589
      Episode_Reward/ang_vel_xy_l2: -0.1467
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.6632
     Episode_Reward/action_rate_l2: -0.0205
Episode_Reward/flat_orientation_l2: -0.0228
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4161536
                    Iteration time: 5.77s
                      Time elapsed: 00:17:32
                               ETA: 00:17:03

################################################################################
                      [1m Learning iteration 254/500 [0m                      

                       Computation: 2832 steps/s (collection: 5.699s, learning 0.084s)
             Mean action noise std: 0.17
          Mean value_function loss: 0.0125
               Mean surrogate loss: -0.0077
                 Mean entropy loss: -4.6258
                       Mean reward: 179.71
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.3552
Episode_Reward/track_ang_vel_z_exp: 0.1046
       Episode_Reward/lin_vel_z_l2: -0.0587
      Episode_Reward/ang_vel_xy_l2: -0.1535
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.6503
     Episode_Reward/action_rate_l2: -0.0198
Episode_Reward/flat_orientation_l2: -0.0209
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4177920
                    Iteration time: 5.78s
                      Time elapsed: 00:17:38
                               ETA: 00:17:01

################################################################################
                      [1m Learning iteration 255/500 [0m                      

                       Computation: 2819 steps/s (collection: 5.728s, learning 0.082s)
             Mean action noise std: 0.17
          Mean value_function loss: 0.0119
               Mean surrogate loss: -0.0076
                 Mean entropy loss: -4.7023
                       Mean reward: 179.71
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.9149
Episode_Reward/track_ang_vel_z_exp: 0.1185
       Episode_Reward/lin_vel_z_l2: -0.0416
      Episode_Reward/ang_vel_xy_l2: -0.1132
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.6166
     Episode_Reward/action_rate_l2: -0.0183
Episode_Reward/flat_orientation_l2: -0.0198
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4194304
                    Iteration time: 5.81s
                      Time elapsed: 00:17:44
                               ETA: 00:16:58

################################################################################
                      [1m Learning iteration 256/500 [0m                      

                       Computation: 2742 steps/s (collection: 5.894s, learning 0.080s)
             Mean action noise std: 0.17
          Mean value_function loss: 0.0118
               Mean surrogate loss: -0.0063
                 Mean entropy loss: -4.7602
                       Mean reward: 179.86
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.4269
Episode_Reward/track_ang_vel_z_exp: 0.0997
       Episode_Reward/lin_vel_z_l2: -0.0535
      Episode_Reward/ang_vel_xy_l2: -0.1468
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.5520
     Episode_Reward/action_rate_l2: -0.0192
Episode_Reward/flat_orientation_l2: -0.0205
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4210688
                    Iteration time: 5.97s
                      Time elapsed: 00:17:50
                               ETA: 00:16:56

################################################################################
                      [1m Learning iteration 257/500 [0m                      

                       Computation: 2757 steps/s (collection: 5.857s, learning 0.085s)
             Mean action noise std: 0.17
          Mean value_function loss: 0.0120
               Mean surrogate loss: -0.0091
                 Mean entropy loss: -4.8535
                       Mean reward: 179.96
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.5725
Episode_Reward/track_ang_vel_z_exp: 0.1033
       Episode_Reward/lin_vel_z_l2: -0.0565
      Episode_Reward/ang_vel_xy_l2: -0.1435
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.6095
     Episode_Reward/action_rate_l2: -0.0192
Episode_Reward/flat_orientation_l2: -0.0203
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 5.94s
                      Time elapsed: 00:17:56
                               ETA: 00:16:53

################################################################################
                      [1m Learning iteration 258/500 [0m                      

                       Computation: 2834 steps/s (collection: 5.695s, learning 0.086s)
             Mean action noise std: 0.17
          Mean value_function loss: 0.0121
               Mean surrogate loss: -0.0056
                 Mean entropy loss: -4.9432
                       Mean reward: 179.96
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.6263
Episode_Reward/track_ang_vel_z_exp: 0.1047
       Episode_Reward/lin_vel_z_l2: -0.0573
      Episode_Reward/ang_vel_xy_l2: -0.1420
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.6294
     Episode_Reward/action_rate_l2: -0.0192
Episode_Reward/flat_orientation_l2: -0.0202
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4243456
                    Iteration time: 5.78s
                      Time elapsed: 00:18:01
                               ETA: 00:16:50

################################################################################
                      [1m Learning iteration 259/500 [0m                      

                       Computation: 2830 steps/s (collection: 5.705s, learning 0.084s)
             Mean action noise std: 0.17
          Mean value_function loss: 0.0116
               Mean surrogate loss: -0.0056
                 Mean entropy loss: -4.9883
                       Mean reward: 180.01
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.4861
Episode_Reward/track_ang_vel_z_exp: 0.1047
       Episode_Reward/lin_vel_z_l2: -0.0587
      Episode_Reward/ang_vel_xy_l2: -0.1462
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.6211
     Episode_Reward/action_rate_l2: -0.0191
Episode_Reward/flat_orientation_l2: -0.0200
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4259840
                    Iteration time: 5.79s
                      Time elapsed: 00:18:07
                               ETA: 00:16:48

################################################################################
                      [1m Learning iteration 260/500 [0m                      

                       Computation: 2820 steps/s (collection: 5.725s, learning 0.083s)
             Mean action noise std: 0.17
          Mean value_function loss: 0.0117
               Mean surrogate loss: -0.0080
                 Mean entropy loss: -5.0535
                       Mean reward: 180.01
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.4313
Episode_Reward/track_ang_vel_z_exp: 0.1047
       Episode_Reward/lin_vel_z_l2: -0.0593
      Episode_Reward/ang_vel_xy_l2: -0.1479
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.6179
     Episode_Reward/action_rate_l2: -0.0190
Episode_Reward/flat_orientation_l2: -0.0199
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4276224
                    Iteration time: 5.81s
                      Time elapsed: 00:18:13
                               ETA: 00:16:45

################################################################################
                      [1m Learning iteration 261/500 [0m                      

                       Computation: 2820 steps/s (collection: 5.726s, learning 0.083s)
             Mean action noise std: 0.17
          Mean value_function loss: 0.0130
               Mean surrogate loss: -0.0060
                 Mean entropy loss: -5.1317
                       Mean reward: 180.32
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.5033
Episode_Reward/track_ang_vel_z_exp: 0.1059
       Episode_Reward/lin_vel_z_l2: -0.0581
      Episode_Reward/ang_vel_xy_l2: -0.1463
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.6277
     Episode_Reward/action_rate_l2: -0.0189
Episode_Reward/flat_orientation_l2: -0.0200
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4292608
                    Iteration time: 5.81s
                      Time elapsed: 00:18:19
                               ETA: 00:16:42

################################################################################
                      [1m Learning iteration 262/500 [0m                      

                       Computation: 2846 steps/s (collection: 5.674s, learning 0.082s)
             Mean action noise std: 0.17
          Mean value_function loss: 0.0115
               Mean surrogate loss: -0.0072
                 Mean entropy loss: -5.1959
                       Mean reward: 180.39
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.5041
Episode_Reward/track_ang_vel_z_exp: 0.1046
       Episode_Reward/lin_vel_z_l2: -0.0607
      Episode_Reward/ang_vel_xy_l2: -0.1531
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.6149
     Episode_Reward/action_rate_l2: -0.0188
Episode_Reward/flat_orientation_l2: -0.0211
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4308992
                    Iteration time: 5.76s
                      Time elapsed: 00:18:25
                               ETA: 00:16:40

################################################################################
                      [1m Learning iteration 263/500 [0m                      

                       Computation: 2851 steps/s (collection: 5.660s, learning 0.086s)
             Mean action noise std: 0.16
          Mean value_function loss: 0.0110
               Mean surrogate loss: -0.0054
                 Mean entropy loss: -5.2616
                       Mean reward: 180.39
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.4487
Episode_Reward/track_ang_vel_z_exp: 0.1034
       Episode_Reward/lin_vel_z_l2: -0.0620
      Episode_Reward/ang_vel_xy_l2: -0.1553
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.6055
     Episode_Reward/action_rate_l2: -0.0188
Episode_Reward/flat_orientation_l2: -0.0212
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 5.75s
                      Time elapsed: 00:18:30
                               ETA: 00:16:37

################################################################################
                      [1m Learning iteration 264/500 [0m                      

                       Computation: 2833 steps/s (collection: 5.697s, learning 0.085s)
             Mean action noise std: 0.16
          Mean value_function loss: 0.0113
               Mean surrogate loss: -0.0067
                 Mean entropy loss: -5.3227
                       Mean reward: 180.39
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.4487
Episode_Reward/track_ang_vel_z_exp: 0.1034
       Episode_Reward/lin_vel_z_l2: -0.0620
      Episode_Reward/ang_vel_xy_l2: -0.1553
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.6055
     Episode_Reward/action_rate_l2: -0.0188
Episode_Reward/flat_orientation_l2: -0.0212
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4341760
                    Iteration time: 5.78s
                      Time elapsed: 00:18:36
                               ETA: 00:16:34

################################################################################
                      [1m Learning iteration 265/500 [0m                      

                       Computation: 2844 steps/s (collection: 5.675s, learning 0.085s)
             Mean action noise std: 0.16
          Mean value_function loss: 0.0113
               Mean surrogate loss: -0.0072
                 Mean entropy loss: -5.4462
                       Mean reward: 180.70
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.5042
Episode_Reward/track_ang_vel_z_exp: 0.1052
       Episode_Reward/lin_vel_z_l2: -0.0617
      Episode_Reward/ang_vel_xy_l2: -0.1520
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.6174
     Episode_Reward/action_rate_l2: -0.0187
Episode_Reward/flat_orientation_l2: -0.0206
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4358144
                    Iteration time: 5.76s
                      Time elapsed: 00:18:42
                               ETA: 00:16:31

################################################################################
                      [1m Learning iteration 266/500 [0m                      

                       Computation: 2874 steps/s (collection: 5.610s, learning 0.090s)
             Mean action noise std: 0.16
          Mean value_function loss: 0.0106
               Mean surrogate loss: -0.0066
                 Mean entropy loss: -5.5077
                       Mean reward: 180.70
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.8038
Episode_Reward/track_ang_vel_z_exp: 0.1146
       Episode_Reward/lin_vel_z_l2: -0.0598
      Episode_Reward/ang_vel_xy_l2: -0.1345
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.6816
     Episode_Reward/action_rate_l2: -0.0180
Episode_Reward/flat_orientation_l2: -0.0177
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4374528
                    Iteration time: 5.70s
                      Time elapsed: 00:18:48
                               ETA: 00:16:28

################################################################################
                      [1m Learning iteration 267/500 [0m                      

                       Computation: 2834 steps/s (collection: 5.694s, learning 0.086s)
             Mean action noise std: 0.16
          Mean value_function loss: 0.0110
               Mean surrogate loss: -0.0077
                 Mean entropy loss: -5.6136
                       Mean reward: 180.70
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.8038
Episode_Reward/track_ang_vel_z_exp: 0.1146
       Episode_Reward/lin_vel_z_l2: -0.0598
      Episode_Reward/ang_vel_xy_l2: -0.1345
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.6816
     Episode_Reward/action_rate_l2: -0.0180
Episode_Reward/flat_orientation_l2: -0.0177
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4390912
                    Iteration time: 5.78s
                      Time elapsed: 00:18:53
                               ETA: 00:16:25

################################################################################
                      [1m Learning iteration 268/500 [0m                      

                       Computation: 2813 steps/s (collection: 5.741s, learning 0.084s)
             Mean action noise std: 0.16
          Mean value_function loss: 0.0105
               Mean surrogate loss: -0.0065
                 Mean entropy loss: -5.6659
                       Mean reward: 180.70
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.8038
Episode_Reward/track_ang_vel_z_exp: 0.1146
       Episode_Reward/lin_vel_z_l2: -0.0598
      Episode_Reward/ang_vel_xy_l2: -0.1345
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.6816
     Episode_Reward/action_rate_l2: -0.0180
Episode_Reward/flat_orientation_l2: -0.0177
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4407296
                    Iteration time: 5.82s
                      Time elapsed: 00:18:59
                               ETA: 00:16:22

################################################################################
                      [1m Learning iteration 269/500 [0m                      

                       Computation: 2774 steps/s (collection: 5.808s, learning 0.098s)
             Mean action noise std: 0.16
          Mean value_function loss: 0.0103
               Mean surrogate loss: -0.0061
                 Mean entropy loss: -5.7484
                       Mean reward: 180.70
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.8038
Episode_Reward/track_ang_vel_z_exp: 0.1146
       Episode_Reward/lin_vel_z_l2: -0.0598
      Episode_Reward/ang_vel_xy_l2: -0.1345
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.6816
     Episode_Reward/action_rate_l2: -0.0180
Episode_Reward/flat_orientation_l2: -0.0177
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 5.91s
                      Time elapsed: 00:19:05
                               ETA: 00:16:20

################################################################################
                      [1m Learning iteration 270/500 [0m                      

                       Computation: 2814 steps/s (collection: 5.737s, learning 0.085s)
             Mean action noise std: 0.16
          Mean value_function loss: 0.0101
               Mean surrogate loss: -0.0071
                 Mean entropy loss: -5.8307
                       Mean reward: 180.70
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.8038
Episode_Reward/track_ang_vel_z_exp: 0.1146
       Episode_Reward/lin_vel_z_l2: -0.0598
      Episode_Reward/ang_vel_xy_l2: -0.1345
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.6816
     Episode_Reward/action_rate_l2: -0.0180
Episode_Reward/flat_orientation_l2: -0.0177
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4440064
                    Iteration time: 5.82s
                      Time elapsed: 00:19:11
                               ETA: 00:16:17

################################################################################
                      [1m Learning iteration 271/500 [0m                      

                       Computation: 2822 steps/s (collection: 5.715s, learning 0.089s)
             Mean action noise std: 0.16
          Mean value_function loss: 0.0091
               Mean surrogate loss: -0.0079
                 Mean entropy loss: -5.9209
                       Mean reward: 181.27
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.8092
Episode_Reward/track_ang_vel_z_exp: 0.1148
       Episode_Reward/lin_vel_z_l2: -0.0593
      Episode_Reward/ang_vel_xy_l2: -0.1337
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.6780
     Episode_Reward/action_rate_l2: -0.0179
Episode_Reward/flat_orientation_l2: -0.0177
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4456448
                    Iteration time: 5.80s
                      Time elapsed: 00:19:17
                               ETA: 00:16:14

################################################################################
                      [1m Learning iteration 272/500 [0m                      

                       Computation: 2804 steps/s (collection: 5.755s, learning 0.087s)
             Mean action noise std: 0.16
          Mean value_function loss: 0.0112
               Mean surrogate loss: -0.0071
                 Mean entropy loss: -5.9915
                       Mean reward: 181.45
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.9075
Episode_Reward/track_ang_vel_z_exp: 0.1164
       Episode_Reward/lin_vel_z_l2: -0.0452
      Episode_Reward/ang_vel_xy_l2: -0.1160
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.5636
     Episode_Reward/action_rate_l2: -0.0159
Episode_Reward/flat_orientation_l2: -0.0182
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4472832
                    Iteration time: 5.84s
                      Time elapsed: 00:19:23
                               ETA: 00:16:11

################################################################################
                      [1m Learning iteration 273/500 [0m                      

                       Computation: 2823 steps/s (collection: 5.709s, learning 0.094s)
             Mean action noise std: 0.15
          Mean value_function loss: 0.0102
               Mean surrogate loss: -0.0049
                 Mean entropy loss: -6.0895
                       Mean reward: 181.72
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.6131
Episode_Reward/track_ang_vel_z_exp: 0.1171
       Episode_Reward/lin_vel_z_l2: -0.0446
      Episode_Reward/ang_vel_xy_l2: -0.1191
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.5593
     Episode_Reward/action_rate_l2: -0.0158
Episode_Reward/flat_orientation_l2: -0.0186
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4489216
                    Iteration time: 5.80s
                      Time elapsed: 00:19:28
                               ETA: 00:16:08

################################################################################
                      [1m Learning iteration 274/500 [0m                      

                       Computation: 2837 steps/s (collection: 5.682s, learning 0.093s)
             Mean action noise std: 0.15
          Mean value_function loss: 0.0107
               Mean surrogate loss: -0.0065
                 Mean entropy loss: -6.1446
                       Mean reward: 181.72
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.4805
Episode_Reward/track_ang_vel_z_exp: 0.1185
       Episode_Reward/lin_vel_z_l2: -0.0433
      Episode_Reward/ang_vel_xy_l2: -0.1186
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.5585
     Episode_Reward/action_rate_l2: -0.0156
Episode_Reward/flat_orientation_l2: -0.0188
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4505600
                    Iteration time: 5.78s
                      Time elapsed: 00:19:34
                               ETA: 00:16:05

################################################################################
                      [1m Learning iteration 275/500 [0m                      

                       Computation: 2819 steps/s (collection: 5.710s, learning 0.100s)
             Mean action noise std: 0.15
          Mean value_function loss: 0.0104
               Mean surrogate loss: -0.0062
                 Mean entropy loss: -6.2612
                       Mean reward: 181.72
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.4805
Episode_Reward/track_ang_vel_z_exp: 0.1185
       Episode_Reward/lin_vel_z_l2: -0.0433
      Episode_Reward/ang_vel_xy_l2: -0.1186
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.5585
     Episode_Reward/action_rate_l2: -0.0156
Episode_Reward/flat_orientation_l2: -0.0188
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 5.81s
                      Time elapsed: 00:19:40
                               ETA: 00:16:02

################################################################################
                      [1m Learning iteration 276/500 [0m                      

                       Computation: 2829 steps/s (collection: 5.702s, learning 0.089s)
             Mean action noise std: 0.15
          Mean value_function loss: 0.0106
               Mean surrogate loss: -0.0070
                 Mean entropy loss: -6.3343
                       Mean reward: 181.72
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.4805
Episode_Reward/track_ang_vel_z_exp: 0.1185
       Episode_Reward/lin_vel_z_l2: -0.0433
      Episode_Reward/ang_vel_xy_l2: -0.1186
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.5585
     Episode_Reward/action_rate_l2: -0.0156
Episode_Reward/flat_orientation_l2: -0.0188
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4538368
                    Iteration time: 5.79s
                      Time elapsed: 00:19:46
                               ETA: 00:15:59

################################################################################
                      [1m Learning iteration 277/500 [0m                      

                       Computation: 2834 steps/s (collection: 5.696s, learning 0.084s)
             Mean action noise std: 0.15
          Mean value_function loss: 0.0105
               Mean surrogate loss: -0.0060
                 Mean entropy loss: -6.3971
                       Mean reward: 182.37
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.5076
Episode_Reward/track_ang_vel_z_exp: 0.1170
       Episode_Reward/lin_vel_z_l2: -0.0456
      Episode_Reward/ang_vel_xy_l2: -0.1213
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.5534
     Episode_Reward/action_rate_l2: -0.0156
Episode_Reward/flat_orientation_l2: -0.0191
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4554752
                    Iteration time: 5.78s
                      Time elapsed: 00:19:52
                               ETA: 00:15:56

################################################################################
                      [1m Learning iteration 278/500 [0m                      

                       Computation: 2809 steps/s (collection: 5.738s, learning 0.093s)
             Mean action noise std: 0.15
          Mean value_function loss: 0.0097
               Mean surrogate loss: -0.0068
                 Mean entropy loss: -6.4719
                       Mean reward: 184.90
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.7295
Episode_Reward/track_ang_vel_z_exp: 0.1110
       Episode_Reward/lin_vel_z_l2: -0.0542
      Episode_Reward/ang_vel_xy_l2: -0.1272
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.5333
     Episode_Reward/action_rate_l2: -0.0154
Episode_Reward/flat_orientation_l2: -0.0186
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4571136
                    Iteration time: 5.83s
                      Time elapsed: 00:19:57
                               ETA: 00:15:53

################################################################################
                      [1m Learning iteration 279/500 [0m                      

                       Computation: 2804 steps/s (collection: 5.758s, learning 0.084s)
             Mean action noise std: 0.15
          Mean value_function loss: 0.0103
               Mean surrogate loss: -0.0067
                 Mean entropy loss: -6.5254
                       Mean reward: 189.26
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.8745
Episode_Reward/track_ang_vel_z_exp: 0.1155
       Episode_Reward/lin_vel_z_l2: -0.0475
      Episode_Reward/ang_vel_xy_l2: -0.1124
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.5348
     Episode_Reward/action_rate_l2: -0.0149
Episode_Reward/flat_orientation_l2: -0.0186
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0312
--------------------------------------------------------------------------------
                   Total timesteps: 4587520
                    Iteration time: 5.84s
                      Time elapsed: 00:20:03
                               ETA: 00:15:50

################################################################################
                      [1m Learning iteration 280/500 [0m                      

                       Computation: 2802 steps/s (collection: 5.762s, learning 0.084s)
             Mean action noise std: 0.15
          Mean value_function loss: 0.0105
               Mean surrogate loss: -0.0077
                 Mean entropy loss: -6.5961
                       Mean reward: 194.55
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.7199
Episode_Reward/track_ang_vel_z_exp: 0.1140
       Episode_Reward/lin_vel_z_l2: -0.0505
      Episode_Reward/ang_vel_xy_l2: -0.1183
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.5490
     Episode_Reward/action_rate_l2: -0.0149
Episode_Reward/flat_orientation_l2: -0.0194
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 4603904
                    Iteration time: 5.85s
                      Time elapsed: 00:20:09
                               ETA: 00:15:46

################################################################################
                      [1m Learning iteration 281/500 [0m                      

                       Computation: 2788 steps/s (collection: 5.790s, learning 0.085s)
             Mean action noise std: 0.15
          Mean value_function loss: 0.0104
               Mean surrogate loss: -0.0063
                 Mean entropy loss: -6.6647
                       Mean reward: 209.31
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.8464
Episode_Reward/track_ang_vel_z_exp: 0.1172
       Episode_Reward/lin_vel_z_l2: -0.0469
      Episode_Reward/ang_vel_xy_l2: -0.1133
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.5471
     Episode_Reward/action_rate_l2: -0.0146
Episode_Reward/flat_orientation_l2: -0.0186
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.9688
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 5.88s
                      Time elapsed: 00:20:15
                               ETA: 00:15:43

################################################################################
                      [1m Learning iteration 282/500 [0m                      

                       Computation: 2797 steps/s (collection: 5.772s, learning 0.083s)
             Mean action noise std: 0.15
          Mean value_function loss: 0.0116
               Mean surrogate loss: -0.0048
                 Mean entropy loss: -6.7119
                       Mean reward: 211.44
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.7775
Episode_Reward/track_ang_vel_z_exp: 0.1166
       Episode_Reward/lin_vel_z_l2: -0.0477
      Episode_Reward/ang_vel_xy_l2: -0.1142
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.5430
     Episode_Reward/action_rate_l2: -0.0144
Episode_Reward/flat_orientation_l2: -0.0188
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 2.1875
--------------------------------------------------------------------------------
                   Total timesteps: 4636672
                    Iteration time: 5.86s
                      Time elapsed: 00:20:21
                               ETA: 00:15:40

################################################################################
                      [1m Learning iteration 283/500 [0m                      

                       Computation: 2784 steps/s (collection: 5.791s, learning 0.094s)
             Mean action noise std: 0.15
          Mean value_function loss: 0.0097
               Mean surrogate loss: -0.0064
                 Mean entropy loss: -6.8079
                       Mean reward: 211.86
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.8799
Episode_Reward/track_ang_vel_z_exp: 0.1183
       Episode_Reward/lin_vel_z_l2: -0.0465
      Episode_Reward/ang_vel_xy_l2: -0.1113
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.5413
     Episode_Reward/action_rate_l2: -0.0142
Episode_Reward/flat_orientation_l2: -0.0188
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.9375
--------------------------------------------------------------------------------
                   Total timesteps: 4653056
                    Iteration time: 5.88s
                      Time elapsed: 00:20:27
                               ETA: 00:15:37

################################################################################
                      [1m Learning iteration 284/500 [0m                      

                       Computation: 2756 steps/s (collection: 5.860s, learning 0.084s)
             Mean action noise std: 0.15
          Mean value_function loss: 0.0120
               Mean surrogate loss: -0.0074
                 Mean entropy loss: -6.8721
                       Mean reward: 211.58
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.8136
Episode_Reward/track_ang_vel_z_exp: 0.1183
       Episode_Reward/lin_vel_z_l2: -0.0453
      Episode_Reward/ang_vel_xy_l2: -0.1101
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.5457
     Episode_Reward/action_rate_l2: -0.0140
Episode_Reward/flat_orientation_l2: -0.0185
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 2.2812
--------------------------------------------------------------------------------
                   Total timesteps: 4669440
                    Iteration time: 5.94s
                      Time elapsed: 00:20:33
                               ETA: 00:15:34

################################################################################
                      [1m Learning iteration 285/500 [0m                      

                       Computation: 2769 steps/s (collection: 5.833s, learning 0.084s)
             Mean action noise std: 0.14
          Mean value_function loss: 0.0105
               Mean surrogate loss: -0.0068
                 Mean entropy loss: -6.9635
                       Mean reward: 211.68
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.8118
Episode_Reward/track_ang_vel_z_exp: 0.1178
       Episode_Reward/lin_vel_z_l2: -0.0467
      Episode_Reward/ang_vel_xy_l2: -0.1112
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.5377
     Episode_Reward/action_rate_l2: -0.0140
Episode_Reward/flat_orientation_l2: -0.0187
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.7812
--------------------------------------------------------------------------------
                   Total timesteps: 4685824
                    Iteration time: 5.92s
                      Time elapsed: 00:20:38
                               ETA: 00:15:31

################################################################################
                      [1m Learning iteration 286/500 [0m                      

                       Computation: 2760 steps/s (collection: 5.836s, learning 0.098s)
             Mean action noise std: 0.14
          Mean value_function loss: 0.0091
               Mean surrogate loss: -0.0071
                 Mean entropy loss: -7.0544
                       Mean reward: 213.05
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.8214
Episode_Reward/track_ang_vel_z_exp: 0.1187
       Episode_Reward/lin_vel_z_l2: -0.0485
      Episode_Reward/ang_vel_xy_l2: -0.1141
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.5191
     Episode_Reward/action_rate_l2: -0.0140
Episode_Reward/flat_orientation_l2: -0.0189
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.5312
--------------------------------------------------------------------------------
                   Total timesteps: 4702208
                    Iteration time: 5.93s
                      Time elapsed: 00:20:44
                               ETA: 00:15:28

################################################################################
                      [1m Learning iteration 287/500 [0m                      

                       Computation: 2769 steps/s (collection: 5.784s, learning 0.131s)
             Mean action noise std: 0.14
          Mean value_function loss: 0.0096
               Mean surrogate loss: -0.0064
                 Mean entropy loss: -7.1346
                       Mean reward: 214.56
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.9063
Episode_Reward/track_ang_vel_z_exp: 0.1212
       Episode_Reward/lin_vel_z_l2: -0.0455
      Episode_Reward/ang_vel_xy_l2: -0.1071
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.5305
     Episode_Reward/action_rate_l2: -0.0135
Episode_Reward/flat_orientation_l2: -0.0183
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.4688
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 5.92s
                      Time elapsed: 00:20:50
                               ETA: 00:15:25

################################################################################
                      [1m Learning iteration 288/500 [0m                      

                       Computation: 2785 steps/s (collection: 5.799s, learning 0.083s)
             Mean action noise std: 0.14
          Mean value_function loss: 0.0096
               Mean surrogate loss: -0.0060
                 Mean entropy loss: -7.1925
                       Mean reward: 214.91
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.8049
Episode_Reward/track_ang_vel_z_exp: 0.1204
       Episode_Reward/lin_vel_z_l2: -0.0451
      Episode_Reward/ang_vel_xy_l2: -0.1088
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.5258
     Episode_Reward/action_rate_l2: -0.0134
Episode_Reward/flat_orientation_l2: -0.0188
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.3438
--------------------------------------------------------------------------------
                   Total timesteps: 4734976
                    Iteration time: 5.88s
                      Time elapsed: 00:20:56
                               ETA: 00:15:21

################################################################################
                      [1m Learning iteration 289/500 [0m                      

                       Computation: 2806 steps/s (collection: 5.747s, learning 0.091s)
             Mean action noise std: 0.14
          Mean value_function loss: 0.0101
               Mean surrogate loss: -0.0066
                 Mean entropy loss: -7.2488
                       Mean reward: 215.66
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.8881
Episode_Reward/track_ang_vel_z_exp: 0.1204
       Episode_Reward/lin_vel_z_l2: -0.0456
      Episode_Reward/ang_vel_xy_l2: -0.1055
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.5429
     Episode_Reward/action_rate_l2: -0.0134
Episode_Reward/flat_orientation_l2: -0.0188
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 4751360
                    Iteration time: 5.84s
                      Time elapsed: 00:21:02
                               ETA: 00:15:18

################################################################################
                      [1m Learning iteration 290/500 [0m                      

                       Computation: 2815 steps/s (collection: 5.736s, learning 0.083s)
             Mean action noise std: 0.14
          Mean value_function loss: 0.0105
               Mean surrogate loss: -0.0053
                 Mean entropy loss: -7.3362
                       Mean reward: 214.80
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.8779
Episode_Reward/track_ang_vel_z_exp: 0.1201
       Episode_Reward/lin_vel_z_l2: -0.0464
      Episode_Reward/ang_vel_xy_l2: -0.1084
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.5248
     Episode_Reward/action_rate_l2: -0.0132
Episode_Reward/flat_orientation_l2: -0.0185
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 4767744
                    Iteration time: 5.82s
                      Time elapsed: 00:21:08
                               ETA: 00:15:15

################################################################################
                      [1m Learning iteration 291/500 [0m                      

                       Computation: 2814 steps/s (collection: 5.729s, learning 0.093s)
             Mean action noise std: 0.14
          Mean value_function loss: 0.0093
               Mean surrogate loss: -0.0056
                 Mean entropy loss: -7.3740
                       Mean reward: 215.29
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.7909
Episode_Reward/track_ang_vel_z_exp: 0.1193
       Episode_Reward/lin_vel_z_l2: -0.0490
      Episode_Reward/ang_vel_xy_l2: -0.1091
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.5178
     Episode_Reward/action_rate_l2: -0.0131
Episode_Reward/flat_orientation_l2: -0.0190
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.2812
--------------------------------------------------------------------------------
                   Total timesteps: 4784128
                    Iteration time: 5.82s
                      Time elapsed: 00:21:14
                               ETA: 00:15:12

################################################################################
                      [1m Learning iteration 292/500 [0m                      

                       Computation: 2791 steps/s (collection: 5.785s, learning 0.084s)
             Mean action noise std: 0.14
          Mean value_function loss: 0.0087
               Mean surrogate loss: -0.0047
                 Mean entropy loss: -7.4701
                       Mean reward: 215.87
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.8815
Episode_Reward/track_ang_vel_z_exp: 0.1233
       Episode_Reward/lin_vel_z_l2: -0.0416
      Episode_Reward/ang_vel_xy_l2: -0.1014
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.5160
     Episode_Reward/action_rate_l2: -0.0126
Episode_Reward/flat_orientation_l2: -0.0186
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0625
--------------------------------------------------------------------------------
                   Total timesteps: 4800512
                    Iteration time: 5.87s
                      Time elapsed: 00:21:20
                               ETA: 00:15:08

################################################################################
                      [1m Learning iteration 293/500 [0m                      

                       Computation: 2824 steps/s (collection: 5.716s, learning 0.084s)
             Mean action noise std: 0.14
          Mean value_function loss: 0.0089
               Mean surrogate loss: -0.0052
                 Mean entropy loss: -7.5112
                       Mean reward: 215.45
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.6269
Episode_Reward/track_ang_vel_z_exp: 0.1282
       Episode_Reward/lin_vel_z_l2: -0.0372
      Episode_Reward/ang_vel_xy_l2: -0.0945
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.5404
     Episode_Reward/action_rate_l2: -0.0124
Episode_Reward/flat_orientation_l2: -0.0182
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 5.80s
                      Time elapsed: 00:21:25
                               ETA: 00:15:05

################################################################################
                      [1m Learning iteration 294/500 [0m                      

                       Computation: 2846 steps/s (collection: 5.673s, learning 0.084s)
             Mean action noise std: 0.14
          Mean value_function loss: 0.0088
               Mean surrogate loss: -0.0073
                 Mean entropy loss: -7.6193
                       Mean reward: 215.17
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.7257
Episode_Reward/track_ang_vel_z_exp: 0.1279
       Episode_Reward/lin_vel_z_l2: -0.0388
      Episode_Reward/ang_vel_xy_l2: -0.0975
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.5396
     Episode_Reward/action_rate_l2: -0.0123
Episode_Reward/flat_orientation_l2: -0.0180
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0312
--------------------------------------------------------------------------------
                   Total timesteps: 4833280
                    Iteration time: 5.76s
                      Time elapsed: 00:21:31
                               ETA: 00:15:01

################################################################################
                      [1m Learning iteration 295/500 [0m                      

                       Computation: 2815 steps/s (collection: 5.728s, learning 0.091s)
             Mean action noise std: 0.14
          Mean value_function loss: 0.0087
               Mean surrogate loss: -0.0048
                 Mean entropy loss: -7.6872
                       Mean reward: 214.80
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.9955
Episode_Reward/track_ang_vel_z_exp: 0.1211
       Episode_Reward/lin_vel_z_l2: -0.0432
      Episode_Reward/ang_vel_xy_l2: -0.1033
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.5442
     Episode_Reward/action_rate_l2: -0.0123
Episode_Reward/flat_orientation_l2: -0.0177
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0938
--------------------------------------------------------------------------------
                   Total timesteps: 4849664
                    Iteration time: 5.82s
                      Time elapsed: 00:21:37
                               ETA: 00:14:58

################################################################################
                      [1m Learning iteration 296/500 [0m                      

                       Computation: 2797 steps/s (collection: 5.747s, learning 0.109s)
             Mean action noise std: 0.14
          Mean value_function loss: 0.0089
               Mean surrogate loss: -0.0070
                 Mean entropy loss: -7.7642
                       Mean reward: 214.84
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.8105
Episode_Reward/track_ang_vel_z_exp: 0.1269
       Episode_Reward/lin_vel_z_l2: -0.0394
      Episode_Reward/ang_vel_xy_l2: -0.0952
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.5356
     Episode_Reward/action_rate_l2: -0.0119
Episode_Reward/flat_orientation_l2: -0.0175
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4866048
                    Iteration time: 5.86s
                      Time elapsed: 00:21:43
                               ETA: 00:14:55

################################################################################
                      [1m Learning iteration 297/500 [0m                      

                       Computation: 2810 steps/s (collection: 5.721s, learning 0.108s)
             Mean action noise std: 0.14
          Mean value_function loss: 0.0092
               Mean surrogate loss: -0.0047
                 Mean entropy loss: -7.8084
                       Mean reward: 215.07
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.0668
Episode_Reward/track_ang_vel_z_exp: 0.1219
       Episode_Reward/lin_vel_z_l2: -0.0446
      Episode_Reward/ang_vel_xy_l2: -0.1025
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.5128
     Episode_Reward/action_rate_l2: -0.0122
Episode_Reward/flat_orientation_l2: -0.0188
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4882432
                    Iteration time: 5.83s
                      Time elapsed: 00:21:49
                               ETA: 00:14:51

################################################################################
                      [1m Learning iteration 298/500 [0m                      

                       Computation: 2839 steps/s (collection: 5.684s, learning 0.085s)
             Mean action noise std: 0.13
          Mean value_function loss: 0.0096
               Mean surrogate loss: -0.0074
                 Mean entropy loss: -7.9232
                       Mean reward: 215.26
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.0812
Episode_Reward/track_ang_vel_z_exp: 0.1237
       Episode_Reward/lin_vel_z_l2: -0.0422
      Episode_Reward/ang_vel_xy_l2: -0.0992
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.5060
     Episode_Reward/action_rate_l2: -0.0119
Episode_Reward/flat_orientation_l2: -0.0184
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4898816
                    Iteration time: 5.77s
                      Time elapsed: 00:21:54
                               ETA: 00:14:48

################################################################################
                      [1m Learning iteration 299/500 [0m                      

                       Computation: 2831 steps/s (collection: 5.703s, learning 0.083s)
             Mean action noise std: 0.13
          Mean value_function loss: 0.0074
               Mean surrogate loss: -0.0061
                 Mean entropy loss: -8.0186
                       Mean reward: 215.13
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.8531
Episode_Reward/track_ang_vel_z_exp: 0.1101
       Episode_Reward/lin_vel_z_l2: -0.0563
      Episode_Reward/ang_vel_xy_l2: -0.1226
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.4844
     Episode_Reward/action_rate_l2: -0.0125
Episode_Reward/flat_orientation_l2: -0.0207
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 5.79s
                      Time elapsed: 00:22:00
                               ETA: 00:14:44

################################################################################
                      [1m Learning iteration 300/500 [0m                      

                       Computation: 2822 steps/s (collection: 5.721s, learning 0.083s)
             Mean action noise std: 0.13
          Mean value_function loss: 0.0088
               Mean surrogate loss: -0.0058
                 Mean entropy loss: -8.0902
                       Mean reward: 215.14
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.7727
Episode_Reward/track_ang_vel_z_exp: 0.1170
       Episode_Reward/lin_vel_z_l2: -0.0556
      Episode_Reward/ang_vel_xy_l2: -0.1186
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.4386
     Episode_Reward/action_rate_l2: -0.0117
Episode_Reward/flat_orientation_l2: -0.0200
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4931584
                    Iteration time: 5.80s
                      Time elapsed: 00:22:06
                               ETA: 00:14:41

################################################################################
                      [1m Learning iteration 301/500 [0m                      

                       Computation: 2843 steps/s (collection: 5.676s, learning 0.086s)
             Mean action noise std: 0.13
          Mean value_function loss: 0.0090
               Mean surrogate loss: -0.0057
                 Mean entropy loss: -8.1449
                       Mean reward: 215.22
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.8430
Episode_Reward/track_ang_vel_z_exp: 0.1188
       Episode_Reward/lin_vel_z_l2: -0.0523
      Episode_Reward/ang_vel_xy_l2: -0.1136
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.4540
     Episode_Reward/action_rate_l2: -0.0117
Episode_Reward/flat_orientation_l2: -0.0190
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4947968
                    Iteration time: 5.76s
                      Time elapsed: 00:22:12
                               ETA: 00:14:37

################################################################################
                      [1m Learning iteration 302/500 [0m                      

                       Computation: 2860 steps/s (collection: 5.643s, learning 0.084s)
             Mean action noise std: 0.13
          Mean value_function loss: 0.0104
               Mean surrogate loss: -0.0031
                 Mean entropy loss: -8.2012
                       Mean reward: 215.31
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.9625
Episode_Reward/track_ang_vel_z_exp: 0.1206
       Episode_Reward/lin_vel_z_l2: -0.0485
      Episode_Reward/ang_vel_xy_l2: -0.1064
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.4788
     Episode_Reward/action_rate_l2: -0.0116
Episode_Reward/flat_orientation_l2: -0.0178
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4964352
                    Iteration time: 5.73s
                      Time elapsed: 00:22:17
                               ETA: 00:14:34

################################################################################
                      [1m Learning iteration 303/500 [0m                      

                       Computation: 2845 steps/s (collection: 5.663s, learning 0.095s)
             Mean action noise std: 0.13
          Mean value_function loss: 0.0078
               Mean surrogate loss: -0.0054
                 Mean entropy loss: -8.2804
                       Mean reward: 215.09
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.5528
Episode_Reward/track_ang_vel_z_exp: 0.1230
       Episode_Reward/lin_vel_z_l2: -0.0400
      Episode_Reward/ang_vel_xy_l2: -0.0956
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.5752
     Episode_Reward/action_rate_l2: -0.0110
Episode_Reward/flat_orientation_l2: -0.0186
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4980736
                    Iteration time: 5.76s
                      Time elapsed: 00:22:23
                               ETA: 00:14:30

################################################################################
                      [1m Learning iteration 304/500 [0m                      

                       Computation: 2823 steps/s (collection: 5.717s, learning 0.085s)
             Mean action noise std: 0.13
          Mean value_function loss: 0.0118
               Mean surrogate loss: -0.0048
                 Mean entropy loss: -8.3707
                       Mean reward: 215.19
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.2280
Episode_Reward/track_ang_vel_z_exp: 0.1346
       Episode_Reward/lin_vel_z_l2: -0.0356
      Episode_Reward/ang_vel_xy_l2: -0.0861
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.5048
     Episode_Reward/action_rate_l2: -0.0108
Episode_Reward/flat_orientation_l2: -0.0177
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4997120
                    Iteration time: 5.80s
                      Time elapsed: 00:22:29
                               ETA: 00:14:27

################################################################################
                      [1m Learning iteration 305/500 [0m                      

                       Computation: 2817 steps/s (collection: 5.710s, learning 0.105s)
             Mean action noise std: 0.13
          Mean value_function loss: 0.0080
               Mean surrogate loss: -0.0062
                 Mean entropy loss: -8.4326
                       Mean reward: 215.05
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.9972
Episode_Reward/track_ang_vel_z_exp: 0.1334
       Episode_Reward/lin_vel_z_l2: -0.0359
      Episode_Reward/ang_vel_xy_l2: -0.0885
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.5147
     Episode_Reward/action_rate_l2: -0.0109
Episode_Reward/flat_orientation_l2: -0.0177
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 5.81s
                      Time elapsed: 00:22:35
                               ETA: 00:14:23

################################################################################
                      [1m Learning iteration 306/500 [0m                      

                       Computation: 2835 steps/s (collection: 5.692s, learning 0.086s)
             Mean action noise std: 0.13
          Mean value_function loss: 0.0079
               Mean surrogate loss: -0.0072
                 Mean entropy loss: -8.4914
                       Mean reward: 215.05
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.4413
Episode_Reward/track_ang_vel_z_exp: 0.1298
       Episode_Reward/lin_vel_z_l2: -0.0358
      Episode_Reward/ang_vel_xy_l2: -0.0951
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.5516
     Episode_Reward/action_rate_l2: -0.0110
Episode_Reward/flat_orientation_l2: -0.0178
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5029888
                    Iteration time: 5.78s
                      Time elapsed: 00:22:41
                               ETA: 00:14:20

################################################################################
                      [1m Learning iteration 307/500 [0m                      

                       Computation: 2851 steps/s (collection: 5.660s, learning 0.086s)
             Mean action noise std: 0.13
          Mean value_function loss: 0.0083
               Mean surrogate loss: -0.0074
                 Mean entropy loss: -8.5457
                       Mean reward: 215.05
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.4413
Episode_Reward/track_ang_vel_z_exp: 0.1298
       Episode_Reward/lin_vel_z_l2: -0.0358
      Episode_Reward/ang_vel_xy_l2: -0.0951
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.5516
     Episode_Reward/action_rate_l2: -0.0110
Episode_Reward/flat_orientation_l2: -0.0178
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5046272
                    Iteration time: 5.75s
                      Time elapsed: 00:22:46
                               ETA: 00:14:16

################################################################################
                      [1m Learning iteration 308/500 [0m                      

                       Computation: 2817 steps/s (collection: 5.732s, learning 0.082s)
             Mean action noise std: 0.13
          Mean value_function loss: 0.0074
               Mean surrogate loss: -0.0046
                 Mean entropy loss: -8.6028
                       Mean reward: 215.05
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.4413
Episode_Reward/track_ang_vel_z_exp: 0.1298
       Episode_Reward/lin_vel_z_l2: -0.0358
      Episode_Reward/ang_vel_xy_l2: -0.0951
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.5516
     Episode_Reward/action_rate_l2: -0.0110
Episode_Reward/flat_orientation_l2: -0.0178
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5062656
                    Iteration time: 5.81s
                      Time elapsed: 00:22:52
                               ETA: 00:14:12

################################################################################
                      [1m Learning iteration 309/500 [0m                      

                       Computation: 2833 steps/s (collection: 5.696s, learning 0.085s)
             Mean action noise std: 0.13
          Mean value_function loss: 0.0074
               Mean surrogate loss: -0.0065
                 Mean entropy loss: -8.6610
                       Mean reward: 215.05
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.4413
Episode_Reward/track_ang_vel_z_exp: 0.1298
       Episode_Reward/lin_vel_z_l2: -0.0358
      Episode_Reward/ang_vel_xy_l2: -0.0951
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.5516
     Episode_Reward/action_rate_l2: -0.0110
Episode_Reward/flat_orientation_l2: -0.0178
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5079040
                    Iteration time: 5.78s
                      Time elapsed: 00:22:58
                               ETA: 00:14:09

################################################################################
                      [1m Learning iteration 310/500 [0m                      

                       Computation: 2884 steps/s (collection: 5.593s, learning 0.088s)
             Mean action noise std: 0.13
          Mean value_function loss: 0.0093
               Mean surrogate loss: -0.0067
                 Mean entropy loss: -8.6680
                       Mean reward: 214.90
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.5389
Episode_Reward/track_ang_vel_z_exp: 0.1264
       Episode_Reward/lin_vel_z_l2: -0.0420
      Episode_Reward/ang_vel_xy_l2: -0.0980
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.5272
     Episode_Reward/action_rate_l2: -0.0108
Episode_Reward/flat_orientation_l2: -0.0180
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5095424
                    Iteration time: 5.68s
                      Time elapsed: 00:23:04
                               ETA: 00:14:05

################################################################################
                      [1m Learning iteration 311/500 [0m                      

                       Computation: 2844 steps/s (collection: 5.674s, learning 0.086s)
             Mean action noise std: 0.13
          Mean value_function loss: 0.0081
               Mean surrogate loss: -0.0065
                 Mean entropy loss: -8.7612
                       Mean reward: 214.90
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.6645
Episode_Reward/track_ang_vel_z_exp: 0.1219
       Episode_Reward/lin_vel_z_l2: -0.0499
      Episode_Reward/ang_vel_xy_l2: -0.1017
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.4957
     Episode_Reward/action_rate_l2: -0.0106
Episode_Reward/flat_orientation_l2: -0.0183
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 5.76s
                      Time elapsed: 00:23:09
                               ETA: 00:14:01

################################################################################
                      [1m Learning iteration 312/500 [0m                      

                       Computation: 2821 steps/s (collection: 5.720s, learning 0.086s)
             Mean action noise std: 0.13
          Mean value_function loss: 0.0079
               Mean surrogate loss: -0.0059
                 Mean entropy loss: -8.8247
                       Mean reward: 214.90
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.6645
Episode_Reward/track_ang_vel_z_exp: 0.1219
       Episode_Reward/lin_vel_z_l2: -0.0499
      Episode_Reward/ang_vel_xy_l2: -0.1017
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.4957
     Episode_Reward/action_rate_l2: -0.0106
Episode_Reward/flat_orientation_l2: -0.0183
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5128192
                    Iteration time: 5.81s
                      Time elapsed: 00:23:15
                               ETA: 00:13:58

################################################################################
                      [1m Learning iteration 313/500 [0m                      

                       Computation: 2824 steps/s (collection: 5.716s, learning 0.086s)
             Mean action noise std: 0.13
          Mean value_function loss: 0.0078
               Mean surrogate loss: -0.0064
                 Mean entropy loss: -8.9063
                       Mean reward: 214.90
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.6645
Episode_Reward/track_ang_vel_z_exp: 0.1219
       Episode_Reward/lin_vel_z_l2: -0.0499
      Episode_Reward/ang_vel_xy_l2: -0.1017
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.4957
     Episode_Reward/action_rate_l2: -0.0106
Episode_Reward/flat_orientation_l2: -0.0183
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5144576
                    Iteration time: 5.80s
                      Time elapsed: 00:23:21
                               ETA: 00:13:54

################################################################################
                      [1m Learning iteration 314/500 [0m                      

                       Computation: 2849 steps/s (collection: 5.665s, learning 0.085s)
             Mean action noise std: 0.12
          Mean value_function loss: 0.0082
               Mean surrogate loss: -0.0076
                 Mean entropy loss: -8.9952
                       Mean reward: 214.90
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.6645
Episode_Reward/track_ang_vel_z_exp: 0.1219
       Episode_Reward/lin_vel_z_l2: -0.0499
      Episode_Reward/ang_vel_xy_l2: -0.1017
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.4957
     Episode_Reward/action_rate_l2: -0.0106
Episode_Reward/flat_orientation_l2: -0.0183
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5160960
                    Iteration time: 5.75s
                      Time elapsed: 00:23:27
                               ETA: 00:13:50

################################################################################
                      [1m Learning iteration 315/500 [0m                      

                       Computation: 2860 steps/s (collection: 5.644s, learning 0.084s)
             Mean action noise std: 0.12
          Mean value_function loss: 0.0087
               Mean surrogate loss: -0.0070
                 Mean entropy loss: -9.0801
                       Mean reward: 214.90
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.6645
Episode_Reward/track_ang_vel_z_exp: 0.1219
       Episode_Reward/lin_vel_z_l2: -0.0499
      Episode_Reward/ang_vel_xy_l2: -0.1017
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.4957
     Episode_Reward/action_rate_l2: -0.0106
Episode_Reward/flat_orientation_l2: -0.0183
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5177344
                    Iteration time: 5.73s
                      Time elapsed: 00:23:33
                               ETA: 00:13:47

################################################################################
                      [1m Learning iteration 316/500 [0m                      

                       Computation: 2819 steps/s (collection: 5.704s, learning 0.106s)
             Mean action noise std: 0.12
          Mean value_function loss: 0.0084
               Mean surrogate loss: -0.0045
                 Mean entropy loss: -9.1393
                       Mean reward: 215.16
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.8238
Episode_Reward/track_ang_vel_z_exp: 0.1227
       Episode_Reward/lin_vel_z_l2: -0.0494
      Episode_Reward/ang_vel_xy_l2: -0.1006
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.4880
     Episode_Reward/action_rate_l2: -0.0104
Episode_Reward/flat_orientation_l2: -0.0184
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5193728
                    Iteration time: 5.81s
                      Time elapsed: 00:23:38
                               ETA: 00:13:43

################################################################################
                      [1m Learning iteration 317/500 [0m                      

                       Computation: 2822 steps/s (collection: 5.708s, learning 0.096s)
             Mean action noise std: 0.12
          Mean value_function loss: 0.0081
               Mean surrogate loss: -0.0049
                 Mean entropy loss: -9.2096
                       Mean reward: 215.09
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.9021
Episode_Reward/track_ang_vel_z_exp: 0.1284
       Episode_Reward/lin_vel_z_l2: -0.0401
      Episode_Reward/ang_vel_xy_l2: -0.0891
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.5159
     Episode_Reward/action_rate_l2: -0.0095
Episode_Reward/flat_orientation_l2: -0.0190
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 5.80s
                      Time elapsed: 00:23:44
                               ETA: 00:13:39

################################################################################
                      [1m Learning iteration 318/500 [0m                      

                       Computation: 2854 steps/s (collection: 5.655s, learning 0.086s)
             Mean action noise std: 0.12
          Mean value_function loss: 0.0076
               Mean surrogate loss: -0.0054
                 Mean entropy loss: -9.2235
                       Mean reward: 215.24
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.9816
Episode_Reward/track_ang_vel_z_exp: 0.1338
       Episode_Reward/lin_vel_z_l2: -0.0388
      Episode_Reward/ang_vel_xy_l2: -0.0878
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.4834
     Episode_Reward/action_rate_l2: -0.0093
Episode_Reward/flat_orientation_l2: -0.0183
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5226496
                    Iteration time: 5.74s
                      Time elapsed: 00:23:50
                               ETA: 00:13:36

################################################################################
                      [1m Learning iteration 319/500 [0m                      

                       Computation: 2857 steps/s (collection: 5.651s, learning 0.083s)
             Mean action noise std: 0.12
          Mean value_function loss: 0.0081
               Mean surrogate loss: -0.0062
                 Mean entropy loss: -9.2948
                       Mean reward: 215.33
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.1653
Episode_Reward/track_ang_vel_z_exp: 0.1330
       Episode_Reward/lin_vel_z_l2: -0.0448
      Episode_Reward/ang_vel_xy_l2: -0.0916
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.4343
     Episode_Reward/action_rate_l2: -0.0095
Episode_Reward/flat_orientation_l2: -0.0179
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5242880
                    Iteration time: 5.73s
                      Time elapsed: 00:23:56
                               ETA: 00:13:32

################################################################################
                      [1m Learning iteration 320/500 [0m                      

                       Computation: 2813 steps/s (collection: 5.737s, learning 0.086s)
             Mean action noise std: 0.12
          Mean value_function loss: 0.0106
               Mean surrogate loss: -0.0047
                 Mean entropy loss: -9.3215
                       Mean reward: 215.33
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.0969
Episode_Reward/track_ang_vel_z_exp: 0.1334
       Episode_Reward/lin_vel_z_l2: -0.0433
      Episode_Reward/ang_vel_xy_l2: -0.0860
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.4634
     Episode_Reward/action_rate_l2: -0.0092
Episode_Reward/flat_orientation_l2: -0.0183
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5259264
                    Iteration time: 5.82s
                      Time elapsed: 00:24:01
                               ETA: 00:13:28

################################################################################
                      [1m Learning iteration 321/500 [0m                      

                       Computation: 2823 steps/s (collection: 5.719s, learning 0.084s)
             Mean action noise std: 0.12
          Mean value_function loss: 0.0071
               Mean surrogate loss: -0.0062
                 Mean entropy loss: -9.3768
                       Mean reward: 215.59
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.1276
Episode_Reward/track_ang_vel_z_exp: 0.1299
       Episode_Reward/lin_vel_z_l2: -0.0443
      Episode_Reward/ang_vel_xy_l2: -0.0883
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.4503
     Episode_Reward/action_rate_l2: -0.0091
Episode_Reward/flat_orientation_l2: -0.0183
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5275648
                    Iteration time: 5.80s
                      Time elapsed: 00:24:07
                               ETA: 00:13:24

################################################################################
                      [1m Learning iteration 322/500 [0m                      

                       Computation: 2837 steps/s (collection: 5.687s, learning 0.087s)
             Mean action noise std: 0.12
          Mean value_function loss: 0.0078
               Mean surrogate loss: -0.0066
                 Mean entropy loss: -9.4497
                       Mean reward: 215.59
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.2196
Episode_Reward/track_ang_vel_z_exp: 0.1197
       Episode_Reward/lin_vel_z_l2: -0.0474
      Episode_Reward/ang_vel_xy_l2: -0.0954
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4110
     Episode_Reward/action_rate_l2: -0.0089
Episode_Reward/flat_orientation_l2: -0.0183
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5292032
                    Iteration time: 5.77s
                      Time elapsed: 00:24:13
                               ETA: 00:13:20

################################################################################
                      [1m Learning iteration 323/500 [0m                      

                       Computation: 2852 steps/s (collection: 5.656s, learning 0.088s)
             Mean action noise std: 0.12
          Mean value_function loss: 0.0072
               Mean surrogate loss: -0.0051
                 Mean entropy loss: -9.4652
                       Mean reward: 215.59
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.2196
Episode_Reward/track_ang_vel_z_exp: 0.1197
       Episode_Reward/lin_vel_z_l2: -0.0474
      Episode_Reward/ang_vel_xy_l2: -0.0954
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4110
     Episode_Reward/action_rate_l2: -0.0089
Episode_Reward/flat_orientation_l2: -0.0183
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 5.74s
                      Time elapsed: 00:24:19
                               ETA: 00:13:17

################################################################################
                      [1m Learning iteration 324/500 [0m                      

                       Computation: 2795 steps/s (collection: 5.777s, learning 0.083s)
             Mean action noise std: 0.12
          Mean value_function loss: 0.0081
               Mean surrogate loss: -0.0041
                 Mean entropy loss: -9.5392
                       Mean reward: 216.11
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.0933
Episode_Reward/track_ang_vel_z_exp: 0.1239
       Episode_Reward/lin_vel_z_l2: -0.0492
      Episode_Reward/ang_vel_xy_l2: -0.0938
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4193
     Episode_Reward/action_rate_l2: -0.0086
Episode_Reward/flat_orientation_l2: -0.0194
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5324800
                    Iteration time: 5.86s
                      Time elapsed: 00:24:25
                               ETA: 00:13:13

################################################################################
                      [1m Learning iteration 325/500 [0m                      

                       Computation: 2833 steps/s (collection: 5.695s, learning 0.087s)
             Mean action noise std: 0.12
          Mean value_function loss: 0.0077
               Mean surrogate loss: -0.0063
                 Mean entropy loss: -9.5917
                       Mean reward: 216.11
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.9868
Episode_Reward/track_ang_vel_z_exp: 0.1215
       Episode_Reward/lin_vel_z_l2: -0.0553
      Episode_Reward/ang_vel_xy_l2: -0.0964
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4225
     Episode_Reward/action_rate_l2: -0.0084
Episode_Reward/flat_orientation_l2: -0.0198
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5341184
                    Iteration time: 5.78s
                      Time elapsed: 00:24:30
                               ETA: 00:13:09

################################################################################
                      [1m Learning iteration 326/500 [0m                      

                       Computation: 2859 steps/s (collection: 5.644s, learning 0.086s)
             Mean action noise std: 0.12
          Mean value_function loss: 0.0065
               Mean surrogate loss: -0.0060
                 Mean entropy loss: -9.6617
                       Mean reward: 216.11
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.9868
Episode_Reward/track_ang_vel_z_exp: 0.1215
       Episode_Reward/lin_vel_z_l2: -0.0553
      Episode_Reward/ang_vel_xy_l2: -0.0964
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4225
     Episode_Reward/action_rate_l2: -0.0084
Episode_Reward/flat_orientation_l2: -0.0198
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5357568
                    Iteration time: 5.73s
                      Time elapsed: 00:24:36
                               ETA: 00:13:05

################################################################################
                      [1m Learning iteration 327/500 [0m                      

                       Computation: 2853 steps/s (collection: 5.656s, learning 0.085s)
             Mean action noise std: 0.12
          Mean value_function loss: 0.0067
               Mean surrogate loss: -0.0056
                 Mean entropy loss: -9.7107
                       Mean reward: 216.11
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 6.9868
Episode_Reward/track_ang_vel_z_exp: 0.1215
       Episode_Reward/lin_vel_z_l2: -0.0553
      Episode_Reward/ang_vel_xy_l2: -0.0964
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4225
     Episode_Reward/action_rate_l2: -0.0084
Episode_Reward/flat_orientation_l2: -0.0198
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5373952
                    Iteration time: 5.74s
                      Time elapsed: 00:24:42
                               ETA: 00:13:01

################################################################################
                      [1m Learning iteration 328/500 [0m                      

                       Computation: 2817 steps/s (collection: 5.733s, learning 0.082s)
             Mean action noise std: 0.12
          Mean value_function loss: 0.0074
               Mean surrogate loss: -0.0040
                 Mean entropy loss: -9.7757
                       Mean reward: 216.30
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.1924
Episode_Reward/track_ang_vel_z_exp: 0.1243
       Episode_Reward/lin_vel_z_l2: -0.0509
      Episode_Reward/ang_vel_xy_l2: -0.0915
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4164
     Episode_Reward/action_rate_l2: -0.0084
Episode_Reward/flat_orientation_l2: -0.0191
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5390336
                    Iteration time: 5.81s
                      Time elapsed: 00:24:48
                               ETA: 00:12:58

################################################################################
                      [1m Learning iteration 329/500 [0m                      

                       Computation: 2815 steps/s (collection: 5.734s, learning 0.086s)
             Mean action noise std: 0.12
          Mean value_function loss: 0.0078
               Mean surrogate loss: -0.0052
                 Mean entropy loss: -9.8559
                       Mean reward: 216.30
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.2859
Episode_Reward/track_ang_vel_z_exp: 0.1255
       Episode_Reward/lin_vel_z_l2: -0.0489
      Episode_Reward/ang_vel_xy_l2: -0.0892
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4135
     Episode_Reward/action_rate_l2: -0.0084
Episode_Reward/flat_orientation_l2: -0.0187
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 5.82s
                      Time elapsed: 00:24:53
                               ETA: 00:12:54

################################################################################
                      [1m Learning iteration 330/500 [0m                      

                       Computation: 2838 steps/s (collection: 5.686s, learning 0.086s)
             Mean action noise std: 0.12
          Mean value_function loss: 0.0075
               Mean surrogate loss: -0.0043
                 Mean entropy loss: -9.9170
                       Mean reward: 216.30
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.2859
Episode_Reward/track_ang_vel_z_exp: 0.1255
       Episode_Reward/lin_vel_z_l2: -0.0489
      Episode_Reward/ang_vel_xy_l2: -0.0892
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4135
     Episode_Reward/action_rate_l2: -0.0084
Episode_Reward/flat_orientation_l2: -0.0187
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5423104
                    Iteration time: 5.77s
                      Time elapsed: 00:24:59
                               ETA: 00:12:50

################################################################################
                      [1m Learning iteration 331/500 [0m                      

                       Computation: 2833 steps/s (collection: 5.699s, learning 0.083s)
             Mean action noise std: 0.12
          Mean value_function loss: 0.0067
               Mean surrogate loss: -0.0055
                 Mean entropy loss: -9.9636
                       Mean reward: 216.30
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.2859
Episode_Reward/track_ang_vel_z_exp: 0.1255
       Episode_Reward/lin_vel_z_l2: -0.0489
      Episode_Reward/ang_vel_xy_l2: -0.0892
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4135
     Episode_Reward/action_rate_l2: -0.0084
Episode_Reward/flat_orientation_l2: -0.0187
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5439488
                    Iteration time: 5.78s
                      Time elapsed: 00:25:05
                               ETA: 00:12:46

################################################################################
                      [1m Learning iteration 332/500 [0m                      

                       Computation: 2801 steps/s (collection: 5.762s, learning 0.086s)
             Mean action noise std: 0.12
          Mean value_function loss: 0.0070
               Mean surrogate loss: -0.0049
                 Mean entropy loss: -10.0298
                       Mean reward: 216.30
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.2859
Episode_Reward/track_ang_vel_z_exp: 0.1255
       Episode_Reward/lin_vel_z_l2: -0.0489
      Episode_Reward/ang_vel_xy_l2: -0.0892
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4135
     Episode_Reward/action_rate_l2: -0.0084
Episode_Reward/flat_orientation_l2: -0.0187
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5455872
                    Iteration time: 5.85s
                      Time elapsed: 00:25:11
                               ETA: 00:12:42

################################################################################
                      [1m Learning iteration 333/500 [0m                      

                       Computation: 2821 steps/s (collection: 5.722s, learning 0.086s)
             Mean action noise std: 0.12
          Mean value_function loss: 0.0068
               Mean surrogate loss: -0.0066
                 Mean entropy loss: -10.0996
                       Mean reward: 216.30
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.2859
Episode_Reward/track_ang_vel_z_exp: 0.1255
       Episode_Reward/lin_vel_z_l2: -0.0489
      Episode_Reward/ang_vel_xy_l2: -0.0892
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4135
     Episode_Reward/action_rate_l2: -0.0084
Episode_Reward/flat_orientation_l2: -0.0187
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5472256
                    Iteration time: 5.81s
                      Time elapsed: 00:25:17
                               ETA: 00:12:38

################################################################################
                      [1m Learning iteration 334/500 [0m                      

                       Computation: 2847 steps/s (collection: 5.658s, learning 0.096s)
             Mean action noise std: 0.11
          Mean value_function loss: 0.0078
               Mean surrogate loss: -0.0063
                 Mean entropy loss: -10.1928
                       Mean reward: 216.28
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.0297
Episode_Reward/track_ang_vel_z_exp: 0.1343
       Episode_Reward/lin_vel_z_l2: -0.0423
      Episode_Reward/ang_vel_xy_l2: -0.0850
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4339
     Episode_Reward/action_rate_l2: -0.0079
Episode_Reward/flat_orientation_l2: -0.0184
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5488640
                    Iteration time: 5.75s
                      Time elapsed: 00:25:22
                               ETA: 00:12:34

################################################################################
                      [1m Learning iteration 335/500 [0m                      

                       Computation: 2857 steps/s (collection: 5.647s, learning 0.086s)
             Mean action noise std: 0.11
          Mean value_function loss: 0.0081
               Mean surrogate loss: -0.0045
                 Mean entropy loss: -10.3226
                       Mean reward: 216.38
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.3469
Episode_Reward/track_ang_vel_z_exp: 0.1412
       Episode_Reward/lin_vel_z_l2: -0.0405
      Episode_Reward/ang_vel_xy_l2: -0.0775
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4399
     Episode_Reward/action_rate_l2: -0.0075
Episode_Reward/flat_orientation_l2: -0.0189
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 5.73s
                      Time elapsed: 00:25:28
                               ETA: 00:12:30

################################################################################
                      [1m Learning iteration 336/500 [0m                      

                       Computation: 2809 steps/s (collection: 5.737s, learning 0.095s)
             Mean action noise std: 0.11
          Mean value_function loss: 0.0063
               Mean surrogate loss: -0.0061
                 Mean entropy loss: -10.3700
                       Mean reward: 216.38
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.3696
Episode_Reward/track_ang_vel_z_exp: 0.1455
       Episode_Reward/lin_vel_z_l2: -0.0349
      Episode_Reward/ang_vel_xy_l2: -0.0753
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4794
     Episode_Reward/action_rate_l2: -0.0074
Episode_Reward/flat_orientation_l2: -0.0179
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5521408
                    Iteration time: 5.83s
                      Time elapsed: 00:25:34
                               ETA: 00:12:26

################################################################################
                      [1m Learning iteration 337/500 [0m                      

                       Computation: 2843 steps/s (collection: 5.676s, learning 0.085s)
             Mean action noise std: 0.11
          Mean value_function loss: 0.0073
               Mean surrogate loss: -0.0061
                 Mean entropy loss: -10.4278
                       Mean reward: 216.38
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.3696
Episode_Reward/track_ang_vel_z_exp: 0.1455
       Episode_Reward/lin_vel_z_l2: -0.0349
      Episode_Reward/ang_vel_xy_l2: -0.0753
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4794
     Episode_Reward/action_rate_l2: -0.0074
Episode_Reward/flat_orientation_l2: -0.0179
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5537792
                    Iteration time: 5.76s
                      Time elapsed: 00:25:40
                               ETA: 00:12:22

################################################################################
                      [1m Learning iteration 338/500 [0m                      

                       Computation: 2854 steps/s (collection: 5.649s, learning 0.091s)
             Mean action noise std: 0.11
          Mean value_function loss: 0.0074
               Mean surrogate loss: -0.0074
                 Mean entropy loss: -10.5089
                       Mean reward: 216.38
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.3696
Episode_Reward/track_ang_vel_z_exp: 0.1455
       Episode_Reward/lin_vel_z_l2: -0.0349
      Episode_Reward/ang_vel_xy_l2: -0.0753
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4794
     Episode_Reward/action_rate_l2: -0.0074
Episode_Reward/flat_orientation_l2: -0.0179
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5554176
                    Iteration time: 5.74s
                      Time elapsed: 00:25:46
                               ETA: 00:12:18

################################################################################
                      [1m Learning iteration 339/500 [0m                      

                       Computation: 2840 steps/s (collection: 5.657s, learning 0.111s)
             Mean action noise std: 0.11
          Mean value_function loss: 0.0068
               Mean surrogate loss: -0.0050
                 Mean entropy loss: -10.5326
                       Mean reward: 216.38
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.3696
Episode_Reward/track_ang_vel_z_exp: 0.1455
       Episode_Reward/lin_vel_z_l2: -0.0349
      Episode_Reward/ang_vel_xy_l2: -0.0753
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4794
     Episode_Reward/action_rate_l2: -0.0074
Episode_Reward/flat_orientation_l2: -0.0179
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5570560
                    Iteration time: 5.77s
                      Time elapsed: 00:25:51
                               ETA: 00:12:14

################################################################################
                      [1m Learning iteration 340/500 [0m                      

                       Computation: 2821 steps/s (collection: 5.724s, learning 0.084s)
             Mean action noise std: 0.11
          Mean value_function loss: 0.0072
               Mean surrogate loss: -0.0053
                 Mean entropy loss: -10.5776
                       Mean reward: 217.34
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.2451
Episode_Reward/track_ang_vel_z_exp: 0.1381
       Episode_Reward/lin_vel_z_l2: -0.0421
      Episode_Reward/ang_vel_xy_l2: -0.0808
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4317
     Episode_Reward/action_rate_l2: -0.0073
Episode_Reward/flat_orientation_l2: -0.0183
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5586944
                    Iteration time: 5.81s
                      Time elapsed: 00:25:57
                               ETA: 00:12:10

################################################################################
                      [1m Learning iteration 341/500 [0m                      

                       Computation: 2830 steps/s (collection: 5.702s, learning 0.086s)
             Mean action noise std: 0.11
          Mean value_function loss: 0.0071
               Mean surrogate loss: -0.0045
                 Mean entropy loss: -10.6760
                       Mean reward: 218.92
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.2062
Episode_Reward/track_ang_vel_z_exp: 0.1327
       Episode_Reward/lin_vel_z_l2: -0.0440
      Episode_Reward/ang_vel_xy_l2: -0.0824
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4083
     Episode_Reward/action_rate_l2: -0.0073
Episode_Reward/flat_orientation_l2: -0.0185
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 5.79s
                      Time elapsed: 00:26:03
                               ETA: 00:12:06

################################################################################
                      [1m Learning iteration 342/500 [0m                      

                       Computation: 2838 steps/s (collection: 5.685s, learning 0.086s)
             Mean action noise std: 0.11
          Mean value_function loss: 0.0075
               Mean surrogate loss: -0.0049
                 Mean entropy loss: -10.7139
                       Mean reward: 221.51
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.0951
Episode_Reward/track_ang_vel_z_exp: 0.1368
       Episode_Reward/lin_vel_z_l2: -0.0411
      Episode_Reward/ang_vel_xy_l2: -0.0809
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4185
     Episode_Reward/action_rate_l2: -0.0070
Episode_Reward/flat_orientation_l2: -0.0181
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0312
--------------------------------------------------------------------------------
                   Total timesteps: 5619712
                    Iteration time: 5.77s
                      Time elapsed: 00:26:09
                               ETA: 00:12:02

################################################################################
                      [1m Learning iteration 343/500 [0m                      

                       Computation: 2819 steps/s (collection: 5.726s, learning 0.085s)
             Mean action noise std: 0.11
          Mean value_function loss: 0.0083
               Mean surrogate loss: -0.0065
                 Mean entropy loss: -10.7643
                       Mean reward: 228.36
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.1587
Episode_Reward/track_ang_vel_z_exp: 0.1355
       Episode_Reward/lin_vel_z_l2: -0.0434
      Episode_Reward/ang_vel_xy_l2: -0.0807
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4129
     Episode_Reward/action_rate_l2: -0.0070
Episode_Reward/flat_orientation_l2: -0.0186
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.8438
--------------------------------------------------------------------------------
                   Total timesteps: 5636096
                    Iteration time: 5.81s
                      Time elapsed: 00:26:14
                               ETA: 00:11:58

################################################################################
                      [1m Learning iteration 344/500 [0m                      

                       Computation: 2764 steps/s (collection: 5.842s, learning 0.085s)
             Mean action noise std: 0.11
          Mean value_function loss: 0.0126
               Mean surrogate loss: -0.0060
                 Mean entropy loss: -10.7844
                       Mean reward: 233.93
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.2052
Episode_Reward/track_ang_vel_z_exp: 0.1374
       Episode_Reward/lin_vel_z_l2: -0.0404
      Episode_Reward/ang_vel_xy_l2: -0.0789
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4328
     Episode_Reward/action_rate_l2: -0.0068
Episode_Reward/flat_orientation_l2: -0.0182
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 2.1875
--------------------------------------------------------------------------------
                   Total timesteps: 5652480
                    Iteration time: 5.93s
                      Time elapsed: 00:26:20
                               ETA: 00:11:54

################################################################################
                      [1m Learning iteration 345/500 [0m                      

                       Computation: 2780 steps/s (collection: 5.809s, learning 0.083s)
             Mean action noise std: 0.11
          Mean value_function loss: 0.0090
               Mean surrogate loss: -0.0061
                 Mean entropy loss: -10.7951
                       Mean reward: 234.05
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.1895
Episode_Reward/track_ang_vel_z_exp: 0.1394
       Episode_Reward/lin_vel_z_l2: -0.0404
      Episode_Reward/ang_vel_xy_l2: -0.0785
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4276
     Episode_Reward/action_rate_l2: -0.0068
Episode_Reward/flat_orientation_l2: -0.0182
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 5668864
                    Iteration time: 5.89s
                      Time elapsed: 00:26:26
                               ETA: 00:11:50

################################################################################
                      [1m Learning iteration 346/500 [0m                      

                       Computation: 2814 steps/s (collection: 5.735s, learning 0.088s)
             Mean action noise std: 0.11
          Mean value_function loss: 0.0087
               Mean surrogate loss: -0.0052
                 Mean entropy loss: -10.8640
                       Mean reward: 235.63
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.2446
Episode_Reward/track_ang_vel_z_exp: 0.1392
       Episode_Reward/lin_vel_z_l2: -0.0406
      Episode_Reward/ang_vel_xy_l2: -0.0788
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4142
     Episode_Reward/action_rate_l2: -0.0067
Episode_Reward/flat_orientation_l2: -0.0181
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 2.1875
--------------------------------------------------------------------------------
                   Total timesteps: 5685248
                    Iteration time: 5.82s
                      Time elapsed: 00:26:32
                               ETA: 00:11:46

################################################################################
                      [1m Learning iteration 347/500 [0m                      

                       Computation: 2812 steps/s (collection: 5.732s, learning 0.093s)
             Mean action noise std: 0.11
          Mean value_function loss: 0.0083
               Mean surrogate loss: -0.0045
                 Mean entropy loss: -10.9117
                       Mean reward: 236.74
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.2590
Episode_Reward/track_ang_vel_z_exp: 0.1420
       Episode_Reward/lin_vel_z_l2: -0.0387
      Episode_Reward/ang_vel_xy_l2: -0.0761
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4163
     Episode_Reward/action_rate_l2: -0.0066
Episode_Reward/flat_orientation_l2: -0.0179
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.9688
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 5.82s
                      Time elapsed: 00:26:38
                               ETA: 00:11:42

################################################################################
                      [1m Learning iteration 348/500 [0m                      

                       Computation: 2782 steps/s (collection: 5.804s, learning 0.084s)
             Mean action noise std: 0.11
          Mean value_function loss: 0.0084
               Mean surrogate loss: -0.0021
                 Mean entropy loss: -10.9670
                       Mean reward: 236.16
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.2250
Episode_Reward/track_ang_vel_z_exp: 0.1402
       Episode_Reward/lin_vel_z_l2: -0.0392
      Episode_Reward/ang_vel_xy_l2: -0.0769
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4259
     Episode_Reward/action_rate_l2: -0.0065
Episode_Reward/flat_orientation_l2: -0.0180
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.5312
--------------------------------------------------------------------------------
                   Total timesteps: 5718016
                    Iteration time: 5.89s
                      Time elapsed: 00:26:44
                               ETA: 00:11:38

################################################################################
                      [1m Learning iteration 349/500 [0m                      

                       Computation: 2801 steps/s (collection: 5.763s, learning 0.085s)
             Mean action noise std: 0.11
          Mean value_function loss: 0.0072
               Mean surrogate loss: -0.0055
                 Mean entropy loss: -11.0312
                       Mean reward: 235.90
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.2496
Episode_Reward/track_ang_vel_z_exp: 0.1409
       Episode_Reward/lin_vel_z_l2: -0.0395
      Episode_Reward/ang_vel_xy_l2: -0.0770
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4140
     Episode_Reward/action_rate_l2: -0.0065
Episode_Reward/flat_orientation_l2: -0.0183
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.4375
--------------------------------------------------------------------------------
                   Total timesteps: 5734400
                    Iteration time: 5.85s
                      Time elapsed: 00:26:50
                               ETA: 00:11:34

################################################################################
                      [1m Learning iteration 350/500 [0m                      

                       Computation: 2834 steps/s (collection: 5.688s, learning 0.093s)
             Mean action noise std: 0.11
          Mean value_function loss: 0.0076
               Mean surrogate loss: -0.0065
                 Mean entropy loss: -11.0842
                       Mean reward: 235.26
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.1838
Episode_Reward/track_ang_vel_z_exp: 0.1403
       Episode_Reward/lin_vel_z_l2: -0.0404
      Episode_Reward/ang_vel_xy_l2: -0.0775
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4266
     Episode_Reward/action_rate_l2: -0.0064
Episode_Reward/flat_orientation_l2: -0.0183
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.4688
--------------------------------------------------------------------------------
                   Total timesteps: 5750784
                    Iteration time: 5.78s
                      Time elapsed: 00:26:55
                               ETA: 00:11:30

################################################################################
                      [1m Learning iteration 351/500 [0m                      

                       Computation: 2824 steps/s (collection: 5.708s, learning 0.093s)
             Mean action noise std: 0.11
          Mean value_function loss: 0.0069
               Mean surrogate loss: -0.0051
                 Mean entropy loss: -11.1257
                       Mean reward: 235.32
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.2826
Episode_Reward/track_ang_vel_z_exp: 0.1414
       Episode_Reward/lin_vel_z_l2: -0.0378
      Episode_Reward/ang_vel_xy_l2: -0.0757
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4211
     Episode_Reward/action_rate_l2: -0.0062
Episode_Reward/flat_orientation_l2: -0.0179
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.3125
--------------------------------------------------------------------------------
                   Total timesteps: 5767168
                    Iteration time: 5.80s
                      Time elapsed: 00:27:01
                               ETA: 00:11:26

################################################################################
                      [1m Learning iteration 352/500 [0m                      

                       Computation: 2819 steps/s (collection: 5.728s, learning 0.084s)
             Mean action noise std: 0.11
          Mean value_function loss: 0.0064
               Mean surrogate loss: -0.0066
                 Mean entropy loss: -11.2324
                       Mean reward: 236.14
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.1979
Episode_Reward/track_ang_vel_z_exp: 0.1415
       Episode_Reward/lin_vel_z_l2: -0.0388
      Episode_Reward/ang_vel_xy_l2: -0.0778
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4243
     Episode_Reward/action_rate_l2: -0.0063
Episode_Reward/flat_orientation_l2: -0.0182
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.2188
--------------------------------------------------------------------------------
                   Total timesteps: 5783552
                    Iteration time: 5.81s
                      Time elapsed: 00:27:07
                               ETA: 00:11:22

################################################################################
                      [1m Learning iteration 353/500 [0m                      

                       Computation: 2819 steps/s (collection: 5.718s, learning 0.093s)
             Mean action noise std: 0.11
          Mean value_function loss: 0.0062
               Mean surrogate loss: -0.0041
                 Mean entropy loss: -11.3109
                       Mean reward: 237.46
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.3072
Episode_Reward/track_ang_vel_z_exp: 0.1409
       Episode_Reward/lin_vel_z_l2: -0.0416
      Episode_Reward/ang_vel_xy_l2: -0.0774
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4018
     Episode_Reward/action_rate_l2: -0.0063
Episode_Reward/flat_orientation_l2: -0.0184
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 5.81s
                      Time elapsed: 00:27:13
                               ETA: 00:11:18

################################################################################
                      [1m Learning iteration 354/500 [0m                      

                       Computation: 2852 steps/s (collection: 5.651s, learning 0.093s)
             Mean action noise std: 0.11
          Mean value_function loss: 0.0063
               Mean surrogate loss: -0.0040
                 Mean entropy loss: -11.3384
                       Mean reward: 237.47
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.2503
Episode_Reward/track_ang_vel_z_exp: 0.1406
       Episode_Reward/lin_vel_z_l2: -0.0380
      Episode_Reward/ang_vel_xy_l2: -0.0760
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4311
     Episode_Reward/action_rate_l2: -0.0061
Episode_Reward/flat_orientation_l2: -0.0181
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.3125
--------------------------------------------------------------------------------
                   Total timesteps: 5816320
                    Iteration time: 5.74s
                      Time elapsed: 00:27:19
                               ETA: 00:11:14

################################################################################
                      [1m Learning iteration 355/500 [0m                      

                       Computation: 2828 steps/s (collection: 5.697s, learning 0.095s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0059
               Mean surrogate loss: -0.0020
                 Mean entropy loss: -11.3805
                       Mean reward: 237.10
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4130
Episode_Reward/track_ang_vel_z_exp: 0.1478
       Episode_Reward/lin_vel_z_l2: -0.0354
      Episode_Reward/ang_vel_xy_l2: -0.0696
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4288
     Episode_Reward/action_rate_l2: -0.0059
Episode_Reward/flat_orientation_l2: -0.0177
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5832704
                    Iteration time: 5.79s
                      Time elapsed: 00:27:24
                               ETA: 00:11:09

################################################################################
                      [1m Learning iteration 356/500 [0m                      

                       Computation: 2821 steps/s (collection: 5.722s, learning 0.085s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0057
               Mean surrogate loss: -0.0046
                 Mean entropy loss: -11.4183
                       Mean reward: 237.19
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.1095
Episode_Reward/track_ang_vel_z_exp: 0.1511
       Episode_Reward/lin_vel_z_l2: -0.0315
      Episode_Reward/ang_vel_xy_l2: -0.0708
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.4719
     Episode_Reward/action_rate_l2: -0.0058
Episode_Reward/flat_orientation_l2: -0.0166
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0312
--------------------------------------------------------------------------------
                   Total timesteps: 5849088
                    Iteration time: 5.81s
                      Time elapsed: 00:27:30
                               ETA: 00:11:05

################################################################################
                      [1m Learning iteration 357/500 [0m                      

                       Computation: 2835 steps/s (collection: 5.693s, learning 0.085s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0059
               Mean surrogate loss: -0.0035
                 Mean entropy loss: -11.4611
                       Mean reward: 237.55
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.3833
Episode_Reward/track_ang_vel_z_exp: 0.1450
       Episode_Reward/lin_vel_z_l2: -0.0330
      Episode_Reward/ang_vel_xy_l2: -0.0682
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.4334
     Episode_Reward/action_rate_l2: -0.0057
Episode_Reward/flat_orientation_l2: -0.0172
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5865472
                    Iteration time: 5.78s
                      Time elapsed: 00:27:36
                               ETA: 00:11:01

################################################################################
                      [1m Learning iteration 358/500 [0m                      

                       Computation: 2836 steps/s (collection: 5.678s, learning 0.099s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0061
               Mean surrogate loss: -0.0054
                 Mean entropy loss: -11.4968
                       Mean reward: 238.07
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4556
Episode_Reward/track_ang_vel_z_exp: 0.1507
       Episode_Reward/lin_vel_z_l2: -0.0342
      Episode_Reward/ang_vel_xy_l2: -0.0692
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.4352
     Episode_Reward/action_rate_l2: -0.0057
Episode_Reward/flat_orientation_l2: -0.0174
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0938
--------------------------------------------------------------------------------
                   Total timesteps: 5881856
                    Iteration time: 5.78s
                      Time elapsed: 00:27:42
                               ETA: 00:10:57

################################################################################
                      [1m Learning iteration 359/500 [0m                      

                       Computation: 2835 steps/s (collection: 5.680s, learning 0.099s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0062
               Mean surrogate loss: -0.0048
                 Mean entropy loss: -11.5505
                       Mean reward: 238.26
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.3664
Episode_Reward/track_ang_vel_z_exp: 0.1459
       Episode_Reward/lin_vel_z_l2: -0.0393
      Episode_Reward/ang_vel_xy_l2: -0.0714
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.4012
     Episode_Reward/action_rate_l2: -0.0058
Episode_Reward/flat_orientation_l2: -0.0179
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 5.78s
                      Time elapsed: 00:27:48
                               ETA: 00:10:53

################################################################################
                      [1m Learning iteration 360/500 [0m                      

                       Computation: 2807 steps/s (collection: 5.746s, learning 0.089s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0064
               Mean surrogate loss: -0.0055
                 Mean entropy loss: -11.5733
                       Mean reward: 238.56
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4308
Episode_Reward/track_ang_vel_z_exp: 0.1514
       Episode_Reward/lin_vel_z_l2: -0.0356
      Episode_Reward/ang_vel_xy_l2: -0.0696
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.4231
     Episode_Reward/action_rate_l2: -0.0056
Episode_Reward/flat_orientation_l2: -0.0182
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5914624
                    Iteration time: 5.84s
                      Time elapsed: 00:27:53
                               ETA: 00:10:49

################################################################################
                      [1m Learning iteration 361/500 [0m                      

                       Computation: 2813 steps/s (collection: 5.735s, learning 0.087s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0056
               Mean surrogate loss: -0.0046
                 Mean entropy loss: -11.6509
                       Mean reward: 239.13
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.3116
Episode_Reward/track_ang_vel_z_exp: 0.1463
       Episode_Reward/lin_vel_z_l2: -0.0429
      Episode_Reward/ang_vel_xy_l2: -0.0743
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3832
     Episode_Reward/action_rate_l2: -0.0058
Episode_Reward/flat_orientation_l2: -0.0186
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5931008
                    Iteration time: 5.82s
                      Time elapsed: 00:27:59
                               ETA: 00:10:44

################################################################################
                      [1m Learning iteration 362/500 [0m                      

                       Computation: 2843 steps/s (collection: 5.674s, learning 0.088s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0066
               Mean surrogate loss: -0.0027
                 Mean entropy loss: -11.6982
                       Mean reward: 239.12
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.2950
Episode_Reward/track_ang_vel_z_exp: 0.1431
       Episode_Reward/lin_vel_z_l2: -0.0473
      Episode_Reward/ang_vel_xy_l2: -0.0771
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3734
     Episode_Reward/action_rate_l2: -0.0058
Episode_Reward/flat_orientation_l2: -0.0195
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5947392
                    Iteration time: 5.76s
                      Time elapsed: 00:28:05
                               ETA: 00:10:40

################################################################################
                      [1m Learning iteration 363/500 [0m                      

                       Computation: 2835 steps/s (collection: 5.694s, learning 0.084s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0054
               Mean surrogate loss: -0.0050
                 Mean entropy loss: -11.7747
                       Mean reward: 239.12
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.3344
Episode_Reward/track_ang_vel_z_exp: 0.1414
       Episode_Reward/lin_vel_z_l2: -0.0457
      Episode_Reward/ang_vel_xy_l2: -0.0768
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3908
     Episode_Reward/action_rate_l2: -0.0058
Episode_Reward/flat_orientation_l2: -0.0196
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5963776
                    Iteration time: 5.78s
                      Time elapsed: 00:28:11
                               ETA: 00:10:36

################################################################################
                      [1m Learning iteration 364/500 [0m                      

                       Computation: 2807 steps/s (collection: 5.748s, learning 0.088s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0055
               Mean surrogate loss: -0.0035
                 Mean entropy loss: -11.8592
                       Mean reward: 239.23
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4767
Episode_Reward/track_ang_vel_z_exp: 0.1502
       Episode_Reward/lin_vel_z_l2: -0.0390
      Episode_Reward/ang_vel_xy_l2: -0.0670
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3655
     Episode_Reward/action_rate_l2: -0.0054
Episode_Reward/flat_orientation_l2: -0.0186
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5980160
                    Iteration time: 5.84s
                      Time elapsed: 00:28:17
                               ETA: 00:10:32

################################################################################
                      [1m Learning iteration 365/500 [0m                      

                       Computation: 2845 steps/s (collection: 5.668s, learning 0.090s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0056
               Mean surrogate loss: -0.0045
                 Mean entropy loss: -11.9164
                       Mean reward: 239.52
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.3098
Episode_Reward/track_ang_vel_z_exp: 0.1512
       Episode_Reward/lin_vel_z_l2: -0.0376
      Episode_Reward/ang_vel_xy_l2: -0.0706
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3814
     Episode_Reward/action_rate_l2: -0.0054
Episode_Reward/flat_orientation_l2: -0.0177
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 5.76s
                      Time elapsed: 00:28:22
                               ETA: 00:10:28

################################################################################
                      [1m Learning iteration 366/500 [0m                      

                       Computation: 2862 steps/s (collection: 5.631s, learning 0.093s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0057
               Mean surrogate loss: -0.0030
                 Mean entropy loss: -11.9516
                       Mean reward: 239.60
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.2993
Episode_Reward/track_ang_vel_z_exp: 0.1568
       Episode_Reward/lin_vel_z_l2: -0.0318
      Episode_Reward/ang_vel_xy_l2: -0.0687
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.4295
     Episode_Reward/action_rate_l2: -0.0053
Episode_Reward/flat_orientation_l2: -0.0170
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6012928
                    Iteration time: 5.72s
                      Time elapsed: 00:28:28
                               ETA: 00:10:23

################################################################################
                      [1m Learning iteration 367/500 [0m                      

                       Computation: 2862 steps/s (collection: 5.637s, learning 0.086s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0060
               Mean surrogate loss: -0.0042
                 Mean entropy loss: -11.9963
                       Mean reward: 239.60
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.3781
Episode_Reward/track_ang_vel_z_exp: 0.1559
       Episode_Reward/lin_vel_z_l2: -0.0310
      Episode_Reward/ang_vel_xy_l2: -0.0658
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.4465
     Episode_Reward/action_rate_l2: -0.0052
Episode_Reward/flat_orientation_l2: -0.0171
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6029312
                    Iteration time: 5.72s
                      Time elapsed: 00:28:34
                               ETA: 00:10:19

################################################################################
                      [1m Learning iteration 368/500 [0m                      

                       Computation: 2816 steps/s (collection: 5.731s, learning 0.087s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0055
               Mean surrogate loss: -0.0040
                 Mean entropy loss: -12.0128
                       Mean reward: 239.82
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5088
Episode_Reward/track_ang_vel_z_exp: 0.1578
       Episode_Reward/lin_vel_z_l2: -0.0291
      Episode_Reward/ang_vel_xy_l2: -0.0653
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.4278
     Episode_Reward/action_rate_l2: -0.0050
Episode_Reward/flat_orientation_l2: -0.0174
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6045696
                    Iteration time: 5.82s
                      Time elapsed: 00:28:40
                               ETA: 00:10:15

################################################################################
                      [1m Learning iteration 369/500 [0m                      

                       Computation: 2826 steps/s (collection: 5.698s, learning 0.098s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0051
               Mean surrogate loss: -0.0041
                 Mean entropy loss: -12.0760
                       Mean reward: 239.82
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5275
Episode_Reward/track_ang_vel_z_exp: 0.1581
       Episode_Reward/lin_vel_z_l2: -0.0288
      Episode_Reward/ang_vel_xy_l2: -0.0652
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.4252
     Episode_Reward/action_rate_l2: -0.0050
Episode_Reward/flat_orientation_l2: -0.0175
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6062080
                    Iteration time: 5.80s
                      Time elapsed: 00:28:45
                               ETA: 00:10:11

################################################################################
                      [1m Learning iteration 370/500 [0m                      

                       Computation: 2857 steps/s (collection: 5.626s, learning 0.108s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0055
               Mean surrogate loss: -0.0055
                 Mean entropy loss: -12.1003
                       Mean reward: 239.82
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5275
Episode_Reward/track_ang_vel_z_exp: 0.1581
       Episode_Reward/lin_vel_z_l2: -0.0288
      Episode_Reward/ang_vel_xy_l2: -0.0652
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.4252
     Episode_Reward/action_rate_l2: -0.0050
Episode_Reward/flat_orientation_l2: -0.0175
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6078464
                    Iteration time: 5.73s
                      Time elapsed: 00:28:51
                               ETA: 00:10:06

################################################################################
                      [1m Learning iteration 371/500 [0m                      

                       Computation: 2864 steps/s (collection: 5.636s, learning 0.083s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0060
               Mean surrogate loss: -0.0046
                 Mean entropy loss: -12.1571
                       Mean reward: 239.82
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5275
Episode_Reward/track_ang_vel_z_exp: 0.1581
       Episode_Reward/lin_vel_z_l2: -0.0288
      Episode_Reward/ang_vel_xy_l2: -0.0652
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.4252
     Episode_Reward/action_rate_l2: -0.0050
Episode_Reward/flat_orientation_l2: -0.0175
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 5.72s
                      Time elapsed: 00:28:57
                               ETA: 00:10:02

################################################################################
                      [1m Learning iteration 372/500 [0m                      

                       Computation: 2839 steps/s (collection: 5.687s, learning 0.084s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0060
               Mean surrogate loss: -0.0066
                 Mean entropy loss: -12.1944
                       Mean reward: 239.82
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5275
Episode_Reward/track_ang_vel_z_exp: 0.1581
       Episode_Reward/lin_vel_z_l2: -0.0288
      Episode_Reward/ang_vel_xy_l2: -0.0652
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.4252
     Episode_Reward/action_rate_l2: -0.0050
Episode_Reward/flat_orientation_l2: -0.0175
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6111232
                    Iteration time: 5.77s
                      Time elapsed: 00:29:03
                               ETA: 00:09:58

################################################################################
                      [1m Learning iteration 373/500 [0m                      

                       Computation: 2839 steps/s (collection: 5.684s, learning 0.086s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0060
               Mean surrogate loss: -0.0059
                 Mean entropy loss: -12.2384
                       Mean reward: 239.88
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4566
Episode_Reward/track_ang_vel_z_exp: 0.1475
       Episode_Reward/lin_vel_z_l2: -0.0399
      Episode_Reward/ang_vel_xy_l2: -0.0687
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3744
     Episode_Reward/action_rate_l2: -0.0050
Episode_Reward/flat_orientation_l2: -0.0183
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6127616
                    Iteration time: 5.77s
                      Time elapsed: 00:29:08
                               ETA: 00:09:53

################################################################################
                      [1m Learning iteration 374/500 [0m                      

                       Computation: 2866 steps/s (collection: 5.631s, learning 0.084s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0059
               Mean surrogate loss: -0.0040
                 Mean entropy loss: -12.2637
                       Mean reward: 239.88
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4544
Episode_Reward/track_ang_vel_z_exp: 0.1471
       Episode_Reward/lin_vel_z_l2: -0.0402
      Episode_Reward/ang_vel_xy_l2: -0.0688
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3728
     Episode_Reward/action_rate_l2: -0.0050
Episode_Reward/flat_orientation_l2: -0.0183
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6144000
                    Iteration time: 5.72s
                      Time elapsed: 00:29:14
                               ETA: 00:09:49

################################################################################
                      [1m Learning iteration 375/500 [0m                      

                       Computation: 2872 steps/s (collection: 5.617s, learning 0.086s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0062
               Mean surrogate loss: -0.0058
                 Mean entropy loss: -12.2819
                       Mean reward: 239.88
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4544
Episode_Reward/track_ang_vel_z_exp: 0.1471
       Episode_Reward/lin_vel_z_l2: -0.0402
      Episode_Reward/ang_vel_xy_l2: -0.0688
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3728
     Episode_Reward/action_rate_l2: -0.0050
Episode_Reward/flat_orientation_l2: -0.0183
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6160384
                    Iteration time: 5.70s
                      Time elapsed: 00:29:20
                               ETA: 00:09:45

################################################################################
                      [1m Learning iteration 376/500 [0m                      

                       Computation: 2814 steps/s (collection: 5.734s, learning 0.087s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0054
               Mean surrogate loss: -0.0034
                 Mean entropy loss: -12.3161
                       Mean reward: 239.88
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4544
Episode_Reward/track_ang_vel_z_exp: 0.1471
       Episode_Reward/lin_vel_z_l2: -0.0402
      Episode_Reward/ang_vel_xy_l2: -0.0688
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3728
     Episode_Reward/action_rate_l2: -0.0050
Episode_Reward/flat_orientation_l2: -0.0183
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6176768
                    Iteration time: 5.82s
                      Time elapsed: 00:29:26
                               ETA: 00:09:40

################################################################################
                      [1m Learning iteration 377/500 [0m                      

                       Computation: 2818 steps/s (collection: 5.731s, learning 0.082s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0053
               Mean surrogate loss: -0.0064
                 Mean entropy loss: -12.3586
                       Mean reward: 239.88
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4544
Episode_Reward/track_ang_vel_z_exp: 0.1471
       Episode_Reward/lin_vel_z_l2: -0.0402
      Episode_Reward/ang_vel_xy_l2: -0.0688
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3728
     Episode_Reward/action_rate_l2: -0.0050
Episode_Reward/flat_orientation_l2: -0.0183
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 5.81s
                      Time elapsed: 00:29:31
                               ETA: 00:09:36

################################################################################
                      [1m Learning iteration 378/500 [0m                      

                       Computation: 2842 steps/s (collection: 5.650s, learning 0.115s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0059
               Mean surrogate loss: -0.0053
                 Mean entropy loss: -12.4046
                       Mean reward: 239.88
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4544
Episode_Reward/track_ang_vel_z_exp: 0.1471
       Episode_Reward/lin_vel_z_l2: -0.0402
      Episode_Reward/ang_vel_xy_l2: -0.0688
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3728
     Episode_Reward/action_rate_l2: -0.0050
Episode_Reward/flat_orientation_l2: -0.0183
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6209536
                    Iteration time: 5.76s
                      Time elapsed: 00:29:37
                               ETA: 00:09:32

################################################################################
                      [1m Learning iteration 379/500 [0m                      

                       Computation: 2884 steps/s (collection: 5.592s, learning 0.089s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0054
               Mean surrogate loss: -0.0047
                 Mean entropy loss: -12.4751
                       Mean reward: 240.48
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4726
Episode_Reward/track_ang_vel_z_exp: 0.1551
       Episode_Reward/lin_vel_z_l2: -0.0386
      Episode_Reward/ang_vel_xy_l2: -0.0663
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3885
     Episode_Reward/action_rate_l2: -0.0048
Episode_Reward/flat_orientation_l2: -0.0193
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6225920
                    Iteration time: 5.68s
                      Time elapsed: 00:29:43
                               ETA: 00:09:27

################################################################################
                      [1m Learning iteration 380/500 [0m                      

                       Computation: 2781 steps/s (collection: 5.803s, learning 0.086s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0051
               Mean surrogate loss: -0.0043
                 Mean entropy loss: -12.5118
                       Mean reward: 240.41
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5756
Episode_Reward/track_ang_vel_z_exp: 0.1692
       Episode_Reward/lin_vel_z_l2: -0.0288
      Episode_Reward/ang_vel_xy_l2: -0.0581
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3974
     Episode_Reward/action_rate_l2: -0.0046
Episode_Reward/flat_orientation_l2: -0.0175
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6242304
                    Iteration time: 5.89s
                      Time elapsed: 00:29:49
                               ETA: 00:09:23

################################################################################
                      [1m Learning iteration 381/500 [0m                      

                       Computation: 2718 steps/s (collection: 5.878s, learning 0.149s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0048
               Mean surrogate loss: -0.0051
                 Mean entropy loss: -12.5453
                       Mean reward: 240.41
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.1036
Episode_Reward/track_ang_vel_z_exp: 0.1496
       Episode_Reward/lin_vel_z_l2: -0.0462
      Episode_Reward/ang_vel_xy_l2: -0.0750
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3421
     Episode_Reward/action_rate_l2: -0.0051
Episode_Reward/flat_orientation_l2: -0.0192
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6258688
                    Iteration time: 6.03s
                      Time elapsed: 00:29:55
                               ETA: 00:09:19

################################################################################
                      [1m Learning iteration 382/500 [0m                      

                       Computation: 2831 steps/s (collection: 5.702s, learning 0.084s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0056
               Mean surrogate loss: -0.0026
                 Mean entropy loss: -12.5750
                       Mean reward: 240.54
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4169
Episode_Reward/track_ang_vel_z_exp: 0.1544
       Episode_Reward/lin_vel_z_l2: -0.0386
      Episode_Reward/ang_vel_xy_l2: -0.0693
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3418
     Episode_Reward/action_rate_l2: -0.0047
Episode_Reward/flat_orientation_l2: -0.0186
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6275072
                    Iteration time: 5.79s
                      Time elapsed: 00:30:01
                               ETA: 00:09:14

################################################################################
                      [1m Learning iteration 383/500 [0m                      

                       Computation: 2814 steps/s (collection: 5.736s, learning 0.085s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0056
               Mean surrogate loss: -0.0039
                 Mean entropy loss: -12.5860
                       Mean reward: 240.54
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4892
Episode_Reward/track_ang_vel_z_exp: 0.1556
       Episode_Reward/lin_vel_z_l2: -0.0368
      Episode_Reward/ang_vel_xy_l2: -0.0680
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3417
     Episode_Reward/action_rate_l2: -0.0047
Episode_Reward/flat_orientation_l2: -0.0184
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 5.82s
                      Time elapsed: 00:30:06
                               ETA: 00:09:10

################################################################################
                      [1m Learning iteration 384/500 [0m                      

                       Computation: 2816 steps/s (collection: 5.730s, learning 0.087s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0052
               Mean surrogate loss: -0.0030
                 Mean entropy loss: -12.6259
                       Mean reward: 240.70
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4845
Episode_Reward/track_ang_vel_z_exp: 0.1634
       Episode_Reward/lin_vel_z_l2: -0.0374
      Episode_Reward/ang_vel_xy_l2: -0.0661
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3466
     Episode_Reward/action_rate_l2: -0.0046
Episode_Reward/flat_orientation_l2: -0.0185
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6307840
                    Iteration time: 5.82s
                      Time elapsed: 00:30:12
                               ETA: 00:09:06

################################################################################
                      [1m Learning iteration 385/500 [0m                      

                       Computation: 2833 steps/s (collection: 5.695s, learning 0.086s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0059
               Mean surrogate loss: -0.0038
                 Mean entropy loss: -12.6647
                       Mean reward: 240.70
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4831
Episode_Reward/track_ang_vel_z_exp: 0.1655
       Episode_Reward/lin_vel_z_l2: -0.0376
      Episode_Reward/ang_vel_xy_l2: -0.0656
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3480
     Episode_Reward/action_rate_l2: -0.0046
Episode_Reward/flat_orientation_l2: -0.0186
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6324224
                    Iteration time: 5.78s
                      Time elapsed: 00:30:18
                               ETA: 00:09:01

################################################################################
                      [1m Learning iteration 386/500 [0m                      

                       Computation: 2855 steps/s (collection: 5.654s, learning 0.084s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0051
               Mean surrogate loss: -0.0051
                 Mean entropy loss: -12.6958
                       Mean reward: 240.91
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4968
Episode_Reward/track_ang_vel_z_exp: 0.1639
       Episode_Reward/lin_vel_z_l2: -0.0371
      Episode_Reward/ang_vel_xy_l2: -0.0645
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3468
     Episode_Reward/action_rate_l2: -0.0046
Episode_Reward/flat_orientation_l2: -0.0186
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6340608
                    Iteration time: 5.74s
                      Time elapsed: 00:30:24
                               ETA: 00:08:57

################################################################################
                      [1m Learning iteration 387/500 [0m                      

                       Computation: 2840 steps/s (collection: 5.685s, learning 0.083s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0061
               Mean surrogate loss: -0.0050
                 Mean entropy loss: -12.7428
                       Mean reward: 240.96
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4518
Episode_Reward/track_ang_vel_z_exp: 0.1546
       Episode_Reward/lin_vel_z_l2: -0.0427
      Episode_Reward/ang_vel_xy_l2: -0.0668
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3906
     Episode_Reward/action_rate_l2: -0.0046
Episode_Reward/flat_orientation_l2: -0.0203
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6356992
                    Iteration time: 5.77s
                      Time elapsed: 00:30:30
                               ETA: 00:08:52

################################################################################
                      [1m Learning iteration 388/500 [0m                      

                       Computation: 2834 steps/s (collection: 5.688s, learning 0.092s)
             Mean action noise std: 0.10
          Mean value_function loss: 0.0062
               Mean surrogate loss: -0.0047
                 Mean entropy loss: -12.7977
                       Mean reward: 240.96
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4456
Episode_Reward/track_ang_vel_z_exp: 0.1543
       Episode_Reward/lin_vel_z_l2: -0.0431
      Episode_Reward/ang_vel_xy_l2: -0.0673
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3938
     Episode_Reward/action_rate_l2: -0.0046
Episode_Reward/flat_orientation_l2: -0.0204
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6373376
                    Iteration time: 5.78s
                      Time elapsed: 00:30:35
                               ETA: 00:08:48

################################################################################
                      [1m Learning iteration 389/500 [0m                      

                       Computation: 2836 steps/s (collection: 5.692s, learning 0.085s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0052
               Mean surrogate loss: -0.0041
                 Mean entropy loss: -12.9452
                       Mean reward: 240.96
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4456
Episode_Reward/track_ang_vel_z_exp: 0.1543
       Episode_Reward/lin_vel_z_l2: -0.0431
      Episode_Reward/ang_vel_xy_l2: -0.0673
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3938
     Episode_Reward/action_rate_l2: -0.0046
Episode_Reward/flat_orientation_l2: -0.0204
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 5.78s
                      Time elapsed: 00:30:41
                               ETA: 00:08:44

################################################################################
                      [1m Learning iteration 390/500 [0m                      

                       Computation: 2850 steps/s (collection: 5.652s, learning 0.095s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0053
               Mean surrogate loss: -0.0032
                 Mean entropy loss: -12.9583
                       Mean reward: 240.98
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4075
Episode_Reward/track_ang_vel_z_exp: 0.1552
       Episode_Reward/lin_vel_z_l2: -0.0425
      Episode_Reward/ang_vel_xy_l2: -0.0671
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3789
     Episode_Reward/action_rate_l2: -0.0046
Episode_Reward/flat_orientation_l2: -0.0200
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6406144
                    Iteration time: 5.75s
                      Time elapsed: 00:30:47
                               ETA: 00:08:39

################################################################################
                      [1m Learning iteration 391/500 [0m                      

                       Computation: 2867 steps/s (collection: 5.627s, learning 0.086s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0052
               Mean surrogate loss: -0.0035
                 Mean entropy loss: -13.0608
                       Mean reward: 240.98
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.2713
Episode_Reward/track_ang_vel_z_exp: 0.1584
       Episode_Reward/lin_vel_z_l2: -0.0403
      Episode_Reward/ang_vel_xy_l2: -0.0665
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3257
     Episode_Reward/action_rate_l2: -0.0046
Episode_Reward/flat_orientation_l2: -0.0186
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6422528
                    Iteration time: 5.71s
                      Time elapsed: 00:30:53
                               ETA: 00:08:35

################################################################################
                      [1m Learning iteration 392/500 [0m                      

                       Computation: 2817 steps/s (collection: 5.720s, learning 0.095s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0053
               Mean surrogate loss: -0.0036
                 Mean entropy loss: -13.1474
                       Mean reward: 240.98
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.2713
Episode_Reward/track_ang_vel_z_exp: 0.1584
       Episode_Reward/lin_vel_z_l2: -0.0403
      Episode_Reward/ang_vel_xy_l2: -0.0665
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3257
     Episode_Reward/action_rate_l2: -0.0046
Episode_Reward/flat_orientation_l2: -0.0186
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6438912
                    Iteration time: 5.81s
                      Time elapsed: 00:30:58
                               ETA: 00:08:30

################################################################################
                      [1m Learning iteration 393/500 [0m                      

                       Computation: 2814 steps/s (collection: 5.736s, learning 0.086s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0051
               Mean surrogate loss: -0.0042
                 Mean entropy loss: -13.1996
                       Mean reward: 240.98
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.2713
Episode_Reward/track_ang_vel_z_exp: 0.1584
       Episode_Reward/lin_vel_z_l2: -0.0403
      Episode_Reward/ang_vel_xy_l2: -0.0665
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3257
     Episode_Reward/action_rate_l2: -0.0046
Episode_Reward/flat_orientation_l2: -0.0186
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6455296
                    Iteration time: 5.82s
                      Time elapsed: 00:31:04
                               ETA: 00:08:26

################################################################################
                      [1m Learning iteration 394/500 [0m                      

                       Computation: 2835 steps/s (collection: 5.694s, learning 0.084s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0054
               Mean surrogate loss: -0.0029
                 Mean entropy loss: -13.2202
                       Mean reward: 240.98
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.2713
Episode_Reward/track_ang_vel_z_exp: 0.1584
       Episode_Reward/lin_vel_z_l2: -0.0403
      Episode_Reward/ang_vel_xy_l2: -0.0665
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3257
     Episode_Reward/action_rate_l2: -0.0046
Episode_Reward/flat_orientation_l2: -0.0186
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6471680
                    Iteration time: 5.78s
                      Time elapsed: 00:31:10
                               ETA: 00:08:21

################################################################################
                      [1m Learning iteration 395/500 [0m                      

                       Computation: 2852 steps/s (collection: 5.650s, learning 0.093s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0055
               Mean surrogate loss: -0.0034
                 Mean entropy loss: -13.2962
                       Mean reward: 240.98
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.2713
Episode_Reward/track_ang_vel_z_exp: 0.1584
       Episode_Reward/lin_vel_z_l2: -0.0403
      Episode_Reward/ang_vel_xy_l2: -0.0665
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3257
     Episode_Reward/action_rate_l2: -0.0046
Episode_Reward/flat_orientation_l2: -0.0186
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 5.74s
                      Time elapsed: 00:31:16
                               ETA: 00:08:17

################################################################################
                      [1m Learning iteration 396/500 [0m                      

                       Computation: 2809 steps/s (collection: 5.747s, learning 0.084s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0051
               Mean surrogate loss: -0.0062
                 Mean entropy loss: -13.3326
                       Mean reward: 241.01
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.2762
Episode_Reward/track_ang_vel_z_exp: 0.1588
       Episode_Reward/lin_vel_z_l2: -0.0394
      Episode_Reward/ang_vel_xy_l2: -0.0661
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3374
     Episode_Reward/action_rate_l2: -0.0046
Episode_Reward/flat_orientation_l2: -0.0184
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6504448
                    Iteration time: 5.83s
                      Time elapsed: 00:31:22
                               ETA: 00:08:13

################################################################################
                      [1m Learning iteration 397/500 [0m                      

                       Computation: 2831 steps/s (collection: 5.705s, learning 0.083s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0051
               Mean surrogate loss: -0.0044
                 Mean entropy loss: -13.3313
                       Mean reward: 240.95
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.3244
Episode_Reward/track_ang_vel_z_exp: 0.1548
       Episode_Reward/lin_vel_z_l2: -0.0351
      Episode_Reward/ang_vel_xy_l2: -0.0659
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3981
     Episode_Reward/action_rate_l2: -0.0043
Episode_Reward/flat_orientation_l2: -0.0174
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6520832
                    Iteration time: 5.79s
                      Time elapsed: 00:31:27
                               ETA: 00:08:08

################################################################################
                      [1m Learning iteration 398/500 [0m                      

                       Computation: 2860 steps/s (collection: 5.642s, learning 0.087s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0049
               Mean surrogate loss: -0.0043
                 Mean entropy loss: -13.3744
                       Mean reward: 240.96
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.3864
Episode_Reward/track_ang_vel_z_exp: 0.1582
       Episode_Reward/lin_vel_z_l2: -0.0346
      Episode_Reward/ang_vel_xy_l2: -0.0635
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3889
     Episode_Reward/action_rate_l2: -0.0042
Episode_Reward/flat_orientation_l2: -0.0173
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6537216
                    Iteration time: 5.73s
                      Time elapsed: 00:31:33
                               ETA: 00:08:04

################################################################################
                      [1m Learning iteration 399/500 [0m                      

                       Computation: 2865 steps/s (collection: 5.620s, learning 0.098s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0048
               Mean surrogate loss: -0.0033
                 Mean entropy loss: -13.3780
                       Mean reward: 240.96
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4105
Episode_Reward/track_ang_vel_z_exp: 0.1609
       Episode_Reward/lin_vel_z_l2: -0.0336
      Episode_Reward/ang_vel_xy_l2: -0.0621
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3945
     Episode_Reward/action_rate_l2: -0.0041
Episode_Reward/flat_orientation_l2: -0.0171
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6553600
                    Iteration time: 5.72s
                      Time elapsed: 00:31:39
                               ETA: 00:07:59

################################################################################
                      [1m Learning iteration 400/500 [0m                      

                       Computation: 2827 steps/s (collection: 5.685s, learning 0.110s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0047
               Mean surrogate loss: -0.0034
                 Mean entropy loss: -13.4428
                       Mean reward: 240.96
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4105
Episode_Reward/track_ang_vel_z_exp: 0.1609
       Episode_Reward/lin_vel_z_l2: -0.0336
      Episode_Reward/ang_vel_xy_l2: -0.0621
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3945
     Episode_Reward/action_rate_l2: -0.0041
Episode_Reward/flat_orientation_l2: -0.0171
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6569984
                    Iteration time: 5.80s
                      Time elapsed: 00:31:45
                               ETA: 00:07:55

################################################################################
                      [1m Learning iteration 401/500 [0m                      

                       Computation: 2827 steps/s (collection: 5.687s, learning 0.108s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0050
               Mean surrogate loss: -0.0074
                 Mean entropy loss: -13.5208
                       Mean reward: 240.96
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4105
Episode_Reward/track_ang_vel_z_exp: 0.1609
       Episode_Reward/lin_vel_z_l2: -0.0336
      Episode_Reward/ang_vel_xy_l2: -0.0621
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3945
     Episode_Reward/action_rate_l2: -0.0041
Episode_Reward/flat_orientation_l2: -0.0171
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 5.80s
                      Time elapsed: 00:31:50
                               ETA: 00:07:50

################################################################################
                      [1m Learning iteration 402/500 [0m                      

                       Computation: 2860 steps/s (collection: 5.635s, learning 0.092s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0052
               Mean surrogate loss: -0.0033
                 Mean entropy loss: -13.5861
                       Mean reward: 241.11
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4038
Episode_Reward/track_ang_vel_z_exp: 0.1597
       Episode_Reward/lin_vel_z_l2: -0.0349
      Episode_Reward/ang_vel_xy_l2: -0.0630
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3864
     Episode_Reward/action_rate_l2: -0.0041
Episode_Reward/flat_orientation_l2: -0.0176
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6602752
                    Iteration time: 5.73s
                      Time elapsed: 00:31:56
                               ETA: 00:07:46

################################################################################
                      [1m Learning iteration 403/500 [0m                      

                       Computation: 2846 steps/s (collection: 5.670s, learning 0.085s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0057
               Mean surrogate loss: -0.0063
                 Mean entropy loss: -13.6304
                       Mean reward: 242.18
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.3982
Episode_Reward/track_ang_vel_z_exp: 0.1562
       Episode_Reward/lin_vel_z_l2: -0.0400
      Episode_Reward/ang_vel_xy_l2: -0.0668
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3723
     Episode_Reward/action_rate_l2: -0.0042
Episode_Reward/flat_orientation_l2: -0.0191
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6619136
                    Iteration time: 5.76s
                      Time elapsed: 00:32:02
                               ETA: 00:07:41

################################################################################
                      [1m Learning iteration 404/500 [0m                      

                       Computation: 2811 steps/s (collection: 5.743s, learning 0.085s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0053
               Mean surrogate loss: -0.0029
                 Mean entropy loss: -13.6677
                       Mean reward: 243.57
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4791
Episode_Reward/track_ang_vel_z_exp: 0.1641
       Episode_Reward/lin_vel_z_l2: -0.0349
      Episode_Reward/ang_vel_xy_l2: -0.0621
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3647
     Episode_Reward/action_rate_l2: -0.0041
Episode_Reward/flat_orientation_l2: -0.0182
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0312
--------------------------------------------------------------------------------
                   Total timesteps: 6635520
                    Iteration time: 5.83s
                      Time elapsed: 00:32:08
                               ETA: 00:07:37

################################################################################
                      [1m Learning iteration 405/500 [0m                      

                       Computation: 2814 steps/s (collection: 5.736s, learning 0.085s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0057
               Mean surrogate loss: -0.0049
                 Mean entropy loss: -13.7326
                       Mean reward: 245.07
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4507
Episode_Reward/track_ang_vel_z_exp: 0.1621
       Episode_Reward/lin_vel_z_l2: -0.0367
      Episode_Reward/ang_vel_xy_l2: -0.0624
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3604
     Episode_Reward/action_rate_l2: -0.0041
Episode_Reward/flat_orientation_l2: -0.0185
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 6651904
                    Iteration time: 5.82s
                      Time elapsed: 00:32:13
                               ETA: 00:07:32

################################################################################
                      [1m Learning iteration 406/500 [0m                      

                       Computation: 2796 steps/s (collection: 5.773s, learning 0.085s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0072
               Mean surrogate loss: -0.0040
                 Mean entropy loss: -13.7867
                       Mean reward: 248.04
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4226
Episode_Reward/track_ang_vel_z_exp: 0.1624
       Episode_Reward/lin_vel_z_l2: -0.0352
      Episode_Reward/ang_vel_xy_l2: -0.0633
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3725
     Episode_Reward/action_rate_l2: -0.0041
Episode_Reward/flat_orientation_l2: -0.0180
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.9688
--------------------------------------------------------------------------------
                   Total timesteps: 6668288
                    Iteration time: 5.86s
                      Time elapsed: 00:32:19
                               ETA: 00:07:28

################################################################################
                      [1m Learning iteration 407/500 [0m                      

                       Computation: 2757 steps/s (collection: 5.854s, learning 0.088s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0073
               Mean surrogate loss: -0.0023
                 Mean entropy loss: -13.8278
                       Mean reward: 248.21
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4267
Episode_Reward/track_ang_vel_z_exp: 0.1615
       Episode_Reward/lin_vel_z_l2: -0.0354
      Episode_Reward/ang_vel_xy_l2: -0.0630
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3638
     Episode_Reward/action_rate_l2: -0.0041
Episode_Reward/flat_orientation_l2: -0.0183
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 5.94s
                      Time elapsed: 00:32:25
                               ETA: 00:07:23

################################################################################
                      [1m Learning iteration 408/500 [0m                      

                       Computation: 2757 steps/s (collection: 5.859s, learning 0.084s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0077
               Mean surrogate loss: -0.0042
                 Mean entropy loss: -13.8764
                       Mean reward: 248.73
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4521
Episode_Reward/track_ang_vel_z_exp: 0.1642
       Episode_Reward/lin_vel_z_l2: -0.0342
      Episode_Reward/ang_vel_xy_l2: -0.0625
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3692
     Episode_Reward/action_rate_l2: -0.0040
Episode_Reward/flat_orientation_l2: -0.0180
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.9062
--------------------------------------------------------------------------------
                   Total timesteps: 6701056
                    Iteration time: 5.94s
                      Time elapsed: 00:32:31
                               ETA: 00:07:19

################################################################################
                      [1m Learning iteration 409/500 [0m                      

                       Computation: 2758 steps/s (collection: 5.855s, learning 0.085s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0068
               Mean surrogate loss: -0.0042
                 Mean entropy loss: -13.9350
                       Mean reward: 248.85
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4392
Episode_Reward/track_ang_vel_z_exp: 0.1657
       Episode_Reward/lin_vel_z_l2: -0.0339
      Episode_Reward/ang_vel_xy_l2: -0.0616
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3750
     Episode_Reward/action_rate_l2: -0.0040
Episode_Reward/flat_orientation_l2: -0.0179
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 2.3438
--------------------------------------------------------------------------------
                   Total timesteps: 6717440
                    Iteration time: 5.94s
                      Time elapsed: 00:32:37
                               ETA: 00:07:14

################################################################################
                      [1m Learning iteration 410/500 [0m                      

                       Computation: 2780 steps/s (collection: 5.806s, learning 0.086s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0058
               Mean surrogate loss: -0.0043
                 Mean entropy loss: -13.9892
                       Mean reward: 248.17
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.3923
Episode_Reward/track_ang_vel_z_exp: 0.1626
       Episode_Reward/lin_vel_z_l2: -0.0357
      Episode_Reward/ang_vel_xy_l2: -0.0641
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3707
     Episode_Reward/action_rate_l2: -0.0040
Episode_Reward/flat_orientation_l2: -0.0181
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 6733824
                    Iteration time: 5.89s
                      Time elapsed: 00:32:43
                               ETA: 00:07:09

################################################################################
                      [1m Learning iteration 411/500 [0m                      

                       Computation: 2771 steps/s (collection: 5.828s, learning 0.083s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0055
               Mean surrogate loss: -0.0058
                 Mean entropy loss: -14.0434
                       Mean reward: 248.05
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4770
Episode_Reward/track_ang_vel_z_exp: 0.1649
       Episode_Reward/lin_vel_z_l2: -0.0358
      Episode_Reward/ang_vel_xy_l2: -0.0608
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3601
     Episode_Reward/action_rate_l2: -0.0040
Episode_Reward/flat_orientation_l2: -0.0184
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.4688
--------------------------------------------------------------------------------
                   Total timesteps: 6750208
                    Iteration time: 5.91s
                      Time elapsed: 00:32:49
                               ETA: 00:07:05

################################################################################
                      [1m Learning iteration 412/500 [0m                      

                       Computation: 2757 steps/s (collection: 5.859s, learning 0.082s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0054
               Mean surrogate loss: -0.0037
                 Mean entropy loss: -14.0999
                       Mean reward: 247.80
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4520
Episode_Reward/track_ang_vel_z_exp: 0.1654
       Episode_Reward/lin_vel_z_l2: -0.0336
      Episode_Reward/ang_vel_xy_l2: -0.0618
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3785
     Episode_Reward/action_rate_l2: -0.0040
Episode_Reward/flat_orientation_l2: -0.0178
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.4688
--------------------------------------------------------------------------------
                   Total timesteps: 6766592
                    Iteration time: 5.94s
                      Time elapsed: 00:32:55
                               ETA: 00:07:00

################################################################################
                      [1m Learning iteration 413/500 [0m                      

                       Computation: 2765 steps/s (collection: 5.841s, learning 0.084s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0048
               Mean surrogate loss: -0.0030
                 Mean entropy loss: -14.1641
                       Mean reward: 248.72
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4866
Episode_Reward/track_ang_vel_z_exp: 0.1672
       Episode_Reward/lin_vel_z_l2: -0.0342
      Episode_Reward/ang_vel_xy_l2: -0.0621
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3750
     Episode_Reward/action_rate_l2: -0.0040
Episode_Reward/flat_orientation_l2: -0.0181
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.4688
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 5.92s
                      Time elapsed: 00:33:01
                               ETA: 00:06:56

################################################################################
                      [1m Learning iteration 414/500 [0m                      

                       Computation: 2789 steps/s (collection: 5.779s, learning 0.095s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0049
               Mean surrogate loss: -0.0036
                 Mean entropy loss: -14.2033
                       Mean reward: 249.11
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4509
Episode_Reward/track_ang_vel_z_exp: 0.1656
       Episode_Reward/lin_vel_z_l2: -0.0345
      Episode_Reward/ang_vel_xy_l2: -0.0620
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3698
     Episode_Reward/action_rate_l2: -0.0039
Episode_Reward/flat_orientation_l2: -0.0182
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.2812
--------------------------------------------------------------------------------
                   Total timesteps: 6799360
                    Iteration time: 5.87s
                      Time elapsed: 00:33:07
                               ETA: 00:06:51

################################################################################
                      [1m Learning iteration 415/500 [0m                      

                       Computation: 2788 steps/s (collection: 5.790s, learning 0.086s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0047
               Mean surrogate loss: -0.0044
                 Mean entropy loss: -14.2630
                       Mean reward: 249.18
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4527
Episode_Reward/track_ang_vel_z_exp: 0.1630
       Episode_Reward/lin_vel_z_l2: -0.0352
      Episode_Reward/ang_vel_xy_l2: -0.0630
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3601
     Episode_Reward/action_rate_l2: -0.0040
Episode_Reward/flat_orientation_l2: -0.0180
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 6815744
                    Iteration time: 5.88s
                      Time elapsed: 00:33:13
                               ETA: 00:06:47

################################################################################
                      [1m Learning iteration 416/500 [0m                      

                       Computation: 2802 steps/s (collection: 5.751s, learning 0.097s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0059
               Mean surrogate loss: -0.0046
                 Mean entropy loss: -14.3412
                       Mean reward: 249.36
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4861
Episode_Reward/track_ang_vel_z_exp: 0.1685
       Episode_Reward/lin_vel_z_l2: -0.0350
      Episode_Reward/ang_vel_xy_l2: -0.0601
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3573
     Episode_Reward/action_rate_l2: -0.0039
Episode_Reward/flat_orientation_l2: -0.0182
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 6832128
                    Iteration time: 5.85s
                      Time elapsed: 00:33:18
                               ETA: 00:06:42

################################################################################
                      [1m Learning iteration 417/500 [0m                      

                       Computation: 2797 steps/s (collection: 5.770s, learning 0.088s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0047
               Mean surrogate loss: -0.0058
                 Mean entropy loss: -14.3754
                       Mean reward: 249.65
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4641
Episode_Reward/track_ang_vel_z_exp: 0.1704
       Episode_Reward/lin_vel_z_l2: -0.0332
      Episode_Reward/ang_vel_xy_l2: -0.0610
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3850
     Episode_Reward/action_rate_l2: -0.0039
Episode_Reward/flat_orientation_l2: -0.0176
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0625
--------------------------------------------------------------------------------
                   Total timesteps: 6848512
                    Iteration time: 5.86s
                      Time elapsed: 00:33:24
                               ETA: 00:06:38

################################################################################
                      [1m Learning iteration 418/500 [0m                      

                       Computation: 2826 steps/s (collection: 5.715s, learning 0.082s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0045
               Mean surrogate loss: -0.0042
                 Mean entropy loss: -14.4056
                       Mean reward: 249.63
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5189
Episode_Reward/track_ang_vel_z_exp: 0.1720
       Episode_Reward/lin_vel_z_l2: -0.0283
      Episode_Reward/ang_vel_xy_l2: -0.0586
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3825
     Episode_Reward/action_rate_l2: -0.0038
Episode_Reward/flat_orientation_l2: -0.0168
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6864896
                    Iteration time: 5.80s
                      Time elapsed: 00:33:30
                               ETA: 00:06:33

################################################################################
                      [1m Learning iteration 419/500 [0m                      

                       Computation: 2820 steps/s (collection: 5.721s, learning 0.086s)
             Mean action noise std: 0.09
          Mean value_function loss: 0.0045
               Mean surrogate loss: -0.0048
                 Mean entropy loss: -14.4752
                       Mean reward: 249.64
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4802
Episode_Reward/track_ang_vel_z_exp: 0.1697
       Episode_Reward/lin_vel_z_l2: -0.0295
      Episode_Reward/ang_vel_xy_l2: -0.0600
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3645
     Episode_Reward/action_rate_l2: -0.0037
Episode_Reward/flat_orientation_l2: -0.0170
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0312
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 5.81s
                      Time elapsed: 00:33:36
                               ETA: 00:06:28

################################################################################
                      [1m Learning iteration 420/500 [0m                      

                       Computation: 2795 steps/s (collection: 5.776s, learning 0.085s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0043
               Mean surrogate loss: -0.0018
                 Mean entropy loss: -14.4873
                       Mean reward: 249.81
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5512
Episode_Reward/track_ang_vel_z_exp: 0.1683
       Episode_Reward/lin_vel_z_l2: -0.0320
      Episode_Reward/ang_vel_xy_l2: -0.0595
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3401
     Episode_Reward/action_rate_l2: -0.0037
Episode_Reward/flat_orientation_l2: -0.0174
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0938
--------------------------------------------------------------------------------
                   Total timesteps: 6897664
                    Iteration time: 5.86s
                      Time elapsed: 00:33:42
                               ETA: 00:06:24

################################################################################
                      [1m Learning iteration 421/500 [0m                      

                       Computation: 2790 steps/s (collection: 5.785s, learning 0.085s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0046
               Mean surrogate loss: -0.0041
                 Mean entropy loss: -14.5472
                       Mean reward: 249.82
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5106
Episode_Reward/track_ang_vel_z_exp: 0.1704
       Episode_Reward/lin_vel_z_l2: -0.0309
      Episode_Reward/ang_vel_xy_l2: -0.0624
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.4025
     Episode_Reward/action_rate_l2: -0.0038
Episode_Reward/flat_orientation_l2: -0.0168
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6914048
                    Iteration time: 5.87s
                      Time elapsed: 00:33:48
                               ETA: 00:06:19

################################################################################
                      [1m Learning iteration 422/500 [0m                      

                       Computation: 2792 steps/s (collection: 5.760s, learning 0.106s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0044
               Mean surrogate loss: -0.0051
                 Mean entropy loss: -14.5572
                       Mean reward: 250.02
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5558
Episode_Reward/track_ang_vel_z_exp: 0.1731
       Episode_Reward/lin_vel_z_l2: -0.0336
      Episode_Reward/ang_vel_xy_l2: -0.0563
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3591
     Episode_Reward/action_rate_l2: -0.0037
Episode_Reward/flat_orientation_l2: -0.0182
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6930432
                    Iteration time: 5.87s
                      Time elapsed: 00:33:53
                               ETA: 00:06:15

################################################################################
                      [1m Learning iteration 423/500 [0m                      

                       Computation: 2829 steps/s (collection: 5.706s, learning 0.085s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0044
               Mean surrogate loss: -0.0019
                 Mean entropy loss: -14.6061
                       Mean reward: 250.05
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5244
Episode_Reward/track_ang_vel_z_exp: 0.1736
       Episode_Reward/lin_vel_z_l2: -0.0322
      Episode_Reward/ang_vel_xy_l2: -0.0563
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3670
     Episode_Reward/action_rate_l2: -0.0037
Episode_Reward/flat_orientation_l2: -0.0178
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6946816
                    Iteration time: 5.79s
                      Time elapsed: 00:33:59
                               ETA: 00:06:10

################################################################################
                      [1m Learning iteration 424/500 [0m                      

                       Computation: 2806 steps/s (collection: 5.751s, learning 0.087s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0043
               Mean surrogate loss: -0.0034
                 Mean entropy loss: -14.6692
                       Mean reward: 249.97
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5094
Episode_Reward/track_ang_vel_z_exp: 0.1589
       Episode_Reward/lin_vel_z_l2: -0.0455
      Episode_Reward/ang_vel_xy_l2: -0.0621
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3250
     Episode_Reward/action_rate_l2: -0.0039
Episode_Reward/flat_orientation_l2: -0.0212
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6963200
                    Iteration time: 5.84s
                      Time elapsed: 00:34:05
                               ETA: 00:06:05

################################################################################
                      [1m Learning iteration 425/500 [0m                      

                       Computation: 2802 steps/s (collection: 5.763s, learning 0.084s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0043
               Mean surrogate loss: -0.0044
                 Mean entropy loss: -14.7292
                       Mean reward: 249.97
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.3793
Episode_Reward/track_ang_vel_z_exp: 0.1575
       Episode_Reward/lin_vel_z_l2: -0.0464
      Episode_Reward/ang_vel_xy_l2: -0.0668
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3532
     Episode_Reward/action_rate_l2: -0.0041
Episode_Reward/flat_orientation_l2: -0.0203
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 5.85s
                      Time elapsed: 00:34:11
                               ETA: 00:06:01

################################################################################
                      [1m Learning iteration 426/500 [0m                      

                       Computation: 2828 steps/s (collection: 5.708s, learning 0.085s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0044
               Mean surrogate loss: -0.0046
                 Mean entropy loss: -14.7953
                       Mean reward: 250.02
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4677
Episode_Reward/track_ang_vel_z_exp: 0.1667
       Episode_Reward/lin_vel_z_l2: -0.0413
      Episode_Reward/ang_vel_xy_l2: -0.0620
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3511
     Episode_Reward/action_rate_l2: -0.0040
Episode_Reward/flat_orientation_l2: -0.0196
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6995968
                    Iteration time: 5.79s
                      Time elapsed: 00:34:17
                               ETA: 00:05:56

################################################################################
                      [1m Learning iteration 427/500 [0m                      

                       Computation: 2841 steps/s (collection: 5.682s, learning 0.084s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0042
               Mean surrogate loss: -0.0032
                 Mean entropy loss: -14.7941
                       Mean reward: 250.16
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5351
Episode_Reward/track_ang_vel_z_exp: 0.1743
       Episode_Reward/lin_vel_z_l2: -0.0364
      Episode_Reward/ang_vel_xy_l2: -0.0586
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3394
     Episode_Reward/action_rate_l2: -0.0038
Episode_Reward/flat_orientation_l2: -0.0189
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7012352
                    Iteration time: 5.77s
                      Time elapsed: 00:34:23
                               ETA: 00:05:51

################################################################################
                      [1m Learning iteration 428/500 [0m                      

                       Computation: 2806 steps/s (collection: 5.754s, learning 0.085s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0043
               Mean surrogate loss: -0.0030
                 Mean entropy loss: -14.8355
                       Mean reward: 250.20
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5801
Episode_Reward/track_ang_vel_z_exp: 0.1792
       Episode_Reward/lin_vel_z_l2: -0.0301
      Episode_Reward/ang_vel_xy_l2: -0.0552
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3834
     Episode_Reward/action_rate_l2: -0.0037
Episode_Reward/flat_orientation_l2: -0.0178
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7028736
                    Iteration time: 5.84s
                      Time elapsed: 00:34:28
                               ETA: 00:05:47

################################################################################
                      [1m Learning iteration 429/500 [0m                      

                       Computation: 2796 steps/s (collection: 5.775s, learning 0.084s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0042
               Mean surrogate loss: -0.0039
                 Mean entropy loss: -14.9078
                       Mean reward: 250.13
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5282
Episode_Reward/track_ang_vel_z_exp: 0.1746
       Episode_Reward/lin_vel_z_l2: -0.0275
      Episode_Reward/ang_vel_xy_l2: -0.0601
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3856
     Episode_Reward/action_rate_l2: -0.0037
Episode_Reward/flat_orientation_l2: -0.0163
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7045120
                    Iteration time: 5.86s
                      Time elapsed: 00:34:34
                               ETA: 00:05:42

################################################################################
                      [1m Learning iteration 430/500 [0m                      

                       Computation: 2835 steps/s (collection: 5.691s, learning 0.087s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0046
               Mean surrogate loss: -0.0029
                 Mean entropy loss: -14.9349
                       Mean reward: 250.29
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5441
Episode_Reward/track_ang_vel_z_exp: 0.1756
       Episode_Reward/lin_vel_z_l2: -0.0274
      Episode_Reward/ang_vel_xy_l2: -0.0577
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3873
     Episode_Reward/action_rate_l2: -0.0036
Episode_Reward/flat_orientation_l2: -0.0165
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7061504
                    Iteration time: 5.78s
                      Time elapsed: 00:34:40
                               ETA: 00:05:37

################################################################################
                      [1m Learning iteration 431/500 [0m                      

                       Computation: 2825 steps/s (collection: 5.687s, learning 0.111s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0047
               Mean surrogate loss: -0.0052
                 Mean entropy loss: -14.9967
                       Mean reward: 250.29
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5843
Episode_Reward/track_ang_vel_z_exp: 0.1795
       Episode_Reward/lin_vel_z_l2: -0.0269
      Episode_Reward/ang_vel_xy_l2: -0.0528
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3932
     Episode_Reward/action_rate_l2: -0.0035
Episode_Reward/flat_orientation_l2: -0.0170
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 5.80s
                      Time elapsed: 00:34:46
                               ETA: 00:05:33

################################################################################
                      [1m Learning iteration 432/500 [0m                      

                       Computation: 2813 steps/s (collection: 5.739s, learning 0.085s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0046
               Mean surrogate loss: -0.0030
                 Mean entropy loss: -14.9991
                       Mean reward: 250.29
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5843
Episode_Reward/track_ang_vel_z_exp: 0.1795
       Episode_Reward/lin_vel_z_l2: -0.0269
      Episode_Reward/ang_vel_xy_l2: -0.0528
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3932
     Episode_Reward/action_rate_l2: -0.0035
Episode_Reward/flat_orientation_l2: -0.0170
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7094272
                    Iteration time: 5.82s
                      Time elapsed: 00:34:52
                               ETA: 00:05:28

################################################################################
                      [1m Learning iteration 433/500 [0m                      

                       Computation: 2766 steps/s (collection: 5.838s, learning 0.085s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0044
               Mean surrogate loss: -0.0013
                 Mean entropy loss: -15.0149
                       Mean reward: 250.29
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5843
Episode_Reward/track_ang_vel_z_exp: 0.1795
       Episode_Reward/lin_vel_z_l2: -0.0269
      Episode_Reward/ang_vel_xy_l2: -0.0528
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3932
     Episode_Reward/action_rate_l2: -0.0035
Episode_Reward/flat_orientation_l2: -0.0170
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7110656
                    Iteration time: 5.92s
                      Time elapsed: 00:34:58
                               ETA: 00:05:23

################################################################################
                      [1m Learning iteration 434/500 [0m                      

                       Computation: 2815 steps/s (collection: 5.734s, learning 0.086s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0041
               Mean surrogate loss: -0.0038
                 Mean entropy loss: -15.0712
                       Mean reward: 250.29
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5843
Episode_Reward/track_ang_vel_z_exp: 0.1795
       Episode_Reward/lin_vel_z_l2: -0.0269
      Episode_Reward/ang_vel_xy_l2: -0.0528
     Episode_Reward/dof_torques_l2: -0.0011
         Episode_Reward/dof_acc_l2: -0.3932
     Episode_Reward/action_rate_l2: -0.0035
Episode_Reward/flat_orientation_l2: -0.0170
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7127040
                    Iteration time: 5.82s
                      Time elapsed: 00:35:03
                               ETA: 00:05:19

################################################################################
                      [1m Learning iteration 435/500 [0m                      

                       Computation: 2837 steps/s (collection: 5.689s, learning 0.085s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0039
               Mean surrogate loss: -0.0037
                 Mean entropy loss: -15.1135
                       Mean reward: 250.43
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5675
Episode_Reward/track_ang_vel_z_exp: 0.1819
       Episode_Reward/lin_vel_z_l2: -0.0309
      Episode_Reward/ang_vel_xy_l2: -0.0538
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3812
     Episode_Reward/action_rate_l2: -0.0035
Episode_Reward/flat_orientation_l2: -0.0183
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7143424
                    Iteration time: 5.77s
                      Time elapsed: 00:35:09
                               ETA: 00:05:14

################################################################################
                      [1m Learning iteration 436/500 [0m                      

                       Computation: 2800 steps/s (collection: 5.758s, learning 0.092s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0025
                 Mean entropy loss: -15.1256
                       Mean reward: 250.43
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5507
Episode_Reward/track_ang_vel_z_exp: 0.1843
       Episode_Reward/lin_vel_z_l2: -0.0348
      Episode_Reward/ang_vel_xy_l2: -0.0549
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3692
     Episode_Reward/action_rate_l2: -0.0036
Episode_Reward/flat_orientation_l2: -0.0196
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7159808
                    Iteration time: 5.85s
                      Time elapsed: 00:35:15
                               ETA: 00:05:09

################################################################################
                      [1m Learning iteration 437/500 [0m                      

                       Computation: 2800 steps/s (collection: 5.761s, learning 0.089s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0044
               Mean surrogate loss: -0.0042
                 Mean entropy loss: -15.1751
                       Mean reward: 250.43
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5507
Episode_Reward/track_ang_vel_z_exp: 0.1843
       Episode_Reward/lin_vel_z_l2: -0.0348
      Episode_Reward/ang_vel_xy_l2: -0.0549
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3692
     Episode_Reward/action_rate_l2: -0.0036
Episode_Reward/flat_orientation_l2: -0.0196
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 5.85s
                      Time elapsed: 00:35:21
                               ETA: 00:05:05

################################################################################
                      [1m Learning iteration 438/500 [0m                      

                       Computation: 2820 steps/s (collection: 5.723s, learning 0.087s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0041
               Mean surrogate loss: -0.0040
                 Mean entropy loss: -15.1971
                       Mean reward: 250.43
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5507
Episode_Reward/track_ang_vel_z_exp: 0.1843
       Episode_Reward/lin_vel_z_l2: -0.0348
      Episode_Reward/ang_vel_xy_l2: -0.0549
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3692
     Episode_Reward/action_rate_l2: -0.0036
Episode_Reward/flat_orientation_l2: -0.0196
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7192576
                    Iteration time: 5.81s
                      Time elapsed: 00:35:27
                               ETA: 00:05:00

################################################################################
                      [1m Learning iteration 439/500 [0m                      

                       Computation: 2828 steps/s (collection: 5.705s, learning 0.088s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0043
               Mean surrogate loss: -0.0037
                 Mean entropy loss: -15.2301
                       Mean reward: 250.43
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5507
Episode_Reward/track_ang_vel_z_exp: 0.1843
       Episode_Reward/lin_vel_z_l2: -0.0348
      Episode_Reward/ang_vel_xy_l2: -0.0549
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3692
     Episode_Reward/action_rate_l2: -0.0036
Episode_Reward/flat_orientation_l2: -0.0196
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7208960
                    Iteration time: 5.79s
                      Time elapsed: 00:35:32
                               ETA: 00:04:55

################################################################################
                      [1m Learning iteration 440/500 [0m                      

                       Computation: 2809 steps/s (collection: 5.746s, learning 0.085s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0042
               Mean surrogate loss: -0.0038
                 Mean entropy loss: -15.2686
                       Mean reward: 250.43
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5507
Episode_Reward/track_ang_vel_z_exp: 0.1843
       Episode_Reward/lin_vel_z_l2: -0.0348
      Episode_Reward/ang_vel_xy_l2: -0.0549
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3692
     Episode_Reward/action_rate_l2: -0.0036
Episode_Reward/flat_orientation_l2: -0.0196
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7225344
                    Iteration time: 5.83s
                      Time elapsed: 00:35:38
                               ETA: 00:04:50

################################################################################
                      [1m Learning iteration 441/500 [0m                      

                       Computation: 2809 steps/s (collection: 5.748s, learning 0.083s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0044
               Mean surrogate loss: -0.0052
                 Mean entropy loss: -15.3453
                       Mean reward: 250.31
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4901
Episode_Reward/track_ang_vel_z_exp: 0.1793
       Episode_Reward/lin_vel_z_l2: -0.0362
      Episode_Reward/ang_vel_xy_l2: -0.0580
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3600
     Episode_Reward/action_rate_l2: -0.0036
Episode_Reward/flat_orientation_l2: -0.0194
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7241728
                    Iteration time: 5.83s
                      Time elapsed: 00:35:44
                               ETA: 00:04:46

################################################################################
                      [1m Learning iteration 442/500 [0m                      

                       Computation: 2832 steps/s (collection: 5.676s, learning 0.108s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0045
               Mean surrogate loss: -0.0017
                 Mean entropy loss: -15.3417
                       Mean reward: 250.29
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.3559
Episode_Reward/track_ang_vel_z_exp: 0.1689
       Episode_Reward/lin_vel_z_l2: -0.0307
      Episode_Reward/ang_vel_xy_l2: -0.0628
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3701
     Episode_Reward/action_rate_l2: -0.0036
Episode_Reward/flat_orientation_l2: -0.0176
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7258112
                    Iteration time: 5.78s
                      Time elapsed: 00:35:50
                               ETA: 00:04:41

################################################################################
                      [1m Learning iteration 443/500 [0m                      

                       Computation: 2842 steps/s (collection: 5.677s, learning 0.086s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0063
                 Mean entropy loss: -15.3949
                       Mean reward: 250.31
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4799
Episode_Reward/track_ang_vel_z_exp: 0.1720
       Episode_Reward/lin_vel_z_l2: -0.0327
      Episode_Reward/ang_vel_xy_l2: -0.0613
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3589
     Episode_Reward/action_rate_l2: -0.0036
Episode_Reward/flat_orientation_l2: -0.0177
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 5.76s
                      Time elapsed: 00:35:56
                               ETA: 00:04:36

################################################################################
                      [1m Learning iteration 444/500 [0m                      

                       Computation: 2807 steps/s (collection: 5.750s, learning 0.085s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0042
               Mean surrogate loss: -0.0048
                 Mean entropy loss: -15.4033
                       Mean reward: 250.30
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5881
Episode_Reward/track_ang_vel_z_exp: 0.1793
       Episode_Reward/lin_vel_z_l2: -0.0370
      Episode_Reward/ang_vel_xy_l2: -0.0585
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3365
     Episode_Reward/action_rate_l2: -0.0036
Episode_Reward/flat_orientation_l2: -0.0187
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7290880
                    Iteration time: 5.84s
                      Time elapsed: 00:36:01
                               ETA: 00:04:32

################################################################################
                      [1m Learning iteration 445/500 [0m                      

                       Computation: 2814 steps/s (collection: 5.737s, learning 0.084s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0041
               Mean surrogate loss: -0.0027
                 Mean entropy loss: -15.4623
                       Mean reward: 250.30
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5799
Episode_Reward/track_ang_vel_z_exp: 0.1884
       Episode_Reward/lin_vel_z_l2: -0.0348
      Episode_Reward/ang_vel_xy_l2: -0.0546
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3382
     Episode_Reward/action_rate_l2: -0.0035
Episode_Reward/flat_orientation_l2: -0.0191
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7307264
                    Iteration time: 5.82s
                      Time elapsed: 00:36:07
                               ETA: 00:04:27

################################################################################
                      [1m Learning iteration 446/500 [0m                      

                       Computation: 2836 steps/s (collection: 5.690s, learning 0.086s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0044
               Mean surrogate loss: -0.0043
                 Mean entropy loss: -15.5030
                       Mean reward: 250.27
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5188
Episode_Reward/track_ang_vel_z_exp: 0.1815
       Episode_Reward/lin_vel_z_l2: -0.0366
      Episode_Reward/ang_vel_xy_l2: -0.0576
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3446
     Episode_Reward/action_rate_l2: -0.0035
Episode_Reward/flat_orientation_l2: -0.0189
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7323648
                    Iteration time: 5.78s
                      Time elapsed: 00:36:13
                               ETA: 00:04:22

################################################################################
                      [1m Learning iteration 447/500 [0m                      

                       Computation: 2827 steps/s (collection: 5.709s, learning 0.086s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0058
                 Mean entropy loss: -15.5999
                       Mean reward: 250.27
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.3842
Episode_Reward/track_ang_vel_z_exp: 0.1663
       Episode_Reward/lin_vel_z_l2: -0.0406
      Episode_Reward/ang_vel_xy_l2: -0.0643
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3586
     Episode_Reward/action_rate_l2: -0.0037
Episode_Reward/flat_orientation_l2: -0.0183
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7340032
                    Iteration time: 5.79s
                      Time elapsed: 00:36:19
                               ETA: 00:04:17

################################################################################
                      [1m Learning iteration 448/500 [0m                      

                       Computation: 2812 steps/s (collection: 5.742s, learning 0.084s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0041
                 Mean entropy loss: -15.6463
                       Mean reward: 250.27
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.3842
Episode_Reward/track_ang_vel_z_exp: 0.1663
       Episode_Reward/lin_vel_z_l2: -0.0406
      Episode_Reward/ang_vel_xy_l2: -0.0643
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3586
     Episode_Reward/action_rate_l2: -0.0037
Episode_Reward/flat_orientation_l2: -0.0183
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7356416
                    Iteration time: 5.83s
                      Time elapsed: 00:36:25
                               ETA: 00:04:13

################################################################################
                      [1m Learning iteration 449/500 [0m                      

                       Computation: 2782 steps/s (collection: 5.801s, learning 0.086s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0042
               Mean surrogate loss: -0.0031
                 Mean entropy loss: -15.6535
                       Mean reward: 250.41
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5395
Episode_Reward/track_ang_vel_z_exp: 0.1737
       Episode_Reward/lin_vel_z_l2: -0.0399
      Episode_Reward/ang_vel_xy_l2: -0.0613
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3379
     Episode_Reward/action_rate_l2: -0.0036
Episode_Reward/flat_orientation_l2: -0.0192
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 5.89s
                      Time elapsed: 00:36:31
                               ETA: 00:04:08

################################################################################
                      [1m Learning iteration 450/500 [0m                      

                       Computation: 2816 steps/s (collection: 5.721s, learning 0.096s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0042
               Mean surrogate loss: -0.0035
                 Mean entropy loss: -15.6641
                       Mean reward: 250.41
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5714
Episode_Reward/track_ang_vel_z_exp: 0.1700
       Episode_Reward/lin_vel_z_l2: -0.0434
      Episode_Reward/ang_vel_xy_l2: -0.0636
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3455
     Episode_Reward/action_rate_l2: -0.0037
Episode_Reward/flat_orientation_l2: -0.0200
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7389184
                    Iteration time: 5.82s
                      Time elapsed: 00:36:36
                               ETA: 00:04:03

################################################################################
                      [1m Learning iteration 451/500 [0m                      

                       Computation: 2831 steps/s (collection: 5.698s, learning 0.088s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0035
                 Mean entropy loss: -15.7200
                       Mean reward: 250.41
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5714
Episode_Reward/track_ang_vel_z_exp: 0.1700
       Episode_Reward/lin_vel_z_l2: -0.0434
      Episode_Reward/ang_vel_xy_l2: -0.0636
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3455
     Episode_Reward/action_rate_l2: -0.0037
Episode_Reward/flat_orientation_l2: -0.0200
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7405568
                    Iteration time: 5.79s
                      Time elapsed: 00:36:42
                               ETA: 00:03:58

################################################################################
                      [1m Learning iteration 452/500 [0m                      

                       Computation: 2793 steps/s (collection: 5.782s, learning 0.082s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0036
               Mean surrogate loss: -0.0014
                 Mean entropy loss: -15.7179
                       Mean reward: 250.41
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5714
Episode_Reward/track_ang_vel_z_exp: 0.1700
       Episode_Reward/lin_vel_z_l2: -0.0434
      Episode_Reward/ang_vel_xy_l2: -0.0636
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3455
     Episode_Reward/action_rate_l2: -0.0037
Episode_Reward/flat_orientation_l2: -0.0200
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7421952
                    Iteration time: 5.86s
                      Time elapsed: 00:36:48
                               ETA: 00:03:54

################################################################################
                      [1m Learning iteration 453/500 [0m                      

                       Computation: 2792 steps/s (collection: 5.782s, learning 0.084s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0055
                 Mean entropy loss: -15.7704
                       Mean reward: 250.58
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.6047
Episode_Reward/track_ang_vel_z_exp: 0.1772
       Episode_Reward/lin_vel_z_l2: -0.0376
      Episode_Reward/ang_vel_xy_l2: -0.0589
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3500
     Episode_Reward/action_rate_l2: -0.0036
Episode_Reward/flat_orientation_l2: -0.0192
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7438336
                    Iteration time: 5.87s
                      Time elapsed: 00:36:54
                               ETA: 00:03:49

################################################################################
                      [1m Learning iteration 454/500 [0m                      

                       Computation: 2824 steps/s (collection: 5.717s, learning 0.083s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0036
               Mean surrogate loss: -0.0019
                 Mean entropy loss: -15.7772
                       Mean reward: 250.58
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.6157
Episode_Reward/track_ang_vel_z_exp: 0.1796
       Episode_Reward/lin_vel_z_l2: -0.0357
      Episode_Reward/ang_vel_xy_l2: -0.0573
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3515
     Episode_Reward/action_rate_l2: -0.0035
Episode_Reward/flat_orientation_l2: -0.0190
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7454720
                    Iteration time: 5.80s
                      Time elapsed: 00:37:00
                               ETA: 00:03:44

################################################################################
                      [1m Learning iteration 455/500 [0m                      

                       Computation: 2832 steps/s (collection: 5.692s, learning 0.093s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0043
               Mean surrogate loss: -0.0027
                 Mean entropy loss: -15.8200
                       Mean reward: 250.58
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.6157
Episode_Reward/track_ang_vel_z_exp: 0.1796
       Episode_Reward/lin_vel_z_l2: -0.0357
      Episode_Reward/ang_vel_xy_l2: -0.0573
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3515
     Episode_Reward/action_rate_l2: -0.0035
Episode_Reward/flat_orientation_l2: -0.0190
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 5.78s
                      Time elapsed: 00:37:06
                               ETA: 00:03:39

################################################################################
                      [1m Learning iteration 456/500 [0m                      

                       Computation: 2794 steps/s (collection: 5.770s, learning 0.092s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0038
               Mean surrogate loss: 0.0011
                 Mean entropy loss: -15.8319
                       Mean reward: 250.58
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.6157
Episode_Reward/track_ang_vel_z_exp: 0.1796
       Episode_Reward/lin_vel_z_l2: -0.0357
      Episode_Reward/ang_vel_xy_l2: -0.0573
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3515
     Episode_Reward/action_rate_l2: -0.0035
Episode_Reward/flat_orientation_l2: -0.0190
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7487488
                    Iteration time: 5.86s
                      Time elapsed: 00:37:11
                               ETA: 00:03:34

################################################################################
                      [1m Learning iteration 457/500 [0m                      

                       Computation: 2786 steps/s (collection: 5.795s, learning 0.085s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0055
                 Mean entropy loss: -15.8787
                       Mean reward: 250.58
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.6157
Episode_Reward/track_ang_vel_z_exp: 0.1796
       Episode_Reward/lin_vel_z_l2: -0.0357
      Episode_Reward/ang_vel_xy_l2: -0.0573
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3515
     Episode_Reward/action_rate_l2: -0.0035
Episode_Reward/flat_orientation_l2: -0.0190
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7503872
                    Iteration time: 5.88s
                      Time elapsed: 00:37:17
                               ETA: 00:03:30

################################################################################
                      [1m Learning iteration 458/500 [0m                      

                       Computation: 2819 steps/s (collection: 5.727s, learning 0.085s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0034
               Mean surrogate loss: -0.0038
                 Mean entropy loss: -15.9151
                       Mean reward: 250.58
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.6157
Episode_Reward/track_ang_vel_z_exp: 0.1796
       Episode_Reward/lin_vel_z_l2: -0.0357
      Episode_Reward/ang_vel_xy_l2: -0.0573
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3515
     Episode_Reward/action_rate_l2: -0.0035
Episode_Reward/flat_orientation_l2: -0.0190
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7520256
                    Iteration time: 5.81s
                      Time elapsed: 00:37:23
                               ETA: 00:03:25

################################################################################
                      [1m Learning iteration 459/500 [0m                      

                       Computation: 2806 steps/s (collection: 5.741s, learning 0.096s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0039
                 Mean entropy loss: -15.9964
                       Mean reward: 250.57
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5880
Episode_Reward/track_ang_vel_z_exp: 0.1831
       Episode_Reward/lin_vel_z_l2: -0.0331
      Episode_Reward/ang_vel_xy_l2: -0.0571
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3568
     Episode_Reward/action_rate_l2: -0.0034
Episode_Reward/flat_orientation_l2: -0.0179
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7536640
                    Iteration time: 5.84s
                      Time elapsed: 00:37:29
                               ETA: 00:03:20

################################################################################
                      [1m Learning iteration 460/500 [0m                      

                       Computation: 2795 steps/s (collection: 5.770s, learning 0.091s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0039
               Mean surrogate loss: -0.0021
                 Mean entropy loss: -15.9972
                       Mean reward: 250.59
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4852
Episode_Reward/track_ang_vel_z_exp: 0.1839
       Episode_Reward/lin_vel_z_l2: -0.0335
      Episode_Reward/ang_vel_xy_l2: -0.0629
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3626
     Episode_Reward/action_rate_l2: -0.0035
Episode_Reward/flat_orientation_l2: -0.0176
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7553024
                    Iteration time: 5.86s
                      Time elapsed: 00:37:35
                               ETA: 00:03:15

################################################################################
                      [1m Learning iteration 461/500 [0m                      

                       Computation: 2792 steps/s (collection: 5.757s, learning 0.110s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0036
               Mean surrogate loss: -0.0031
                 Mean entropy loss: -16.0141
                       Mean reward: 250.59
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.6201
Episode_Reward/track_ang_vel_z_exp: 0.1961
       Episode_Reward/lin_vel_z_l2: -0.0301
      Episode_Reward/ang_vel_xy_l2: -0.0534
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3377
     Episode_Reward/action_rate_l2: -0.0033
Episode_Reward/flat_orientation_l2: -0.0180
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 5.87s
                      Time elapsed: 00:37:41
                               ETA: 00:03:10

################################################################################
                      [1m Learning iteration 462/500 [0m                      

                       Computation: 2823 steps/s (collection: 5.718s, learning 0.085s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0041
               Mean surrogate loss: -0.0035
                 Mean entropy loss: -16.0490
                       Mean reward: 250.59
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.6201
Episode_Reward/track_ang_vel_z_exp: 0.1961
       Episode_Reward/lin_vel_z_l2: -0.0301
      Episode_Reward/ang_vel_xy_l2: -0.0534
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3377
     Episode_Reward/action_rate_l2: -0.0033
Episode_Reward/flat_orientation_l2: -0.0180
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7585792
                    Iteration time: 5.80s
                      Time elapsed: 00:37:46
                               ETA: 00:03:06

################################################################################
                      [1m Learning iteration 463/500 [0m                      

                       Computation: 2803 steps/s (collection: 5.730s, learning 0.114s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0045
               Mean surrogate loss: -0.0065
                 Mean entropy loss: -16.1163
                       Mean reward: 250.59
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.6201
Episode_Reward/track_ang_vel_z_exp: 0.1961
       Episode_Reward/lin_vel_z_l2: -0.0301
      Episode_Reward/ang_vel_xy_l2: -0.0534
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3377
     Episode_Reward/action_rate_l2: -0.0033
Episode_Reward/flat_orientation_l2: -0.0180
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7602176
                    Iteration time: 5.84s
                      Time elapsed: 00:37:52
                               ETA: 00:03:01

################################################################################
                      [1m Learning iteration 464/500 [0m                      

                       Computation: 2805 steps/s (collection: 5.757s, learning 0.084s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0024
                 Mean entropy loss: -16.1544
                       Mean reward: 250.59
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.6201
Episode_Reward/track_ang_vel_z_exp: 0.1961
       Episode_Reward/lin_vel_z_l2: -0.0301
      Episode_Reward/ang_vel_xy_l2: -0.0534
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3377
     Episode_Reward/action_rate_l2: -0.0033
Episode_Reward/flat_orientation_l2: -0.0180
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7618560
                    Iteration time: 5.84s
                      Time elapsed: 00:37:58
                               ETA: 00:02:56

################################################################################
                      [1m Learning iteration 465/500 [0m                      

                       Computation: 2784 steps/s (collection: 5.788s, learning 0.095s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0036
               Mean surrogate loss: -0.0029
                 Mean entropy loss: -16.1680
                       Mean reward: 251.18
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.6153
Episode_Reward/track_ang_vel_z_exp: 0.1893
       Episode_Reward/lin_vel_z_l2: -0.0341
      Episode_Reward/ang_vel_xy_l2: -0.0546
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3199
     Episode_Reward/action_rate_l2: -0.0033
Episode_Reward/flat_orientation_l2: -0.0186
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7634944
                    Iteration time: 5.88s
                      Time elapsed: 00:38:04
                               ETA: 00:02:51

################################################################################
                      [1m Learning iteration 466/500 [0m                      

                       Computation: 2793 steps/s (collection: 5.782s, learning 0.083s)
             Mean action noise std: 0.08
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0032
                 Mean entropy loss: -16.2286
                       Mean reward: 251.90
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5687
Episode_Reward/track_ang_vel_z_exp: 0.1848
       Episode_Reward/lin_vel_z_l2: -0.0344
      Episode_Reward/ang_vel_xy_l2: -0.0562
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3175
     Episode_Reward/action_rate_l2: -0.0033
Episode_Reward/flat_orientation_l2: -0.0185
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7651328
                    Iteration time: 5.87s
                      Time elapsed: 00:38:10
                               ETA: 00:02:46

################################################################################
                      [1m Learning iteration 467/500 [0m                      

                       Computation: 2803 steps/s (collection: 5.759s, learning 0.085s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0042
               Mean surrogate loss: -0.0050
                 Mean entropy loss: -16.2784
                       Mean reward: 252.82
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5229
Episode_Reward/track_ang_vel_z_exp: 0.1874
       Episode_Reward/lin_vel_z_l2: -0.0327
      Episode_Reward/ang_vel_xy_l2: -0.0568
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3474
     Episode_Reward/action_rate_l2: -0.0034
Episode_Reward/flat_orientation_l2: -0.0180
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0312
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 5.84s
                      Time elapsed: 00:38:16
                               ETA: 00:02:41

################################################################################
                      [1m Learning iteration 468/500 [0m                      

                       Computation: 2752 steps/s (collection: 5.869s, learning 0.083s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0043
               Mean surrogate loss: -0.0036
                 Mean entropy loss: -16.3105
                       Mean reward: 255.07
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5652
Episode_Reward/track_ang_vel_z_exp: 0.1880
       Episode_Reward/lin_vel_z_l2: -0.0333
      Episode_Reward/ang_vel_xy_l2: -0.0558
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3283
     Episode_Reward/action_rate_l2: -0.0033
Episode_Reward/flat_orientation_l2: -0.0182
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.9375
--------------------------------------------------------------------------------
                   Total timesteps: 7684096
                    Iteration time: 5.95s
                      Time elapsed: 00:38:22
                               ETA: 00:02:37

################################################################################
                      [1m Learning iteration 469/500 [0m                      

                       Computation: 2760 steps/s (collection: 5.845s, learning 0.090s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0058
               Mean surrogate loss: -0.0025
                 Mean entropy loss: -16.3485
                       Mean reward: 255.83
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5463
Episode_Reward/track_ang_vel_z_exp: 0.1900
       Episode_Reward/lin_vel_z_l2: -0.0325
      Episode_Reward/ang_vel_xy_l2: -0.0559
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3339
     Episode_Reward/action_rate_l2: -0.0033
Episode_Reward/flat_orientation_l2: -0.0178
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 2.1875
--------------------------------------------------------------------------------
                   Total timesteps: 7700480
                    Iteration time: 5.94s
                      Time elapsed: 00:38:28
                               ETA: 00:02:32

################################################################################
                      [1m Learning iteration 470/500 [0m                      

                       Computation: 2747 steps/s (collection: 5.879s, learning 0.083s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0052
               Mean surrogate loss: -0.0049
                 Mean entropy loss: -16.4555
                       Mean reward: 255.58
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5359
Episode_Reward/track_ang_vel_z_exp: 0.1919
       Episode_Reward/lin_vel_z_l2: -0.0316
      Episode_Reward/ang_vel_xy_l2: -0.0550
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3316
     Episode_Reward/action_rate_l2: -0.0033
Episode_Reward/flat_orientation_l2: -0.0178
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 2.0625
--------------------------------------------------------------------------------
                   Total timesteps: 7716864
                    Iteration time: 5.96s
                      Time elapsed: 00:38:34
                               ETA: 00:02:27

################################################################################
                      [1m Learning iteration 471/500 [0m                      

                       Computation: 2769 steps/s (collection: 5.829s, learning 0.086s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0052
               Mean surrogate loss: -0.0058
                 Mean entropy loss: -16.4701
                       Mean reward: 255.78
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5710
Episode_Reward/track_ang_vel_z_exp: 0.1915
       Episode_Reward/lin_vel_z_l2: -0.0326
      Episode_Reward/ang_vel_xy_l2: -0.0559
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3368
     Episode_Reward/action_rate_l2: -0.0033
Episode_Reward/flat_orientation_l2: -0.0177
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 2.1875
--------------------------------------------------------------------------------
                   Total timesteps: 7733248
                    Iteration time: 5.92s
                      Time elapsed: 00:38:39
                               ETA: 00:02:22

################################################################################
                      [1m Learning iteration 472/500 [0m                      

                       Computation: 2735 steps/s (collection: 5.904s, learning 0.086s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0043
               Mean surrogate loss: -0.0040
                 Mean entropy loss: -16.4664
                       Mean reward: 256.26
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5607
Episode_Reward/track_ang_vel_z_exp: 0.1935
       Episode_Reward/lin_vel_z_l2: -0.0314
      Episode_Reward/ang_vel_xy_l2: -0.0549
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3347
     Episode_Reward/action_rate_l2: -0.0033
Episode_Reward/flat_orientation_l2: -0.0174
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.9688
--------------------------------------------------------------------------------
                   Total timesteps: 7749632
                    Iteration time: 5.99s
                      Time elapsed: 00:38:45
                               ETA: 00:02:17

################################################################################
                      [1m Learning iteration 473/500 [0m                      

                       Computation: 2752 steps/s (collection: 5.868s, learning 0.083s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0043
                 Mean entropy loss: -16.4978
                       Mean reward: 255.46
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5283
Episode_Reward/track_ang_vel_z_exp: 0.1924
       Episode_Reward/lin_vel_z_l2: -0.0315
      Episode_Reward/ang_vel_xy_l2: -0.0558
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3359
     Episode_Reward/action_rate_l2: -0.0033
Episode_Reward/flat_orientation_l2: -0.0175
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.5625
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 5.95s
                      Time elapsed: 00:38:51
                               ETA: 00:02:12

################################################################################
                      [1m Learning iteration 474/500 [0m                      

                       Computation: 2787 steps/s (collection: 5.794s, learning 0.083s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0056
                 Mean entropy loss: -16.5558
                       Mean reward: 254.84
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5467
Episode_Reward/track_ang_vel_z_exp: 0.1943
       Episode_Reward/lin_vel_z_l2: -0.0318
      Episode_Reward/ang_vel_xy_l2: -0.0534
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3333
     Episode_Reward/action_rate_l2: -0.0032
Episode_Reward/flat_orientation_l2: -0.0177
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.4688
--------------------------------------------------------------------------------
                   Total timesteps: 7782400
                    Iteration time: 5.88s
                      Time elapsed: 00:38:57
                               ETA: 00:02:07

################################################################################
                      [1m Learning iteration 475/500 [0m                      

                       Computation: 2793 steps/s (collection: 5.778s, learning 0.087s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0040
                 Mean entropy loss: -16.5695
                       Mean reward: 255.70
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.6007
Episode_Reward/track_ang_vel_z_exp: 0.1939
       Episode_Reward/lin_vel_z_l2: -0.0322
      Episode_Reward/ang_vel_xy_l2: -0.0541
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3284
     Episode_Reward/action_rate_l2: -0.0032
Episode_Reward/flat_orientation_l2: -0.0176
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.4375
--------------------------------------------------------------------------------
                   Total timesteps: 7798784
                    Iteration time: 5.86s
                      Time elapsed: 00:39:03
                               ETA: 00:02:03

################################################################################
                      [1m Learning iteration 476/500 [0m                      

                       Computation: 2770 steps/s (collection: 5.830s, learning 0.084s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0042
               Mean surrogate loss: -0.0021
                 Mean entropy loss: -16.6396
                       Mean reward: 255.68
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5691
Episode_Reward/track_ang_vel_z_exp: 0.1955
       Episode_Reward/lin_vel_z_l2: -0.0315
      Episode_Reward/ang_vel_xy_l2: -0.0538
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3411
     Episode_Reward/action_rate_l2: -0.0032
Episode_Reward/flat_orientation_l2: -0.0178
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 7815168
                    Iteration time: 5.91s
                      Time elapsed: 00:39:09
                               ETA: 00:01:58

################################################################################
                      [1m Learning iteration 477/500 [0m                      

                       Computation: 2756 steps/s (collection: 5.855s, learning 0.090s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0035
                 Mean entropy loss: -16.6363
                       Mean reward: 255.82
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5355
Episode_Reward/track_ang_vel_z_exp: 0.1929
       Episode_Reward/lin_vel_z_l2: -0.0312
      Episode_Reward/ang_vel_xy_l2: -0.0544
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3306
     Episode_Reward/action_rate_l2: -0.0032
Episode_Reward/flat_orientation_l2: -0.0174
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.2188
--------------------------------------------------------------------------------
                   Total timesteps: 7831552
                    Iteration time: 5.94s
                      Time elapsed: 00:39:15
                               ETA: 00:01:53

################################################################################
                      [1m Learning iteration 478/500 [0m                      

                       Computation: 2824 steps/s (collection: 5.716s, learning 0.084s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0051
                 Mean entropy loss: -16.7420
                       Mean reward: 256.14
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5802
Episode_Reward/track_ang_vel_z_exp: 0.1911
       Episode_Reward/lin_vel_z_l2: -0.0333
      Episode_Reward/ang_vel_xy_l2: -0.0545
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3187
     Episode_Reward/action_rate_l2: -0.0032
Episode_Reward/flat_orientation_l2: -0.0178
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 7847936
                    Iteration time: 5.80s
                      Time elapsed: 00:39:21
                               ETA: 00:01:48

################################################################################
                      [1m Learning iteration 479/500 [0m                      

                       Computation: 2819 steps/s (collection: 5.728s, learning 0.082s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0028
               Mean surrogate loss: -0.0055
                 Mean entropy loss: -16.7912
                       Mean reward: 256.29
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5461
Episode_Reward/track_ang_vel_z_exp: 0.1939
       Episode_Reward/lin_vel_z_l2: -0.0313
      Episode_Reward/ang_vel_xy_l2: -0.0548
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3235
     Episode_Reward/action_rate_l2: -0.0032
Episode_Reward/flat_orientation_l2: -0.0175
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.3125
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 5.81s
                      Time elapsed: 00:39:27
                               ETA: 00:01:43

################################################################################
                      [1m Learning iteration 480/500 [0m                      

                       Computation: 2747 steps/s (collection: 5.860s, learning 0.104s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0036
                 Mean entropy loss: -16.8251
                       Mean reward: 256.51
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5491
Episode_Reward/track_ang_vel_z_exp: 0.1992
       Episode_Reward/lin_vel_z_l2: -0.0277
      Episode_Reward/ang_vel_xy_l2: -0.0527
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3396
     Episode_Reward/action_rate_l2: -0.0031
Episode_Reward/flat_orientation_l2: -0.0169
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7880704
                    Iteration time: 5.96s
                      Time elapsed: 00:39:33
                               ETA: 00:01:38

################################################################################
                      [1m Learning iteration 481/500 [0m                      

                       Computation: 2775 steps/s (collection: 5.794s, learning 0.108s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0045
                 Mean entropy loss: -16.8630
                       Mean reward: 256.17
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5790
Episode_Reward/track_ang_vel_z_exp: 0.2024
       Episode_Reward/lin_vel_z_l2: -0.0260
      Episode_Reward/ang_vel_xy_l2: -0.0510
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3581
     Episode_Reward/action_rate_l2: -0.0031
Episode_Reward/flat_orientation_l2: -0.0167
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0312
--------------------------------------------------------------------------------
                   Total timesteps: 7897088
                    Iteration time: 5.90s
                      Time elapsed: 00:39:38
                               ETA: 00:01:33

################################################################################
                      [1m Learning iteration 482/500 [0m                      

                       Computation: 2824 steps/s (collection: 5.715s, learning 0.086s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0041
                 Mean entropy loss: -16.8918
                       Mean reward: 256.21
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.6220
Episode_Reward/track_ang_vel_z_exp: 0.1993
       Episode_Reward/lin_vel_z_l2: -0.0272
      Episode_Reward/ang_vel_xy_l2: -0.0493
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3591
     Episode_Reward/action_rate_l2: -0.0031
Episode_Reward/flat_orientation_l2: -0.0165
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7913472
                    Iteration time: 5.80s
                      Time elapsed: 00:39:44
                               ETA: 00:01:28

################################################################################
                      [1m Learning iteration 483/500 [0m                      

                       Computation: 2813 steps/s (collection: 5.736s, learning 0.087s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0050
                 Mean entropy loss: -16.9361
                       Mean reward: 256.30
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5056
Episode_Reward/track_ang_vel_z_exp: 0.1989
       Episode_Reward/lin_vel_z_l2: -0.0271
      Episode_Reward/ang_vel_xy_l2: -0.0542
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3579
     Episode_Reward/action_rate_l2: -0.0032
Episode_Reward/flat_orientation_l2: -0.0165
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0938
--------------------------------------------------------------------------------
                   Total timesteps: 7929856
                    Iteration time: 5.82s
                      Time elapsed: 00:39:50
                               ETA: 00:01:23

################################################################################
                      [1m Learning iteration 484/500 [0m                      

                       Computation: 2789 steps/s (collection: 5.772s, learning 0.102s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0039
               Mean surrogate loss: -0.0026
                 Mean entropy loss: -16.9726
                       Mean reward: 256.30
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5182
Episode_Reward/track_ang_vel_z_exp: 0.1968
       Episode_Reward/lin_vel_z_l2: -0.0315
      Episode_Reward/ang_vel_xy_l2: -0.0559
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3261
     Episode_Reward/action_rate_l2: -0.0032
Episode_Reward/flat_orientation_l2: -0.0169
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7946240
                    Iteration time: 5.87s
                      Time elapsed: 00:39:56
                               ETA: 00:01:19

################################################################################
                      [1m Learning iteration 485/500 [0m                      

                       Computation: 2789 steps/s (collection: 5.789s, learning 0.084s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0034
               Mean surrogate loss: -0.0028
                 Mean entropy loss: -17.0157
                       Mean reward: 256.16
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.4611
Episode_Reward/track_ang_vel_z_exp: 0.1924
       Episode_Reward/lin_vel_z_l2: -0.0294
      Episode_Reward/ang_vel_xy_l2: -0.0560
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3552
     Episode_Reward/action_rate_l2: -0.0032
Episode_Reward/flat_orientation_l2: -0.0165
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 5.87s
                      Time elapsed: 00:40:02
                               ETA: 00:01:14

################################################################################
                      [1m Learning iteration 486/500 [0m                      

                       Computation: 2796 steps/s (collection: 5.774s, learning 0.084s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0034
               Mean surrogate loss: 0.0000
                 Mean entropy loss: -17.0404
                       Mean reward: 256.37
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.6074
Episode_Reward/track_ang_vel_z_exp: 0.1921
       Episode_Reward/lin_vel_z_l2: -0.0346
      Episode_Reward/ang_vel_xy_l2: -0.0552
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.2915
     Episode_Reward/action_rate_l2: -0.0031
Episode_Reward/flat_orientation_l2: -0.0178
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7979008
                    Iteration time: 5.86s
                      Time elapsed: 00:40:08
                               ETA: 00:01:09

################################################################################
                      [1m Learning iteration 487/500 [0m                      

                       Computation: 2836 steps/s (collection: 5.692s, learning 0.083s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0034
               Mean surrogate loss: -0.0039
                 Mean entropy loss: -17.0904
                       Mean reward: 256.39
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.6425
Episode_Reward/track_ang_vel_z_exp: 0.1872
       Episode_Reward/lin_vel_z_l2: -0.0368
      Episode_Reward/ang_vel_xy_l2: -0.0556
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.2947
     Episode_Reward/action_rate_l2: -0.0031
Episode_Reward/flat_orientation_l2: -0.0185
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7995392
                    Iteration time: 5.78s
                      Time elapsed: 00:40:13
                               ETA: 00:01:04

################################################################################
                      [1m Learning iteration 488/500 [0m                      

                       Computation: 2787 steps/s (collection: 5.792s, learning 0.085s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0015
                 Mean entropy loss: -17.0938
                       Mean reward: 256.39
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.6455
Episode_Reward/track_ang_vel_z_exp: 0.1883
       Episode_Reward/lin_vel_z_l2: -0.0361
      Episode_Reward/ang_vel_xy_l2: -0.0552
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3222
     Episode_Reward/action_rate_l2: -0.0032
Episode_Reward/flat_orientation_l2: -0.0185
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8011776
                    Iteration time: 5.88s
                      Time elapsed: 00:40:19
                               ETA: 00:00:59

################################################################################
                      [1m Learning iteration 489/500 [0m                      

                       Computation: 2798 steps/s (collection: 5.769s, learning 0.085s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0029
               Mean surrogate loss: -0.0038
                 Mean entropy loss: -17.1238
                       Mean reward: 256.43
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.6695
Episode_Reward/track_ang_vel_z_exp: 0.1988
       Episode_Reward/lin_vel_z_l2: -0.0326
      Episode_Reward/ang_vel_xy_l2: -0.0539
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3002
     Episode_Reward/action_rate_l2: -0.0032
Episode_Reward/flat_orientation_l2: -0.0174
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8028160
                    Iteration time: 5.85s
                      Time elapsed: 00:40:25
                               ETA: 00:00:54

################################################################################
                      [1m Learning iteration 490/500 [0m                      

                       Computation: 2827 steps/s (collection: 5.700s, learning 0.095s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0033
                 Mean entropy loss: -17.1257
                       Mean reward: 256.45
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.6382
Episode_Reward/track_ang_vel_z_exp: 0.2062
       Episode_Reward/lin_vel_z_l2: -0.0300
      Episode_Reward/ang_vel_xy_l2: -0.0513
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3132
     Episode_Reward/action_rate_l2: -0.0031
Episode_Reward/flat_orientation_l2: -0.0173
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8044544
                    Iteration time: 5.79s
                      Time elapsed: 00:40:31
                               ETA: 00:00:49

################################################################################
                      [1m Learning iteration 491/500 [0m                      

                       Computation: 2825 steps/s (collection: 5.697s, learning 0.103s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0041
                 Mean entropy loss: -17.1959
                       Mean reward: 256.43
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.6526
Episode_Reward/track_ang_vel_z_exp: 0.2134
       Episode_Reward/lin_vel_z_l2: -0.0256
      Episode_Reward/ang_vel_xy_l2: -0.0474
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3398
     Episode_Reward/action_rate_l2: -0.0030
Episode_Reward/flat_orientation_l2: -0.0167
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 5.80s
                      Time elapsed: 00:40:37
                               ETA: 00:00:44

################################################################################
                      [1m Learning iteration 492/500 [0m                      

                       Computation: 2783 steps/s (collection: 5.788s, learning 0.098s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0044
                 Mean entropy loss: -17.2628
                       Mean reward: 256.43
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.6944
Episode_Reward/track_ang_vel_z_exp: 0.2127
       Episode_Reward/lin_vel_z_l2: -0.0249
      Episode_Reward/ang_vel_xy_l2: -0.0469
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3442
     Episode_Reward/action_rate_l2: -0.0030
Episode_Reward/flat_orientation_l2: -0.0166
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8077312
                    Iteration time: 5.89s
                      Time elapsed: 00:40:43
                               ETA: 00:00:39

################################################################################
                      [1m Learning iteration 493/500 [0m                      

                       Computation: 2790 steps/s (collection: 5.787s, learning 0.083s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0051
                 Mean entropy loss: -17.3061
                       Mean reward: 256.32
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.3213
Episode_Reward/track_ang_vel_z_exp: 0.2046
       Episode_Reward/lin_vel_z_l2: -0.0244
      Episode_Reward/ang_vel_xy_l2: -0.0522
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3774
     Episode_Reward/action_rate_l2: -0.0031
Episode_Reward/flat_orientation_l2: -0.0152
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8093696
                    Iteration time: 5.87s
                      Time elapsed: 00:40:49
                               ETA: 00:00:34

################################################################################
                      [1m Learning iteration 494/500 [0m                      

                       Computation: 2818 steps/s (collection: 5.730s, learning 0.083s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0042
                 Mean entropy loss: -17.3415
                       Mean reward: 256.32
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.2964
Episode_Reward/track_ang_vel_z_exp: 0.2041
       Episode_Reward/lin_vel_z_l2: -0.0244
      Episode_Reward/ang_vel_xy_l2: -0.0526
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3797
     Episode_Reward/action_rate_l2: -0.0031
Episode_Reward/flat_orientation_l2: -0.0151
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8110080
                    Iteration time: 5.81s
                      Time elapsed: 00:40:54
                               ETA: 00:00:29

################################################################################
                      [1m Learning iteration 495/500 [0m                      

                       Computation: 2836 steps/s (collection: 5.691s, learning 0.085s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0029
               Mean surrogate loss: -0.0025
                 Mean entropy loss: -17.3716
                       Mean reward: 256.32
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.2964
Episode_Reward/track_ang_vel_z_exp: 0.2041
       Episode_Reward/lin_vel_z_l2: -0.0244
      Episode_Reward/ang_vel_xy_l2: -0.0526
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3797
     Episode_Reward/action_rate_l2: -0.0031
Episode_Reward/flat_orientation_l2: -0.0151
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8126464
                    Iteration time: 5.78s
                      Time elapsed: 00:41:00
                               ETA: 00:00:24

################################################################################
                      [1m Learning iteration 496/500 [0m                      

                       Computation: 2788 steps/s (collection: 5.787s, learning 0.089s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0019
                 Mean entropy loss: -17.3973
                       Mean reward: 256.32
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.2964
Episode_Reward/track_ang_vel_z_exp: 0.2041
       Episode_Reward/lin_vel_z_l2: -0.0244
      Episode_Reward/ang_vel_xy_l2: -0.0526
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3797
     Episode_Reward/action_rate_l2: -0.0031
Episode_Reward/flat_orientation_l2: -0.0151
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8142848
                    Iteration time: 5.88s
                      Time elapsed: 00:41:06
                               ETA: 00:00:19

################################################################################
                      [1m Learning iteration 497/500 [0m                      

                       Computation: 2795 steps/s (collection: 5.777s, learning 0.083s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0052
                 Mean entropy loss: -17.4360
                       Mean reward: 256.27
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.3029
Episode_Reward/track_ang_vel_z_exp: 0.2038
       Episode_Reward/lin_vel_z_l2: -0.0247
      Episode_Reward/ang_vel_xy_l2: -0.0528
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.3770
     Episode_Reward/action_rate_l2: -0.0031
Episode_Reward/flat_orientation_l2: -0.0151
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 5.86s
                      Time elapsed: 00:41:12
                               ETA: 00:00:14

################################################################################
                      [1m Learning iteration 498/500 [0m                      

                       Computation: 2821 steps/s (collection: 5.722s, learning 0.085s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0029
               Mean surrogate loss: -0.0023
                 Mean entropy loss: -17.4481
                       Mean reward: 256.27
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5053
Episode_Reward/track_ang_vel_z_exp: 0.1955
       Episode_Reward/lin_vel_z_l2: -0.0340
      Episode_Reward/ang_vel_xy_l2: -0.0587
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.2939
     Episode_Reward/action_rate_l2: -0.0032
Episode_Reward/flat_orientation_l2: -0.0164
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8175616
                    Iteration time: 5.81s
                      Time elapsed: 00:41:18
                               ETA: 00:00:09

################################################################################
                      [1m Learning iteration 499/500 [0m                      

                       Computation: 2835 steps/s (collection: 5.692s, learning 0.086s)
             Mean action noise std: 0.07
          Mean value_function loss: 0.0029
               Mean surrogate loss: -0.0047
                 Mean entropy loss: -17.4937
                       Mean reward: 256.27
               Mean episode length: 1999.00
Episode_Reward/track_lin_vel_xy_exp: 7.5053
Episode_Reward/track_ang_vel_z_exp: 0.1955
       Episode_Reward/lin_vel_z_l2: -0.0340
      Episode_Reward/ang_vel_xy_l2: -0.0587
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.2939
     Episode_Reward/action_rate_l2: -0.0032
Episode_Reward/flat_orientation_l2: -0.0164
    Episode_Reward/foot_contact_l2: -0.8995
     Episode_Reward/foot_force_var: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8192000
                    Iteration time: 5.78s
                      Time elapsed: 00:41:23
                               ETA: 00:00:04

Training time: 2493.75 seconds
2026-02-06T15:24:02Z [66,284ms] [Warning] [omni.fabric.plugin] getAttributeCount called on non-existent path /World/envs/env_99/Robot/base_link/visuals/base_link
2026-02-06T15:24:02Z [66,308ms] [Warning] [omni.fabric.plugin] getTypes called on non-existent path /World/envs/env_99/Robot/base_link/visuals/base_link
2026-02-06T15:24:44Z [108,637ms] [Warning] [carb] Client gpu.foundation.plugin has acquired [gpu::unstable::IMemoryBudgetManagerFactory v0.1] 100 times. Consider accessing this interface with carb::getCachedInterface() (Performance warning)
[2610.068s] Simulation App Shutting Down
